{"version":3,"sources":["webpack://@crc/kitten-ai-lab/webpack/bootstrap","webpack://@crc/kitten-ai-lab/./node_modules/convnetjs/build/convnet.js","webpack://@crc/kitten-ai-lab/./src/models/def.ts","webpack://@crc/kitten-ai-lab/./src/controllers/machine_learning/worker.ts"],"names":["installedModules","__webpack_require__","moduleId","exports","module","i","l","modules","call","m","c","d","name","getter","o","Object","defineProperty","enumerable","get","r","Symbol","toStringTag","value","t","mode","__esModule","ns","create","key","bind","n","object","property","prototype","hasOwnProperty","p","s","convnetjs","REVISION","global","return_v","v_val","gaussRandom","u","Math","random","v","sqrt","log","randf","a","b","arrContains","arr","elt","length","randi","floor","randn","mu","std","zeros","isNaN","ArrayBuffer","Array","Float64Array","maxmin","w","maxv","minv","maxi","mini","dv","randperm","temp","j","array","q","weightedSample","lst","probs","cumprob","k","arrUnique","push","getopt","opt","field_name","default_value","Vol","sx","sy","depth","toString","this","dw","scale","x","y","ix","set","add","get_grad","set_grad","add_grad","cloneAndZero","clone","V","addFrom","addFromScaled","setConst","toJSON","json","fromJSON","augment","crop","dx","dy","fliplr","W","W2","img_to_vol","img","convert_grayscale","canvas","document","createElement","width","height","ctx","getContext","drawImage","e","img_data","getImageData","data","H","pv","x1","ConvLayer","out_depth","filters","in_depth","in_sx","in_sy","stride","pad","l1_decay_mul","l2_decay_mul","out_sx","out_sy","layer_type","bias","bias_pref","biases","forward","is_training","in_act","A","f","ax","ay","fx","fy","fd","oy","ox","out_act","backward","chain_grad","ix1","ix2","getParamsAndGrads","response","params","grads","FullyConnLayer","num_neurons","num_inputs","Vw","wi","tfi","PoolLayer","switchx","switchy","winx","winy","InputLayer","SoftmaxLayer","as","amax","es","esum","exp","mul","RegressionLayer","loss","dim","yi","val","SVMLayer","yscore","ReluLayer","V2","N","V2w","SigmoidLayer","v2wi","MaxoutLayer","group_size","switches","ai","a2","TanhLayer","DropoutLayer","drop_prob","dropped","LocalResponseNormalizationLayer","alpha","beta","console","S_cache_","n2","den","max","min","aa","pow","S","SB","SB2","aj","g","QuadTransformLayer","Ni","i0","i1","Net","options","layers","makeLayers","defs","type","new_defs","def","num_classes","activation","tensor","gs","desugar","prev","act","layer_reponse","getPrediction","L","Lj","Trainer","net","learning_rate","l1_decay","l2_decay","batch_size","method","momentum","ro","eps","gsum","xsum","train","start","Date","getTime","fwd_time","cost_loss","l2_decay_loss","l1_decay_loss","bwd_time","pglist","pg","plen","abs","l1grad","gij","gsumi","xsumi","softmax_loss","SGDTrainer","MagicNet","labels","train_ratio","num_folds","num_candidates","num_epochs","ensemble_size","batch_size_min","batch_size_max","l2_decay_min","l2_decay_max","learning_rate_min","learning_rate_max","momentum_min","momentum_max","neurons_min","neurons_max","folds","candidates","evaluated_candidates","unique_labels","iter","foldix","finish_fold_callback","finish_batch_callback","sampleFolds","sampleCandidates","num_train","train_ix","slice","test_ix","sampleCandidate","input_depth","layer_defs","nl","ni","dp","trainer_def","bs","l2","lr","mom","tp","trainer","cand","acc","accv","step","fold","dataix","lastiter","val_acc","evalValErrors","sort","vals","predict_soft","xout","nv","predict","predicted_label","nets","dummy_candidate","onFinishFold","onFinishBatch","lib","window","jsfeat","TrainingStep","ModelType","LastStepBeforeSaving","TARGET_RANGE","dimension","require","self","send_message","postMessage","is_custom_mode","hidden_layers","training_iterations","addEventListener","message","receive_data","JSON","parse","should_reset","forEach","trainer_options","accuracy","num_training","loop_time","classification","class_index","data_row","res","num_correct","num_data","av_loss","Number","toFixed","classification_data","scores","parseFloat","draw_data","num_line","_loop","test_node","main_class","benchmark","density","weight","unshift","training_result","stringify"],"mappings":"aACA,IAAAA,EAAA,GAGA,SAAAC,EAAAC,GAGA,GAAAF,EAAAE,GACA,OAAAF,EAAAE,GAAAC,QAGA,IAAAC,EAAAJ,EAAAE,GAAA,CACAG,EAAAH,EACAI,GAAA,EACAH,QAAA,IAUA,OANAI,EAAAL,GAAAM,KAAAJ,EAAAD,QAAAC,IAAAD,QAAAF,GAGAG,EAAAE,GAAA,EAGAF,EAAAD,QAKAF,EAAAQ,EAAAF,EAGAN,EAAAS,EAAAV,EAGAC,EAAAU,EAAA,SAAAR,EAAAS,EAAAC,GACAZ,EAAAa,EAAAX,EAAAS,IACAG,OAAAC,eAAAb,EAAAS,EAAA,CAA0CK,YAAA,EAAAC,IAAAL,KAK1CZ,EAAAkB,EAAA,SAAAhB,GACA,oBAAAiB,eAAAC,aACAN,OAAAC,eAAAb,EAAAiB,OAAAC,YAAA,CAAwDC,MAAA,WAExDP,OAAAC,eAAAb,EAAA,cAAiDmB,OAAA,KAQjDrB,EAAAsB,EAAA,SAAAD,EAAAE,GAEA,GADA,EAAAA,IAAAF,EAAArB,EAAAqB,IACA,EAAAE,EAAA,OAAAF,EACA,KAAAE,GAAA,iBAAAF,QAAAG,WAAA,OAAAH,EACA,IAAAI,EAAAX,OAAAY,OAAA,MAGA,GAFA1B,EAAAkB,EAAAO,GACAX,OAAAC,eAAAU,EAAA,WAAyCT,YAAA,EAAAK,UACzC,EAAAE,GAAA,iBAAAF,EAAA,QAAAM,KAAAN,EAAArB,EAAAU,EAAAe,EAAAE,EAAA,SAAAA,GAAgH,OAAAN,EAAAM,IAAqBC,KAAA,KAAAD,IACrI,OAAAF,GAIAzB,EAAA6B,EAAA,SAAA1B,GACA,IAAAS,EAAAT,KAAAqB,WACA,WAA2B,OAAArB,EAAA,SAC3B,WAAiC,OAAAA,GAEjC,OADAH,EAAAU,EAAAE,EAAA,IAAAA,GACAA,GAIAZ,EAAAa,EAAA,SAAAiB,EAAAC,GAAsD,OAAAjB,OAAAkB,UAAAC,eAAA1B,KAAAuB,EAAAC,IAGtD/B,EAAAkC,EAAA,GAIAlC,IAAAmC,EAAA,qBClFA,IAAAC,KAAA,CAA8BC,SAAA,UAC9B,SAAAC,GACA,aAGA,IAAAC,GAAA,EACAC,EAAA,EACAC,EAAA,WACA,GAAAF,EAEA,OADAA,GAAA,EACAC,EAEA,IAAAE,EAAA,EAAAC,KAAAC,SAAA,EACAC,EAAA,EAAAF,KAAAC,SAAA,EACA1B,EAAAwB,IAAAG,IACA,MAAA3B,KAAA,SAAAuB,IACA,IAAAhC,EAAAkC,KAAAG,MAAA,EAAAH,KAAAI,IAAA7B,MAGA,OAFAsB,EAAAK,EAAApC,EACA8B,GAAA,EACAG,EAAAjC,GAEAuC,EAAA,SAAAC,EAAAC,GAA8B,OAAAP,KAAAC,UAAAM,EAAAD,MAiB9BE,EAAA,SAAAC,EAAAC,GACA,QAAAjD,EAAA,EAAAyB,EAAAuB,EAAAE,OAA6BlD,EAAAyB,EAAIzB,IACjC,GAAAgD,EAAAhD,KAAAiD,EAAA,SAEA,UA4DAf,EAAAU,QACAV,EAAAiB,MAjFA,SAAAN,EAAAC,GAA8B,OAAAP,KAAAa,MAAAb,KAAAC,UAAAM,EAAAD,OAkF9BX,EAAAmB,MAjFA,SAAAC,EAAAC,GAAgC,OAAAD,EAAAjB,IAAAkB,GAkFhCrB,EAAAsB,MA/EA,SAAA/B,GACA,eAAAgC,MAAAhC,GAA6C,SAC7C,uBAAAiC,YAAA,CAGA,IADA,IAAAV,EAAA,IAAAW,MAAAlC,GACAzB,EAAA,EAAkBA,EAAAyB,EAAIzB,IAAMgD,EAAAhD,GAAA,EAC5B,OAAAgD,EAEA,WAAAY,aAAAnC,IAwEAS,EAAA2B,OAlDA,SAAAC,GACA,OAAAA,EAAAZ,OAAwB,SAMxB,IALA,IAAAa,EAAAD,EAAA,GACAE,EAAAF,EAAA,GACAG,EAAA,EACAC,EAAA,EACAzC,EAAAqC,EAAAZ,OACAlD,EAAA,EAAgBA,EAAAyB,EAAIzB,IACpB8D,EAAA9D,GAAA+D,IAAuBA,EAAAD,EAAA9D,GAAaiE,EAAAjE,GACpC8D,EAAA9D,GAAAgE,IAAuBA,EAAAF,EAAA9D,GAAakE,EAAAlE,GAEpC,OAAYiE,OAAAF,OAAAG,OAAAF,OAAAG,GAAAJ,EAAAC,IAwCZ9B,EAAAkC,SApCA,SAAA3C,GAKA,IAJA,IAEA4C,EAFArE,EAAAyB,EACA6C,EAAA,EAEAC,EAAA,GACAC,EAAA,EAAgBA,EAAA/C,EAAI+C,IAAAD,EAAAC,KACpB,KAAAxE,KACAsE,EAAA/B,KAAAa,MAAAb,KAAAC,UAAAxC,EAAA,IACAqE,EAAAE,EAAAvE,GACAuE,EAAAvE,GAAAuE,EAAAD,GACAC,EAAAD,GAAAD,EAEA,OAAAE,GAyBArC,EAAAuC,eApBA,SAAAC,EAAAC,GAGA,IAFA,IAAA7C,EAAAc,EAAA,KACAgC,EAAA,EACAC,EAAA,EAAApD,EAAAiD,EAAAxB,OAA6B2B,EAAApD,EAAIoD,IAEjC,GAAA/C,GADA8C,GAAAD,EAAAE,IACuB,OAAAH,EAAAG,IAgBvB3C,EAAA4C,UAhEA,SAAA9B,GAEA,IADA,IAAAF,EAAA,GACA9C,EAAA,EAAAyB,EAAAuB,EAAAE,OAA6BlD,EAAAyB,EAAIzB,IACjC+C,EAAAD,EAAAE,EAAAhD,KACA8C,EAAAiC,KAAA/B,EAAAhD,IAGA,OAAA8C,GA0DAZ,EAAAa,cACAb,EAAA8C,OAbA,SAAAC,EAAAC,EAAAC,GACA,gBAAAF,EAAAC,GAAAD,EAAAC,GAAAC,GAlGA,CAgHCnD,GACD,SAAAE,GACA,aASA,IAAAkD,EAAA,SAAAC,EAAAC,EAAAC,EAAAlF,GAEA,sBAAAK,OAAAkB,UAAA4D,SAAArF,KAAAkF,GAAA,CAEAI,KAAAJ,GAAA,EACAI,KAAAH,GAAA,EACAG,KAAAF,MAAAF,EAAAnC,OAGAuC,KAAA3B,EAAA5B,EAAAsB,MAAAiC,KAAAF,OACAE,KAAAC,GAAAxD,EAAAsB,MAAAiC,KAAAF,OACA,QAAAvF,EAAA,EAAkBA,EAAAyF,KAAAF,MAAavF,IAC/ByF,KAAA3B,EAAA9D,GAAAqF,EAAArF,OAEK,CAELyF,KAAAJ,KACAI,KAAAH,KACAG,KAAAF,QACA,IAAA9D,EAAA4D,EAAAC,EAAAC,EAGA,GAFAE,KAAA3B,EAAA5B,EAAAsB,MAAA/B,GACAgE,KAAAC,GAAAxD,EAAAsB,MAAA/B,QACA,IAAApB,EAIA,KAAAsF,EAAApD,KAAAG,KAAA,GAAA2C,EAAAC,EAAAC,IACA,IAAAvF,EAAA,EAAoBA,EAAAyB,EAAIzB,IACxByF,KAAA3B,EAAA9D,GAAAkC,EAAAmB,MAAA,EAAAsC,QAGA,IAAA3F,EAAA,EAAoBA,EAAAyB,EAAIzB,IACxByF,KAAA3B,EAAA9D,GAAAK,IAMA+E,EAAAxD,UAAA,CACAf,IAAA,SAAA+E,EAAAC,EAAAvF,GACA,IAAAwF,GAAAL,KAAAJ,GAAAQ,EAAAD,GAAAH,KAAAF,MAAAjF,EACA,OAAAmF,KAAA3B,EAAAgC,IAEAC,IAAA,SAAAH,EAAAC,EAAAvF,EAAAmC,GACA,IAAAqD,GAAAL,KAAAJ,GAAAQ,EAAAD,GAAAH,KAAAF,MAAAjF,EACAmF,KAAA3B,EAAAgC,GAAArD,GAEAuD,IAAA,SAAAJ,EAAAC,EAAAvF,EAAAmC,GACA,IAAAqD,GAAAL,KAAAJ,GAAAQ,EAAAD,GAAAH,KAAAF,MAAAjF,EACAmF,KAAA3B,EAAAgC,IAAArD,GAEAwD,SAAA,SAAAL,EAAAC,EAAAvF,GACA,IAAAwF,GAAAL,KAAAJ,GAAAQ,EAAAD,GAAAH,KAAAF,MAAAjF,EACA,OAAAmF,KAAAC,GAAAI,IAEAI,SAAA,SAAAN,EAAAC,EAAAvF,EAAAmC,GACA,IAAAqD,GAAAL,KAAAJ,GAAAQ,EAAAD,GAAAH,KAAAF,MAAAjF,EACAmF,KAAAC,GAAAI,GAAArD,GAEA0D,SAAA,SAAAP,EAAAC,EAAAvF,EAAAmC,GACA,IAAAqD,GAAAL,KAAAJ,GAAAQ,EAAAD,GAAAH,KAAAF,MAAAjF,EACAmF,KAAAC,GAAAI,IAAArD,GAEA2D,aAAA,WAA8B,WAAAhB,EAAAK,KAAAJ,GAAAI,KAAAH,GAAAG,KAAAF,MAAA,IAC9Bc,MAAA,WAGA,IAFA,IAAAC,EAAA,IAAAlB,EAAAK,KAAAJ,GAAAI,KAAAH,GAAAG,KAAAF,MAAA,GACA9D,EAAAgE,KAAA3B,EAAAZ,OACAlD,EAAA,EAAkBA,EAAAyB,EAAIzB,IAAMsG,EAAAxC,EAAA9D,GAAAyF,KAAA3B,EAAA9D,GAC5B,OAAAsG,GAEAC,QAAA,SAAAD,GAA0B,QAAAzB,EAAA,EAAaA,EAAAY,KAAA3B,EAAAZ,OAAgB2B,IAAMY,KAAA3B,EAAAe,IAAAyB,EAAAxC,EAAAe,IAC7D2B,cAAA,SAAAF,EAAAzD,GAAmC,QAAAgC,EAAA,EAAaA,EAAAY,KAAA3B,EAAAZ,OAAgB2B,IAAMY,KAAA3B,EAAAe,IAAAhC,EAAAyD,EAAAxC,EAAAe,IACtE4B,SAAA,SAAA5D,GAA2B,QAAAgC,EAAA,EAAaA,EAAAY,KAAA3B,EAAAZ,OAAgB2B,IAAMY,KAAA3B,EAAAe,GAAAhC,GAE9D6D,OAAA,WAEA,IAAAC,EAAA,GAKA,OAJAA,EAAAtB,GAAAI,KAAAJ,GACAsB,EAAArB,GAAAG,KAAAH,GACAqB,EAAApB,MAAAE,KAAAF,MACAoB,EAAA7C,EAAA2B,KAAA3B,EACA6C,GAGAC,SAAA,SAAAD,GACAlB,KAAAJ,GAAAsB,EAAAtB,GACAI,KAAAH,GAAAqB,EAAArB,GACAG,KAAAF,MAAAoB,EAAApB,MAEA,IAAA9D,EAAAgE,KAAAJ,GAAAI,KAAAH,GAAAG,KAAAF,MACAE,KAAA3B,EAAA5B,EAAAsB,MAAA/B,GACAgE,KAAAC,GAAAxD,EAAAsB,MAAA/B,GAEA,QAAAzB,EAAA,EAAkBA,EAAAyB,EAAIzB,IACtByF,KAAA3B,EAAA9D,GAAA2G,EAAA7C,EAAA9D,KAKAkC,EAAAkD,MA7GA,CA8GCpD,GACD,SAAAE,GACA,aACA,IAAAkD,EAAAlD,EAAAkD,IAsGAlD,EAAA2E,QA/FA,SAAAP,EAAAQ,EAAAC,EAAAC,EAAAC,QAEA,QAAAA,GAAA,QACA,QAAAF,EAAA7E,EAAAiB,MAAA,EAAAmD,EAAAjB,GAAAyB,SACA,QAAAE,EAAA9E,EAAAiB,MAAA,EAAAmD,EAAAhB,GAAAwB,IAGA,IAAAI,EACA,GAAAJ,IAAAR,EAAAjB,IAAA,IAAA0B,GAAA,IAAAC,EAAA,CACAE,EAAA,IAAA9B,EAAA0B,IAAAR,EAAAf,MAAA,GACA,QAAAK,EAAA,EAAkBA,EAAAkB,EAAOlB,IACzB,QAAAC,EAAA,EAAoBA,EAAAiB,EAAOjB,IAC3B,KAAAD,EAAAmB,EAAA,GAAAnB,EAAAmB,GAAAT,EAAAjB,IAAAQ,EAAAmB,EAAA,GAAAnB,EAAAmB,GAAAV,EAAAhB,IACA,QAAAhF,EAAA,EAAsBA,EAAAgG,EAAAf,MAAUjF,IAChC4G,EAAAnB,IAAAH,EAAAC,EAAAvF,EAAAgG,EAAAzF,IAAA+E,EAAAmB,EAAAlB,EAAAmB,EAAA1G,SAKA4G,EAAAZ,EAGA,GAAAW,EAAA,CAEA,IAAAE,EAAAD,EAAAd,eACA,IAAAR,EAAA,EAAkBA,EAAAsB,EAAA7B,GAAOO,IACzB,IAAAC,EAAA,EAAoBA,EAAAqB,EAAA5B,GAAOO,IAC3B,IAAAvF,EAAA,EAAsBA,EAAA4G,EAAA3B,MAAUjF,IAChC6G,EAAApB,IAAAH,EAAAC,EAAAvF,EAAA4G,EAAArG,IAAAqG,EAAA7B,GAAAO,EAAA,EAAAC,EAAAvF,IAIA4G,EAAAC,EAEA,OAAAD,GA8DAhF,EAAAkF,WAzDA,SAAAC,EAAAC,QAEA,QAAAA,GAAA,GAEA,IAAAC,EAAAC,SAAAC,cAAA,UACAF,EAAAG,MAAAL,EAAAK,MACAH,EAAAI,OAAAN,EAAAM,OACA,IAAAC,EAAAL,EAAAM,WAAA,MAGA,IACAD,EAAAE,UAAAT,EAAA,KACK,MAAAU,GACL,8BAAAA,EAAAxH,KAEA,SAEA,MAAAwH,EAIA,IACA,IAAAC,EAAAJ,EAAAK,aAAA,IAAAV,EAAAG,MAAAH,EAAAI,QACK,MAAAI,GACL,sBAAAA,EAAAxH,KACA,SAEA,MAAAwH,EASA,IAJA,IAAAjG,EAAAkG,EAAAE,KACAhB,EAAAG,EAAAK,MACAS,EAAAd,EAAAM,OACAS,EAAA,GACApI,EAAA,EAAgBA,EAAA8B,EAAAoB,OAAWlD,IAC3BoI,EAAArD,KAAAjD,EAAA9B,GAAA,QAEA,IAAA4F,EAAA,IAAAR,EAAA8B,EAAAiB,EAAA,KAGA,GAFAvC,EAAA9B,EAAAsE,EAEAd,EAAA,CAEA,IAAAe,EAAA,IAAAjD,EAAA8B,EAAAiB,EAAA,KACA,IAAAnI,EAAA,EAAkBA,EAAAkH,EAAIlH,IACtB,QAAAsE,EAAA,EAAoBA,EAAA6D,EAAI7D,IACxB+D,EAAAtC,IAAA/F,EAAAsE,EAAA,EAAAsB,EAAA/E,IAAAb,EAAAsE,EAAA,IAGAsB,EAAAyC,EAGA,OAAAzC,GArGA,CA2GC5D,GACD,SAAAE,GACA,aACA,IAAAkD,EAAAlD,EAAAkD,IAQAkD,EAAA,SAAArD,GACAA,KAAA,GAGAQ,KAAA8C,UAAAtD,EAAAuD,QACA/C,KAAAJ,GAAAJ,EAAAI,GACAI,KAAAgD,SAAAxD,EAAAwD,SACAhD,KAAAiD,MAAAzD,EAAAyD,MACAjD,KAAAkD,MAAA1D,EAAA0D,MAGAlD,KAAAH,QAAA,IAAAL,EAAAK,GAAAL,EAAAK,GAAAG,KAAAJ,GACAI,KAAAmD,YAAA,IAAA3D,EAAA2D,OAAA3D,EAAA2D,OAAA,EACAnD,KAAAoD,SAAA,IAAA5D,EAAA4D,IAAA5D,EAAA4D,IAAA,EACApD,KAAAqD,kBAAA,IAAA7D,EAAA6D,aAAA7D,EAAA6D,aAAA,EACArD,KAAAsD,kBAAA,IAAA9D,EAAA8D,aAAA9D,EAAA8D,aAAA,EAMAtD,KAAAuD,OAAAzG,KAAAa,OAAAqC,KAAAiD,MAAA,EAAAjD,KAAAoD,IAAApD,KAAAJ,IAAAI,KAAAmD,OAAA,GACAnD,KAAAwD,OAAA1G,KAAAa,OAAAqC,KAAAkD,MAAA,EAAAlD,KAAAoD,IAAApD,KAAAH,IAAAG,KAAAmD,OAAA,GACAnD,KAAAyD,WAAA,OAGA,IAAAC,OAAA,IAAAlE,EAAAmE,UAAAnE,EAAAmE,UAAA,EACA3D,KAAA+C,QAAA,GACA,QAAAxI,EAAA,EAAgBA,EAAAyF,KAAA8C,UAAiBvI,IAAMyF,KAAA+C,QAAAzD,KAAA,IAAAK,EAAAK,KAAAJ,GAAAI,KAAAH,GAAAG,KAAAgD,WACvChD,KAAA4D,OAAA,IAAAjE,EAAA,IAAAK,KAAA8C,UAAAY,IAEAb,EAAA1G,UAAA,CACA0H,QAAA,SAAAhD,EAAAiD,GACA9D,KAAA+D,OAAAlD,EAGA,IADA,IAAAmD,EAAA,IAAArE,EAAAK,KAAAuD,OAAAvD,KAAAwD,OAAAxD,KAAA8C,UAAA,GACAjI,EAAA,EAAkBA,EAAAmF,KAAA8C,UAAiBjI,IAInC,IAHA,IAAAoJ,EAAAjE,KAAA+C,QAAAlI,GACAsF,GAAAH,KAAAoD,IACAhD,GAAAJ,KAAAoD,IACAc,EAAA,EAAqBA,EAAAlE,KAAAuD,OAAgBpD,GAAAH,KAAAmD,OAAAe,IAAA,CACrC9D,GAAAJ,KAAAoD,IACA,QAAAe,EAAA,EAAuBA,EAAAnE,KAAAwD,OAAgBpD,GAAAJ,KAAAmD,OAAAgB,IAAA,CAKvC,IADA,IAAA/G,EAAA,EACAgH,EAAA,EAAyBA,EAAAH,EAAArE,GAAQwE,IACjC,QAAAC,EAAA,EAA2BA,EAAAJ,EAAApE,GAAQwE,IACnC,QAAAC,EAAA,EAA6BA,EAAAL,EAAAnE,MAAWwE,IAAA,CACxC,IAAAC,EAAAnE,EAAAiE,EACAG,EAAArE,EAAAiE,EACAG,GAAA,GAAAA,EAAA1D,EAAAhB,IAAA2E,GAAA,GAAAA,EAAA3D,EAAAjB,KAGAxC,GAAA6G,EAAA5F,GAAA4F,EAAArE,GAAAyE,EAAAD,GAAAH,EAAAnE,MAAAwE,GAAAzD,EAAAxC,GAAAwC,EAAAjB,GAAA2E,EAAAC,GAAA3D,EAAAf,MAAAwE,IAKAlH,GAAA4C,KAAA4D,OAAAvF,EAAAxD,GACAmJ,EAAA1D,IAAA4D,EAAAC,EAAAtJ,EAAAuC,IAKA,OADA4C,KAAAyE,QAAAT,EACAhE,KAAAyE,SAEAC,SAAA,WAGA,IAAA7D,EAAAb,KAAA+D,OACAlD,EAAAZ,GAAAxD,EAAAsB,MAAA8C,EAAAxC,EAAAZ,QACA,QAAA5C,EAAA,EAAkBA,EAAAmF,KAAA8C,UAAiBjI,IAInC,IAHA,IAAAoJ,EAAAjE,KAAA+C,QAAAlI,GACAsF,GAAAH,KAAAoD,IACAhD,GAAAJ,KAAAoD,IACAc,EAAA,EAAqBA,EAAAlE,KAAAuD,OAAgBpD,GAAAH,KAAAmD,OAAAe,IAAA,CACrC9D,GAAAJ,KAAAoD,IACA,QAAAe,EAAA,EAAuBA,EAAAnE,KAAAwD,OAAgBpD,GAAAJ,KAAAmD,OAAAgB,IAAA,CAIvC,IADA,IAAAQ,EAAA3E,KAAAyE,QAAAjE,SAAA0D,EAAAC,EAAAtJ,GACAuJ,EAAA,EAAyBA,EAAAH,EAAArE,GAAQwE,IACjC,QAAAC,EAAA,EAA2BA,EAAAJ,EAAApE,GAAQwE,IACnC,QAAAC,EAAA,EAA6BA,EAAAL,EAAAnE,MAAWwE,IAAA,CACxC,IAAAC,EAAAnE,EAAAiE,EACAG,EAAArE,EAAAiE,EACA,GAAAG,GAAA,GAAAA,EAAA1D,EAAAhB,IAAA2E,GAAA,GAAAA,EAAA3D,EAAAjB,GAAA,CAMA,IAAAgF,GAAA/D,EAAAjB,GAAA2E,EAAAC,GAAA3D,EAAAf,MAAAwE,EACAO,GAAAZ,EAAArE,GAAAyE,EAAAD,GAAAH,EAAAnE,MAAAwE,EACAL,EAAAhE,GAAA4E,IAAAhE,EAAAxC,EAAAuG,GAAAD,EACA9D,EAAAZ,GAAA2E,IAAAX,EAAA5F,EAAAwG,GAAAF,GAKA3E,KAAA4D,OAAA3D,GAAApF,IAAA8J,KAKAG,kBAAA,WAEA,IADA,IAAAC,EAAA,GACAxK,EAAA,EAAkBA,EAAAyF,KAAA8C,UAAiBvI,IACnCwK,EAAAzF,KAAA,CAAuB0F,OAAAhF,KAAA+C,QAAAxI,GAAA8D,EAAA4G,MAAAjF,KAAA+C,QAAAxI,GAAA0F,GAAAqD,aAAAtD,KAAAsD,aAAAD,aAAArD,KAAAqD,eAGvB,OADA0B,EAAAzF,KAAA,CAAqB0F,OAAAhF,KAAA4D,OAAAvF,EAAA4G,MAAAjF,KAAA4D,OAAA3D,GAAAoD,aAAA,EAAAC,aAAA,IACrByB,GAEA9D,OAAA,WACA,IAAAC,EAAA,GACAA,EAAAtB,GAAAI,KAAAJ,GACAsB,EAAArB,GAAAG,KAAAH,GACAqB,EAAAiC,OAAAnD,KAAAmD,OACAjC,EAAA8B,SAAAhD,KAAAgD,SACA9B,EAAA4B,UAAA9C,KAAA8C,UACA5B,EAAAqC,OAAAvD,KAAAuD,OACArC,EAAAsC,OAAAxD,KAAAwD,OACAtC,EAAAuC,WAAAzD,KAAAyD,WACAvC,EAAAmC,aAAArD,KAAAqD,aACAnC,EAAAoC,aAAAtD,KAAAsD,aACApC,EAAAkC,IAAApD,KAAAoD,IACAlC,EAAA6B,QAAA,GACA,QAAAxI,EAAA,EAAkBA,EAAAyF,KAAA+C,QAAAtF,OAAsBlD,IACxC2G,EAAA6B,QAAAzD,KAAAU,KAAA+C,QAAAxI,GAAA0G,UAGA,OADAC,EAAA0C,OAAA5D,KAAA4D,OAAA3C,SACAC,GAEAC,SAAA,SAAAD,GACAlB,KAAA8C,UAAA5B,EAAA4B,UACA9C,KAAAuD,OAAArC,EAAAqC,OACAvD,KAAAwD,OAAAtC,EAAAsC,OACAxD,KAAAyD,WAAAvC,EAAAuC,WACAzD,KAAAJ,GAAAsB,EAAAtB,GACAI,KAAAH,GAAAqB,EAAArB,GACAG,KAAAmD,OAAAjC,EAAAiC,OACAnD,KAAAgD,SAAA9B,EAAA8B,SACAhD,KAAA+C,QAAA,GACA/C,KAAAqD,kBAAA,IAAAnC,EAAAmC,aAAAnC,EAAAmC,aAAA,EACArD,KAAAsD,kBAAA,IAAApC,EAAAoC,aAAApC,EAAAoC,aAAA,EACAtD,KAAAoD,SAAA,IAAAlC,EAAAkC,IAAAlC,EAAAkC,IAAA,EACA,QAAA7I,EAAA,EAAkBA,EAAA2G,EAAA6B,QAAAtF,OAAsBlD,IAAA,CACxC,IAAAyC,EAAA,IAAA2C,EAAA,SACA3C,EAAAmE,SAAAD,EAAA6B,QAAAxI,IACAyF,KAAA+C,QAAAzD,KAAAtC,GAEAgD,KAAA4D,OAAA,IAAAjE,EAAA,SACAK,KAAA4D,OAAAzC,SAAAD,EAAA0C,UAIA,IAAAsB,EAAA,SAAA1F,GACAA,KAAA,GAIAQ,KAAA8C,eAAA,IAAAtD,EAAA2F,YAAA3F,EAAA2F,YAAA3F,EAAAuD,QAGA/C,KAAAqD,kBAAA,IAAA7D,EAAA6D,aAAA7D,EAAA6D,aAAA,EACArD,KAAAsD,kBAAA,IAAA9D,EAAA8D,aAAA9D,EAAA8D,aAAA,EAGAtD,KAAAoF,WAAA5F,EAAAyD,MAAAzD,EAAA0D,MAAA1D,EAAAwD,SACAhD,KAAAuD,OAAA,EACAvD,KAAAwD,OAAA,EACAxD,KAAAyD,WAAA,KAGA,IAAAC,OAAA,IAAAlE,EAAAmE,UAAAnE,EAAAmE,UAAA,EACA3D,KAAA+C,QAAA,GACA,QAAAxI,EAAA,EAAgBA,EAAAyF,KAAA8C,UAAkBvI,IAAMyF,KAAA+C,QAAAzD,KAAA,IAAAK,EAAA,IAAAK,KAAAoF,aACxCpF,KAAA4D,OAAA,IAAAjE,EAAA,IAAAK,KAAA8C,UAAAY,IAGAwB,EAAA/I,UAAA,CACA0H,QAAA,SAAAhD,EAAAiD,GACA9D,KAAA+D,OAAAlD,EAGA,IAFA,IAAAmD,EAAA,IAAArE,EAAA,IAAAK,KAAA8C,UAAA,GACAuC,EAAAxE,EAAAxC,EACA9D,EAAA,EAAkBA,EAAAyF,KAAA8C,UAAiBvI,IAAA,CAGnC,IAFA,IAAA6C,EAAA,EACAkI,EAAAtF,KAAA+C,QAAAxI,GAAA8D,EACAxD,EAAA,EAAoBA,EAAAmF,KAAAoF,WAAkBvK,IACtCuC,GAAAiI,EAAAxK,GAAAyK,EAAAzK,GAEAuC,GAAA4C,KAAA4D,OAAAvF,EAAA9D,GACAyJ,EAAA3F,EAAA9D,GAAA6C,EAGA,OADA4C,KAAAyE,QAAAT,EACAhE,KAAAyE,SAEAC,SAAA,WACA,IAAA7D,EAAAb,KAAA+D,OACAlD,EAAAZ,GAAAxD,EAAAsB,MAAA8C,EAAAxC,EAAAZ,QAGA,QAAAlD,EAAA,EAAkBA,EAAAyF,KAAA8C,UAAiBvI,IAAA,CAGnC,IAFA,IAAAgL,EAAAvF,KAAA+C,QAAAxI,GACAoK,EAAA3E,KAAAyE,QAAAxE,GAAA1F,GACAM,EAAA,EAAoBA,EAAAmF,KAAAoF,WAAkBvK,IACtCgG,EAAAZ,GAAApF,IAAA0K,EAAAlH,EAAAxD,GAAA8J,EACAY,EAAAtF,GAAApF,IAAAgG,EAAAxC,EAAAxD,GAAA8J,EAEA3E,KAAA4D,OAAA3D,GAAA1F,IAAAoK,IAGAG,kBAAA,WAEA,IADA,IAAAC,EAAA,GACAxK,EAAA,EAAkBA,EAAAyF,KAAA8C,UAAiBvI,IACnCwK,EAAAzF,KAAA,CAAuB0F,OAAAhF,KAAA+C,QAAAxI,GAAA8D,EAAA4G,MAAAjF,KAAA+C,QAAAxI,GAAA0F,GAAAoD,aAAArD,KAAAqD,aAAAC,aAAAtD,KAAAsD,eAGvB,OADAyB,EAAAzF,KAAA,CAAqB0F,OAAAhF,KAAA4D,OAAAvF,EAAA4G,MAAAjF,KAAA4D,OAAA3D,GAAAoD,aAAA,EAAAC,aAAA,IACrByB,GAEA9D,OAAA,WACA,IAAAC,EAAA,GACAA,EAAA4B,UAAA9C,KAAA8C,UACA5B,EAAAqC,OAAAvD,KAAAuD,OACArC,EAAAsC,OAAAxD,KAAAwD,OACAtC,EAAAuC,WAAAzD,KAAAyD,WACAvC,EAAAkE,WAAApF,KAAAoF,WACAlE,EAAAmC,aAAArD,KAAAqD,aACAnC,EAAAoC,aAAAtD,KAAAsD,aACApC,EAAA6B,QAAA,GACA,QAAAxI,EAAA,EAAkBA,EAAAyF,KAAA+C,QAAAtF,OAAsBlD,IACxC2G,EAAA6B,QAAAzD,KAAAU,KAAA+C,QAAAxI,GAAA0G,UAGA,OADAC,EAAA0C,OAAA5D,KAAA4D,OAAA3C,SACAC,GAEAC,SAAA,SAAAD,GACAlB,KAAA8C,UAAA5B,EAAA4B,UACA9C,KAAAuD,OAAArC,EAAAqC,OACAvD,KAAAwD,OAAAtC,EAAAsC,OACAxD,KAAAyD,WAAAvC,EAAAuC,WACAzD,KAAAoF,WAAAlE,EAAAkE,WACApF,KAAAqD,kBAAA,IAAAnC,EAAAmC,aAAAnC,EAAAmC,aAAA,EACArD,KAAAsD,kBAAA,IAAApC,EAAAoC,aAAApC,EAAAoC,aAAA,EACAtD,KAAA+C,QAAA,GACA,QAAAxI,EAAA,EAAkBA,EAAA2G,EAAA6B,QAAAtF,OAAsBlD,IAAA,CACxC,IAAAyC,EAAA,IAAA2C,EAAA,SACA3C,EAAAmE,SAAAD,EAAA6B,QAAAxI,IACAyF,KAAA+C,QAAAzD,KAAAtC,GAEAgD,KAAA4D,OAAA,IAAAjE,EAAA,SACAK,KAAA4D,OAAAzC,SAAAD,EAAA0C,UAIAnH,EAAAoG,YACApG,EAAAyI,iBA5QA,CA8QC3I,GACD,SAAAE,GACA,aACA,IAAAkD,EAAAlD,EAAAkD,IAEA6F,EAAA,SAAAhG,GAEAA,KAAA,GAGAQ,KAAAJ,GAAAJ,EAAAI,GACAI,KAAAgD,SAAAxD,EAAAwD,SACAhD,KAAAiD,MAAAzD,EAAAyD,MACAjD,KAAAkD,MAAA1D,EAAA0D,MAGAlD,KAAAH,QAAA,IAAAL,EAAAK,GAAAL,EAAAK,GAAAG,KAAAJ,GACAI,KAAAmD,YAAA,IAAA3D,EAAA2D,OAAA3D,EAAA2D,OAAA,EACAnD,KAAAoD,SAAA,IAAA5D,EAAA4D,IAAA5D,EAAA4D,IAAA,EAGApD,KAAA8C,UAAA9C,KAAAgD,SACAhD,KAAAuD,OAAAzG,KAAAa,OAAAqC,KAAAiD,MAAA,EAAAjD,KAAAoD,IAAApD,KAAAJ,IAAAI,KAAAmD,OAAA,GACAnD,KAAAwD,OAAA1G,KAAAa,OAAAqC,KAAAkD,MAAA,EAAAlD,KAAAoD,IAAApD,KAAAH,IAAAG,KAAAmD,OAAA,GACAnD,KAAAyD,WAAA,OAEAzD,KAAAyF,QAAAhJ,EAAAsB,MAAAiC,KAAAuD,OAAAvD,KAAAwD,OAAAxD,KAAA8C,WACA9C,KAAA0F,QAAAjJ,EAAAsB,MAAAiC,KAAAuD,OAAAvD,KAAAwD,OAAAxD,KAAA8C,YAGA0C,EAAArJ,UAAA,CACA0H,QAAA,SAAAhD,EAAAiD,GACA9D,KAAA+D,OAAAlD,EAKA,IAHA,IAAAmD,EAAA,IAAArE,EAAAK,KAAAuD,OAAAvD,KAAAwD,OAAAxD,KAAA8C,UAAA,GAEA9G,EAAA,EACAnB,EAAA,EAAkBA,EAAAmF,KAAA8C,UAAiBjI,IAGnC,IAFA,IAAAsF,GAAAH,KAAAoD,IACAhD,GAAAJ,KAAAoD,IACAc,EAAA,EAAqBA,EAAAlE,KAAAuD,OAAgBpD,GAAAH,KAAAmD,OAAAe,IAAA,CACrC9D,GAAAJ,KAAAoD,IACA,QAAAe,EAAA,EAAuBA,EAAAnE,KAAAwD,OAAgBpD,GAAAJ,KAAAmD,OAAAgB,IAAA,CAKvC,IAFA,IAAA/G,GAAA,MACAuI,GAAA,EAAAC,GAAA,EACAxB,EAAA,EAAyBA,EAAApE,KAAAJ,GAAWwE,IACpC,QAAAC,EAAA,EAA2BA,EAAArE,KAAAH,GAAWwE,IAAA,CACtC,IAAAE,EAAAnE,EAAAiE,EACAG,EAAArE,EAAAiE,EACA,GAAAG,GAAA,GAAAA,EAAA1D,EAAAhB,IAAA2E,GAAA,GAAAA,EAAA3D,EAAAjB,GAAA,CACA,IAAA5C,EAAA6D,EAAAzF,IAAAoJ,EAAAD,EAAA1J,GAIAmC,EAAAI,IAA6BA,EAAAJ,EAAO2I,EAAAnB,EAASoB,EAAArB,IAI7CvE,KAAAyF,QAAAzJ,GAAA2J,EACA3F,KAAA0F,QAAA1J,GAAA4J,EACA5J,IACAgI,EAAA1D,IAAA4D,EAAAC,EAAAtJ,EAAAuC,IAKA,OADA4C,KAAAyE,QAAAT,EACAhE,KAAAyE,SAEAC,SAAA,WAGA,IAAA7D,EAAAb,KAAA+D,OACAlD,EAAAZ,GAAAxD,EAAAsB,MAAA8C,EAAAxC,EAAAZ,QACAuC,KAAAyE,QAGA,IAHA,IAEAzI,EAAA,EACAnB,EAAA,EAAkBA,EAAAmF,KAAA8C,UAAiBjI,IACnC,CAAAmF,KAAAoD,IACApD,KAAAoD,IACA,IAFA,IAEAc,EAAA,EAAqBA,EAAAlE,KAAAuD,OAAgBvD,KAAAmD,OAAAe,IAAA,EACrClE,KAAAoD,IACA,QAAAe,EAAA,EAAuBA,EAAAnE,KAAAwD,OAAgBxD,KAAAmD,OAAAgB,IAAA,CAEvC,IAAAQ,EAAA3E,KAAAyE,QAAAjE,SAAA0D,EAAAC,EAAAtJ,GACAgG,EAAAH,SAAAV,KAAAyF,QAAAzJ,GAAAgE,KAAA0F,QAAA1J,GAAAnB,EAAA8J,GACA3I,QAMA8I,kBAAA,WACA,UAEA7D,OAAA,WACA,IAAAC,EAAA,GAUA,OATAA,EAAAtB,GAAAI,KAAAJ,GACAsB,EAAArB,GAAAG,KAAAH,GACAqB,EAAAiC,OAAAnD,KAAAmD,OACAjC,EAAA8B,SAAAhD,KAAAgD,SACA9B,EAAA4B,UAAA9C,KAAA8C,UACA5B,EAAAqC,OAAAvD,KAAAuD,OACArC,EAAAsC,OAAAxD,KAAAwD,OACAtC,EAAAuC,WAAAzD,KAAAyD,WACAvC,EAAAkC,IAAApD,KAAAoD,IACAlC,GAEAC,SAAA,SAAAD,GACAlB,KAAA8C,UAAA5B,EAAA4B,UACA9C,KAAAuD,OAAArC,EAAAqC,OACAvD,KAAAwD,OAAAtC,EAAAsC,OACAxD,KAAAyD,WAAAvC,EAAAuC,WACAzD,KAAAJ,GAAAsB,EAAAtB,GACAI,KAAAH,GAAAqB,EAAArB,GACAG,KAAAmD,OAAAjC,EAAAiC,OACAnD,KAAAgD,SAAA9B,EAAA8B,SACAhD,KAAAoD,SAAA,IAAAlC,EAAAkC,IAAAlC,EAAAkC,IAAA,EACApD,KAAAyF,QAAAhJ,EAAAsB,MAAAiC,KAAAuD,OAAAvD,KAAAwD,OAAAxD,KAAA8C,WACA9C,KAAA0F,QAAAjJ,EAAAsB,MAAAiC,KAAAuD,OAAAvD,KAAAwD,OAAAxD,KAAA8C,aAIArG,EAAA+I,YA3HA,CA6HCjJ,GAED,SAAAE,GACA,aACAA,EAAAkD,IAAA,IAEAkG,EAAA,SAAArG,GACAA,KAAA,GAGAQ,KAAAuD,YAAA,IAAA/D,EAAA+D,OAAA/D,EAAA+D,OAAA/D,EAAAyD,MACAjD,KAAAwD,YAAA,IAAAhE,EAAAgE,OAAAhE,EAAAgE,OAAAhE,EAAA0D,MACAlD,KAAA8C,eAAA,IAAAtD,EAAAsD,UAAAtD,EAAAsD,UAAAtD,EAAAwD,SACAhD,KAAAyD,WAAA,SAEAoC,EAAA1J,UAAA,CACA0H,QAAA,SAAAhD,EAAAiD,GAGA,OAFA9D,KAAA+D,OAAAlD,EACAb,KAAAyE,QAAA5D,EACAb,KAAAyE,SAEAC,SAAA,aACAI,kBAAA,WACA,UAEA7D,OAAA,WACA,IAAAC,EAAA,GAKA,OAJAA,EAAA4B,UAAA9C,KAAA8C,UACA5B,EAAAqC,OAAAvD,KAAAuD,OACArC,EAAAsC,OAAAxD,KAAAwD,OACAtC,EAAAuC,WAAAzD,KAAAyD,WACAvC,GAEAC,SAAA,SAAAD,GACAlB,KAAA8C,UAAA5B,EAAA4B,UACA9C,KAAAuD,OAAArC,EAAAqC,OACAvD,KAAAwD,OAAAtC,EAAAsC,OACAxD,KAAAyD,WAAAvC,EAAAuC,aAIAhH,EAAAoJ,aAvCA,CAwCCtJ,GACD,SAAAE,GACA,aACA,IAAAkD,EAAAlD,EAAAkD,IAWAmG,EAAA,SAAAtG,GACAA,KAAA,GAGAQ,KAAAoF,WAAA5F,EAAAyD,MAAAzD,EAAA0D,MAAA1D,EAAAwD,SACAhD,KAAA8C,UAAA9C,KAAAoF,WACApF,KAAAuD,OAAA,EACAvD,KAAAwD,OAAA,EACAxD,KAAAyD,WAAA,WAGAqC,EAAA3J,UAAA,CACA0H,QAAA,SAAAhD,EAAAiD,GACA9D,KAAA+D,OAAAlD,EAOA,IALA,IAAAmD,EAAA,IAAArE,EAAA,IAAAK,KAAA8C,UAAA,GAGAiD,EAAAlF,EAAAxC,EACA2H,EAAAnF,EAAAxC,EAAA,GACA9D,EAAA,EAAkBA,EAAAyF,KAAA8C,UAAiBvI,IACnCwL,EAAAxL,GAAAyL,MAAAD,EAAAxL,IAIA,IAAA0L,EAAAxJ,EAAAsB,MAAAiC,KAAA8C,WACAoD,EAAA,EACA,IAAA3L,EAAA,EAAkBA,EAAAyF,KAAA8C,UAAiBvI,IAAA,CACnC,IAAA+H,EAAAxF,KAAAqJ,IAAAJ,EAAAxL,GAAAyL,GACAE,GAAA5D,EACA2D,EAAA1L,GAAA+H,EAIA,IAAA/H,EAAA,EAAkBA,EAAAyF,KAAA8C,UAAiBvI,IACnC0L,EAAA1L,IAAA2L,EACAlC,EAAA3F,EAAA9D,GAAA0L,EAAA1L,GAKA,OAFAyF,KAAAiG,KACAjG,KAAAyE,QAAAT,EACAhE,KAAAyE,SAEAC,SAAA,SAAAtE,GAGA,IAAAD,EAAAH,KAAA+D,OACA5D,EAAAF,GAAAxD,EAAAsB,MAAAoC,EAAA9B,EAAAZ,QAEA,QAAAlD,EAAA,EAAkBA,EAAAyF,KAAA8C,UAAiBvI,IAAA,CACnC,IACA6L,KADA7L,IAAA6F,EAAA,KACAJ,KAAAiG,GAAA1L,IACA4F,EAAAF,GAAA1F,GAAA6L,EAIA,OAAAtJ,KAAAI,IAAA8C,KAAAiG,GAAA7F,KAEA0E,kBAAA,WACA,UAEA7D,OAAA,WACA,IAAAC,EAAA,GAMA,OALAA,EAAA4B,UAAA9C,KAAA8C,UACA5B,EAAAqC,OAAAvD,KAAAuD,OACArC,EAAAsC,OAAAxD,KAAAwD,OACAtC,EAAAuC,WAAAzD,KAAAyD,WACAvC,EAAAkE,WAAApF,KAAAoF,WACAlE,GAEAC,SAAA,SAAAD,GACAlB,KAAA8C,UAAA5B,EAAA4B,UACA9C,KAAAuD,OAAArC,EAAAqC,OACAvD,KAAAwD,OAAAtC,EAAAsC,OACAxD,KAAAyD,WAAAvC,EAAAuC,WACAzD,KAAAoF,WAAAlE,EAAAkE,aAOA,IAAAiB,EAAA,SAAA7G,GACAA,KAAA,GAGAQ,KAAAoF,WAAA5F,EAAAyD,MAAAzD,EAAA0D,MAAA1D,EAAAwD,SACAhD,KAAA8C,UAAA9C,KAAAoF,WACApF,KAAAuD,OAAA,EACAvD,KAAAwD,OAAA,EACAxD,KAAAyD,WAAA,cAGA4C,EAAAlK,UAAA,CACA0H,QAAA,SAAAhD,EAAAiD,GAGA,OAFA9D,KAAA+D,OAAAlD,EACAb,KAAAyE,QAAA5D,EACAA,GAGA6D,SAAA,SAAAtE,GAGA,IAAAD,EAAAH,KAAA+D,OACA5D,EAAAF,GAAAxD,EAAAsB,MAAAoC,EAAA9B,EAAAZ,QACA,IAAA6I,EAAA,EACA,GAAAlG,aAAAlC,OAAAkC,aAAAjC,aACA,QAAA5D,EAAA,EAAoBA,EAAAyF,KAAA8C,UAAiBvI,IAAA,CACrC,IAAAgH,EAAApB,EAAA9B,EAAA9D,GAAA6F,EAAA7F,GACA4F,EAAAF,GAAA1F,GAAAgH,EACA+E,GAAA,EAAA/E,QAEO,CAGPhH,EAAA6F,EAAAmG,IAAA,IACAC,EAAApG,EAAAqG,IACAlF,EAAApB,EAAA9B,EAAA9D,GAAAiM,EACArG,EAAAF,GAAA1F,GAAAgH,EACA+E,GAAA,EAAA/E,IAEA,OAAA+E,GAEAxB,kBAAA,WACA,UAEA7D,OAAA,WACA,IAAAC,EAAA,GAMA,OALAA,EAAA4B,UAAA9C,KAAA8C,UACA5B,EAAAqC,OAAAvD,KAAAuD,OACArC,EAAAsC,OAAAxD,KAAAwD,OACAtC,EAAAuC,WAAAzD,KAAAyD,WACAvC,EAAAkE,WAAApF,KAAAoF,WACAlE,GAEAC,SAAA,SAAAD,GACAlB,KAAA8C,UAAA5B,EAAA4B,UACA9C,KAAAuD,OAAArC,EAAAqC,OACAvD,KAAAwD,OAAAtC,EAAAsC,OACAxD,KAAAyD,WAAAvC,EAAAuC,WACAzD,KAAAoF,WAAAlE,EAAAkE,aAIA,IAAAsB,EAAA,SAAAlH,GACAA,KAAA,GAGAQ,KAAAoF,WAAA5F,EAAAyD,MAAAzD,EAAA0D,MAAA1D,EAAAwD,SACAhD,KAAA8C,UAAA9C,KAAAoF,WACApF,KAAAuD,OAAA,EACAvD,KAAAwD,OAAA,EACAxD,KAAAyD,WAAA,OAGAiD,EAAAvK,UAAA,CACA0H,QAAA,SAAAhD,EAAAiD,GAGA,OAFA9D,KAAA+D,OAAAlD,EACAb,KAAAyE,QAAA5D,EACAA,GAEA6D,SAAA,SAAAtE,GAGA,IAAAD,EAAAH,KAAA+D,OACA5D,EAAAF,GAAAxD,EAAAsB,MAAAoC,EAAA9B,EAAAZ,QAKA,IAHA,IAAAkJ,EAAAxG,EAAA9B,EAAA+B,GAEAkG,EAAA,EACA/L,EAAA,EAAkBA,EAAAyF,KAAA8C,UAAiBvI,KACnCoM,EAAAxG,EAAA9B,EAAA9D,GAHA,EAGA,IAMA4F,EAAAF,GAAA1F,IAAA,EACA4F,EAAAF,GAAAG,IAAA,EACAkG,IAAAK,EAAAxG,EAAA9B,EAAA9D,GAXA,GAeA,OAAA+L,GAEAxB,kBAAA,WACA,UAEA7D,OAAA,WACA,IAAAC,EAAA,GAMA,OALAA,EAAA4B,UAAA9C,KAAA8C,UACA5B,EAAAqC,OAAAvD,KAAAuD,OACArC,EAAAsC,OAAAxD,KAAAwD,OACAtC,EAAAuC,WAAAzD,KAAAyD,WACAvC,EAAAkE,WAAApF,KAAAoF,WACAlE,GAEAC,SAAA,SAAAD,GACAlB,KAAA8C,UAAA5B,EAAA4B,UACA9C,KAAAuD,OAAArC,EAAAqC,OACAvD,KAAAwD,OAAAtC,EAAAsC,OACAxD,KAAAyD,WAAAvC,EAAAuC,WACAzD,KAAAoF,WAAAlE,EAAAkE,aAIA3I,EAAA4J,kBACA5J,EAAAqJ,eACArJ,EAAAiK,WA7NA,CA+NCnK,GAED,SAAAE,GACA,aACA,IAAAkD,EAAAlD,EAAAkD,IAKAiH,EAAA,SAAApH,GACAA,KAAA,GAGAQ,KAAAuD,OAAA/D,EAAAyD,MACAjD,KAAAwD,OAAAhE,EAAA0D,MACAlD,KAAA8C,UAAAtD,EAAAwD,SACAhD,KAAAyD,WAAA,QAEAmD,EAAAzK,UAAA,CACA0H,QAAA,SAAAhD,EAAAiD,GACA9D,KAAA+D,OAAAlD,EAIA,IAHA,IAAAgG,EAAAhG,EAAAD,QACAkG,EAAAjG,EAAAxC,EAAAZ,OACAsJ,EAAAF,EAAAxI,EACA9D,EAAA,EAAkBA,EAAAuM,EAAIvM,IACtBwM,EAAAxM,GAAA,IAAAwM,EAAAxM,GAAA,GAGA,OADAyF,KAAAyE,QAAAoC,EACA7G,KAAAyE,SAEAC,SAAA,WACA,IAAA7D,EAAAb,KAAA+D,OACA8C,EAAA7G,KAAAyE,QACAqC,EAAAjG,EAAAxC,EAAAZ,OACAoD,EAAAZ,GAAAxD,EAAAsB,MAAA+I,GACA,QAAAvM,EAAA,EAAkBA,EAAAuM,EAAIvM,IACtBsM,EAAAxI,EAAA9D,IAAA,EAAAsG,EAAAZ,GAAA1F,GAAA,EACAsG,EAAAZ,GAAA1F,GAAAsM,EAAA5G,GAAA1F,IAGAuK,kBAAA,WACA,UAEA7D,OAAA,WACA,IAAAC,EAAA,GAKA,OAJAA,EAAA4B,UAAA9C,KAAA8C,UACA5B,EAAAqC,OAAAvD,KAAAuD,OACArC,EAAAsC,OAAAxD,KAAAwD,OACAtC,EAAAuC,WAAAzD,KAAAyD,WACAvC,GAEAC,SAAA,SAAAD,GACAlB,KAAA8C,UAAA5B,EAAA4B,UACA9C,KAAAuD,OAAArC,EAAAqC,OACAvD,KAAAwD,OAAAtC,EAAAsC,OACAxD,KAAAyD,WAAAvC,EAAAuC,aAOA,IAAAuD,EAAA,SAAAxH,GACAA,KAAA,GAGAQ,KAAAuD,OAAA/D,EAAAyD,MACAjD,KAAAwD,OAAAhE,EAAA0D,MACAlD,KAAA8C,UAAAtD,EAAAwD,SACAhD,KAAAyD,WAAA,WAEAuD,EAAA7K,UAAA,CACA0H,QAAA,SAAAhD,EAAAiD,GACA9D,KAAA+D,OAAAlD,EAKA,IAJA,IAAAgG,EAAAhG,EAAAF,eACAmG,EAAAjG,EAAAxC,EAAAZ,OACAsJ,EAAAF,EAAAxI,EACAgH,EAAAxE,EAAAxC,EACA9D,EAAA,EAAkBA,EAAAuM,EAAIvM,IACtBwM,EAAAxM,GAAA,KAAAuC,KAAAqJ,KAAAd,EAAA9K,KAGA,OADAyF,KAAAyE,QAAAoC,EACA7G,KAAAyE,SAEAC,SAAA,WACA,IAAA7D,EAAAb,KAAA+D,OACA8C,EAAA7G,KAAAyE,QACAqC,EAAAjG,EAAAxC,EAAAZ,OACAoD,EAAAZ,GAAAxD,EAAAsB,MAAA+I,GACA,QAAAvM,EAAA,EAAkBA,EAAAuM,EAAIvM,IAAA,CACtB,IAAA0M,EAAAJ,EAAAxI,EAAA9D,GACAsG,EAAAZ,GAAA1F,GAAA0M,GAAA,EAAAA,GAAAJ,EAAA5G,GAAA1F,KAGAuK,kBAAA,WACA,UAEA7D,OAAA,WACA,IAAAC,EAAA,GAKA,OAJAA,EAAA4B,UAAA9C,KAAA8C,UACA5B,EAAAqC,OAAAvD,KAAAuD,OACArC,EAAAsC,OAAAxD,KAAAwD,OACAtC,EAAAuC,WAAAzD,KAAAyD,WACAvC,GAEAC,SAAA,SAAAD,GACAlB,KAAA8C,UAAA5B,EAAA4B,UACA9C,KAAAuD,OAAArC,EAAAqC,OACAvD,KAAAwD,OAAAtC,EAAAsC,OACAxD,KAAAyD,WAAAvC,EAAAuC,aAQA,IAAAyD,EAAA,SAAA1H,GACAA,KAAA,GAGAQ,KAAAmH,gBAAA,IAAA3H,EAAA2H,WAAA3H,EAAA2H,WAAA,EAGAnH,KAAAuD,OAAA/D,EAAAyD,MACAjD,KAAAwD,OAAAhE,EAAA0D,MACAlD,KAAA8C,UAAAhG,KAAAa,MAAA6B,EAAAwD,SAAAhD,KAAAmH,YACAnH,KAAAyD,WAAA,SAEAzD,KAAAoH,SAAA3K,EAAAsB,MAAAiC,KAAAuD,OAAAvD,KAAAwD,OAAAxD,KAAA8C,YAEAoE,EAAA/K,UAAA,CACA0H,QAAA,SAAAhD,EAAAiD,GACA9D,KAAA+D,OAAAlD,EACA,IAAAiG,EAAA9G,KAAA8C,UACA+D,EAAA,IAAAlH,EAAAK,KAAAuD,OAAAvD,KAAAwD,OAAAxD,KAAA8C,UAAA,GAKA,OAAA9C,KAAAuD,QAAA,IAAAvD,KAAAwD,OACA,QAAAjJ,EAAA,EAAoBA,EAAAuM,EAAIvM,IAAA,CAIxB,IAHA,IAAA8F,EAAA9F,EAAAyF,KAAAmH,WACA/J,EAAAyD,EAAAxC,EAAAgC,GACAgH,EAAA,EACAxI,EAAA,EAAsBA,EAAAmB,KAAAmH,WAAkBtI,IAAA,EACxCyI,EAAAzG,EAAAxC,EAAAgC,EAAAxB,IACAzB,IACAA,EAAAkK,EACAD,EAAAxI,GAGAgI,EAAAxI,EAAA9D,GAAA6C,EACA4C,KAAAoH,SAAA7M,GAAA8F,EAAAgH,OAIA,IADA,IAAArL,EAAA,EACAmE,EAAA,EAAoBA,EAAAU,EAAAjB,GAAOO,IAC3B,QAAAC,EAAA,EAAsBA,EAAAS,EAAAhB,GAAOO,IAC7B,IAAA7F,EAAA,EAAwBA,EAAAuM,EAAIvM,IAAA,CAI5B,IAHA8F,EAAA9F,EAAAyF,KAAAmH,WACA/J,EAAAyD,EAAAzF,IAAA+E,EAAAC,EAAAC,GACAgH,EAAA,EACAxI,EAAA,EAA0BA,EAAAmB,KAAAmH,WAAkBtI,IAAA,CAC5C,IAAAyI,KAAAzG,EAAAzF,IAAA+E,EAAAC,EAAAC,EAAAxB,IACAzB,IACAA,EAAAkK,EACAD,EAAAxI,GAGAgI,EAAAvG,IAAAH,EAAAC,EAAA7F,EAAA6C,GACA4C,KAAAoH,SAAApL,GAAAqE,EAAAgH,EACArL,IAOA,OADAgE,KAAAyE,QAAAoC,EACA7G,KAAAyE,SAEAC,SAAA,WACA,IAAA7D,EAAAb,KAAA+D,OACA8C,EAAA7G,KAAAyE,QACAqC,EAAA9G,KAAA8C,UAIA,GAHAjC,EAAAZ,GAAAxD,EAAAsB,MAAA8C,EAAAxC,EAAAZ,QAGA,IAAAuC,KAAAuD,QAAA,IAAAvD,KAAAwD,OACA,QAAAjJ,EAAA,EAAoBA,EAAAuM,EAAIvM,IAAA,CACxB,IAAAoK,EAAAkC,EAAA5G,GAAA1F,GACAsG,EAAAZ,GAAAD,KAAAoH,SAAA7M,IAAAoK,OAKA,IADA,IAAA3I,EAAA,EACAmE,EAAA,EAAoBA,EAAA0G,EAAAjH,GAAQO,IAC5B,QAAAC,EAAA,EAAsBA,EAAAyG,EAAAhH,GAAQO,IAC9B,IAAA7F,EAAA,EAAwBA,EAAAuM,EAAIvM,IAAA,CAC5BoK,EAAAkC,EAAArG,SAAAL,EAAAC,EAAA7F,GACAsG,EAAAJ,SAAAN,EAAAC,EAAAJ,KAAAoH,SAAApL,GAAA2I,GACA3I,MAMA8I,kBAAA,WACA,UAEA7D,OAAA,WACA,IAAAC,EAAA,GAMA,OALAA,EAAA4B,UAAA9C,KAAA8C,UACA5B,EAAAqC,OAAAvD,KAAAuD,OACArC,EAAAsC,OAAAxD,KAAAwD,OACAtC,EAAAuC,WAAAzD,KAAAyD,WACAvC,EAAAiG,WAAAnH,KAAAmH,WACAjG,GAEAC,SAAA,SAAAD,GACAlB,KAAA8C,UAAA5B,EAAA4B,UACA9C,KAAAuD,OAAArC,EAAAqC,OACAvD,KAAAwD,OAAAtC,EAAAsC,OACAxD,KAAAyD,WAAAvC,EAAAuC,WACAzD,KAAAmH,WAAAjG,EAAAiG,WACAnH,KAAAoH,SAAA3K,EAAAsB,MAAAiC,KAAAmH,cAYA,IAAAI,EAAA,SAAA/H,GACAA,KAAA,GAGAQ,KAAAuD,OAAA/D,EAAAyD,MACAjD,KAAAwD,OAAAhE,EAAA0D,MACAlD,KAAA8C,UAAAtD,EAAAwD,SACAhD,KAAAyD,WAAA,QAEA8D,EAAApL,UAAA,CACA0H,QAAA,SAAAhD,EAAAiD,GACA9D,KAAA+D,OAAAlD,EAGA,IAFA,IAnBAV,EACAC,EAkBAyG,EAAAhG,EAAAF,eACAmG,EAAAjG,EAAAxC,EAAAZ,OACAlD,EAAA,EAAkBA,EAAAuM,EAAIvM,IACtBsM,EAAAxI,EAAA9D,IAtBA4F,EAsBAU,EAAAxC,EAAA9D,GArBA6F,aAAAtD,KAAAqJ,IAAA,EAAAhG,IACA,IAAAC,EAAA,IAuBA,OADAJ,KAAAyE,QAAAoC,EACA7G,KAAAyE,SAEAC,SAAA,WACA,IAAA7D,EAAAb,KAAA+D,OACA8C,EAAA7G,KAAAyE,QACAqC,EAAAjG,EAAAxC,EAAAZ,OACAoD,EAAAZ,GAAAxD,EAAAsB,MAAA+I,GACA,QAAAvM,EAAA,EAAkBA,EAAAuM,EAAIvM,IAAA,CACtB,IAAA0M,EAAAJ,EAAAxI,EAAA9D,GACAsG,EAAAZ,GAAA1F,IAAA,EAAA0M,KAAAJ,EAAA5G,GAAA1F,KAGAuK,kBAAA,WACA,UAEA7D,OAAA,WACA,IAAAC,EAAA,GAKA,OAJAA,EAAA4B,UAAA9C,KAAA8C,UACA5B,EAAAqC,OAAAvD,KAAAuD,OACArC,EAAAsC,OAAAxD,KAAAwD,OACAtC,EAAAuC,WAAAzD,KAAAyD,WACAvC,GAEAC,SAAA,SAAAD,GACAlB,KAAA8C,UAAA5B,EAAA4B,UACA9C,KAAAuD,OAAArC,EAAAqC,OACAvD,KAAAwD,OAAAtC,EAAAsC,OACAxD,KAAAyD,WAAAvC,EAAAuC,aAIAhH,EAAA8K,YACA9K,EAAAyK,cACAzK,EAAAmK,YACAnK,EAAAuK,eA/RA,CAiSCzK,GAED,SAAAE,GACA,aACAA,EAAAkD,IAAA,IAQA6H,EAAA,SAAAhI,GACAA,KAAA,GAGAQ,KAAAuD,OAAA/D,EAAAyD,MACAjD,KAAAwD,OAAAhE,EAAA0D,MACAlD,KAAA8C,UAAAtD,EAAAwD,SACAhD,KAAAyD,WAAA,UACAzD,KAAAyH,eAAA,IAAAjI,EAAAiI,UAAAjI,EAAAiI,UAAA,GACAzH,KAAA0H,QAAAjL,EAAAsB,MAAAiC,KAAAuD,OAAAvD,KAAAwD,OAAAxD,KAAA8C,YAEA0E,EAAArL,UAAA,CACA0H,QAAA,SAAAhD,EAAAiD,GACA9D,KAAA+D,OAAAlD,OACA,QAA6CiD,GAAA,GAC7C,IAAA+C,EAAAhG,EAAAD,QACAkG,EAAAjG,EAAAxC,EAAAZ,OACA,GAAAqG,EAEA,QAAAvJ,EAAA,EAAoBA,EAAAuM,EAAIvM,IACxBuC,KAAAC,SAAAiD,KAAAyH,WAA4CZ,EAAAxI,EAAA9D,GAAA,EAAWyF,KAAA0H,QAAAnN,IAAA,GACvCyF,KAAA0H,QAAAnN,IAAA,OAIhB,IAAAA,EAAA,EAAoBA,EAAAuM,EAAIvM,IAAMsM,EAAAxI,EAAA9D,IAAAyF,KAAAyH,UAG9B,OADAzH,KAAAyE,QAAAoC,EACA7G,KAAAyE,SAEAC,SAAA,WACA,IAAA7D,EAAAb,KAAA+D,OACAY,EAAA3E,KAAAyE,QACAqC,EAAAjG,EAAAxC,EAAAZ,OACAoD,EAAAZ,GAAAxD,EAAAsB,MAAA+I,GACA,QAAAvM,EAAA,EAAkBA,EAAAuM,EAAIvM,IACtByF,KAAA0H,QAAAnN,KACAsG,EAAAZ,GAAA1F,GAAAoK,EAAA1E,GAAA1F,KAIAuK,kBAAA,WACA,UAEA7D,OAAA,WACA,IAAAC,EAAA,GAMA,OALAA,EAAA4B,UAAA9C,KAAA8C,UACA5B,EAAAqC,OAAAvD,KAAAuD,OACArC,EAAAsC,OAAAxD,KAAAwD,OACAtC,EAAAuC,WAAAzD,KAAAyD,WACAvC,EAAAuG,UAAAzH,KAAAyH,UACAvG,GAEAC,SAAA,SAAAD,GACAlB,KAAA8C,UAAA5B,EAAA4B,UACA9C,KAAAuD,OAAArC,EAAAqC,OACAvD,KAAAwD,OAAAtC,EAAAsC,OACAxD,KAAAyD,WAAAvC,EAAAuC,WACAzD,KAAAyH,UAAAvG,EAAAuG,YAKAhL,EAAA+K,eAzEA,CA0ECjL,GACD,SAAAE,GACA,aACAA,EAAAkD,IAAA,IAKAgI,EAAA,SAAAnI,GACAA,KAAA,GAGAQ,KAAAZ,EAAAI,EAAAJ,EACAY,KAAAhE,EAAAwD,EAAAxD,EACAgE,KAAA4H,MAAApI,EAAAoI,MACA5H,KAAA6H,KAAArI,EAAAqI,KAGA7H,KAAAuD,OAAA/D,EAAAyD,MACAjD,KAAAwD,OAAAhE,EAAA0D,MACAlD,KAAA8C,UAAAtD,EAAAwD,SACAhD,KAAAyD,WAAA,MAGAzD,KAAAhE,EAAA,MAAwB8L,QAAA5K,IAAA,0CAExByK,EAAAxL,UAAA,CACA0H,QAAA,SAAAhD,EAAAiD,GACA9D,KAAA+D,OAAAlD,EAEA,IAAAmD,EAAAnD,EAAAF,eACAX,KAAA+H,SAAAlH,EAAAF,eAEA,IADA,IAAAqH,EAAAlL,KAAAa,MAAAqC,KAAAhE,EAAA,GACAmE,EAAA,EAAkBA,EAAAU,EAAAjB,GAAOO,IACzB,QAAAC,EAAA,EAAoBA,EAAAS,EAAAhB,GAAOO,IAC3B,QAAA7F,EAAA,EAAsBA,EAAAsG,EAAAf,MAAUvF,IAAA,CAMhC,IAJA,IAAA8M,EAAAxG,EAAAzF,IAAA+E,EAAAC,EAAA7F,GAGA0N,EAAA,EACApJ,EAAA/B,KAAAoL,IAAA,EAAA3N,EAAAyN,GAAuCnJ,GAAA/B,KAAAqL,IAAA5N,EAAAyN,EAAAnH,EAAAf,MAAA,GAA4BjB,IAAA,CACnE,IAAAuJ,EAAAvH,EAAAzF,IAAA+E,EAAAC,EAAAvB,GACAoJ,GAAAG,IAEAH,GAAAjI,KAAA4H,MAAA5H,KAAAhE,EACAiM,GAAAjI,KAAAZ,EACAY,KAAA+H,SAAAzH,IAAAH,EAAAC,EAAA7F,EAAA0N,GACAA,EAAAnL,KAAAuL,IAAAJ,EAAAjI,KAAA6H,MACA7D,EAAA1D,IAAAH,EAAAC,EAAA7F,EAAA8M,EAAAY,GAMA,OADAjI,KAAAyE,QAAAT,EACAhE,KAAAyE,SAEAC,SAAA,WAEA,IAAA7D,EAAAb,KAAA+D,OACAlD,EAAAZ,GAAAxD,EAAAsB,MAAA8C,EAAAxC,EAAAZ,QACAuC,KAAAyE,QAGA,IAHA,IAEAuD,EAAAlL,KAAAa,MAAAqC,KAAAhE,EAAA,GACAmE,EAAA,EAAkBA,EAAAU,EAAAjB,GAAOO,IACzB,QAAAC,EAAA,EAAoBA,EAAAS,EAAAhB,GAAOO,IAC3B,QAAA7F,EAAA,EAAsBA,EAAAsG,EAAAf,MAAUvF,IAQhC,IANA,IAAAoK,EAAA3E,KAAAyE,QAAAjE,SAAAL,EAAAC,EAAA7F,GACA+N,EAAAtI,KAAA+H,SAAA3M,IAAA+E,EAAAC,EAAA7F,GACAgO,EAAAzL,KAAAuL,IAAAC,EAAAtI,KAAA6H,MACAW,EAAAD,IAGA1J,EAAA/B,KAAAoL,IAAA,EAAA3N,EAAAyN,GAAuCnJ,GAAA/B,KAAAqL,IAAA5N,EAAAyN,EAAAnH,EAAAf,MAAA,GAA4BjB,IAAA,CACnE,IAAA4J,EAAA5H,EAAAzF,IAAA+E,EAAAC,EAAAvB,GACA6J,GAAAD,EAAAzI,KAAA6H,KAAA/K,KAAAuL,IAAAC,EAAAtI,KAAA6H,KAAA,GAAA7H,KAAA4H,MAAA5H,KAAAhE,EAAA,EAAAyM,EACA5J,IAAAtE,IAAAmO,GAAAH,GACAG,GAAAF,EACAE,GAAA/D,EACA9D,EAAAH,SAAAP,EAAAC,EAAAvB,EAAA6J,KAOA5D,kBAAA,WAAmC,UACnC7D,OAAA,WACA,IAAAC,EAAA,GASA,OARAA,EAAA9B,EAAAY,KAAAZ,EACA8B,EAAAlF,EAAAgE,KAAAhE,EACAkF,EAAA0G,MAAA5H,KAAA4H,MACA1G,EAAA2G,KAAA7H,KAAA6H,KACA3G,EAAAqC,OAAAvD,KAAAuD,OACArC,EAAAsC,OAAAxD,KAAAwD,OACAtC,EAAA4B,UAAA9C,KAAA8C,UACA5B,EAAAuC,WAAAzD,KAAAyD,WACAvC,GAEAC,SAAA,SAAAD,GACAlB,KAAAZ,EAAA8B,EAAA9B,EACAY,KAAAhE,EAAAkF,EAAAlF,EACAgE,KAAA4H,MAAA1G,EAAA0G,MACA5H,KAAA6H,KAAA3G,EAAA2G,KACA7H,KAAAuD,OAAArC,EAAAqC,OACAvD,KAAAwD,OAAAtC,EAAAsC,OACAxD,KAAA8C,UAAA5B,EAAA4B,UACA9C,KAAAyD,WAAAvC,EAAAuC,aAKAhH,EAAAkL,kCAhHA,CAiHCpL,GACD,SAAAE,GACA,aACA,IAAAkD,EAAAlD,EAAAkD,IAIAgJ,EAAA,SAAAnJ,GACAA,KAAA,GAGAQ,KAAAuD,OAAA/D,EAAAyD,MACAjD,KAAAwD,OAAAhE,EAAA0D,MAMAlD,KAAA8C,UAAAtD,EAAAwD,SAAAxD,EAAAwD,SAAAxD,EAAAwD,SACAhD,KAAAyD,WAAA,iBAGAkF,EAAAxM,UAAA,CACA0H,QAAA,SAAAhD,EAAAiD,GACA9D,KAAA+D,OAAAlD,EAIA,IAHA,IAAAiG,EAAA9G,KAAA8C,UACA8F,EAAA/H,EAAAf,MACA+G,EAAA,IAAAlH,EAAAK,KAAAuD,OAAAvD,KAAAwD,OAAAxD,KAAA8C,UAAA,GACA3C,EAAA,EAAkBA,EAAAU,EAAAjB,GAAOO,IACzB,QAAAC,EAAA,EAAoBA,EAAAS,EAAAhB,GAAOO,IAC3B,QAAA7F,EAAA,EAAsBA,EAAAuM,EAAIvM,IAC1B,GAAAA,EAAAqO,EACA/B,EAAAvG,IAAAH,EAAAC,EAAA7F,EAAAsG,EAAAzF,IAAA+E,EAAAC,EAAA7F,QACa,CACb,IAAAsO,EAAA/L,KAAAa,OAAApD,EAAAqO,MACAE,EAAAvO,EAAAqO,EAAAC,EAAAD,EACA/B,EAAAvG,IAAAH,EAAAC,EAAA7F,EAAAsG,EAAAzF,IAAA+E,EAAAC,EAAAyI,GAAAhI,EAAAzF,IAAA+E,EAAAC,EAAA0I,IAMA,OADA9I,KAAAyE,QAAAoC,EACA7G,KAAAyE,SAEAC,SAAA,WACA,IAAA7D,EAAAb,KAAA+D,OACAlD,EAAAZ,GAAAxD,EAAAsB,MAAA8C,EAAAxC,EAAAZ,QAIA,IAHA,IAAAoJ,EAAA7G,KAAAyE,QACAqC,EAAA9G,KAAA8C,UACA8F,EAAA/H,EAAAf,MACAK,EAAA,EAAkBA,EAAAU,EAAAjB,GAAOO,IACzB,QAAAC,EAAA,EAAoBA,EAAAS,EAAAhB,GAAOO,IAC3B,QAAA7F,EAAA,EAAsBA,EAAAuM,EAAIvM,IAAA,CAC1B,IAAAoK,EAAAkC,EAAArG,SAAAL,EAAAC,EAAA7F,GACA,GAAAA,EAAAqO,EACA/H,EAAAH,SAAAP,EAAAC,EAAA7F,EAAAoK,OACa,CACb,IAAAkE,EAAA/L,KAAAa,OAAApD,EAAAqO,MACAE,EAAAvO,EAAAqO,EAAAC,EAAAD,EACA/H,EAAAH,SAAAP,EAAAC,EAAAyI,EAAAhI,EAAAzF,IAAA+E,EAAAC,EAAA0I,GAAAnE,GACA9D,EAAAH,SAAAP,EAAAC,EAAA0I,EAAAjI,EAAAzF,IAAA+E,EAAAC,EAAAyI,GAAAlE,MAMAG,kBAAA,WACA,UAEA7D,OAAA,WACA,IAAAC,EAAA,GAKA,OAJAA,EAAA4B,UAAA9C,KAAA8C,UACA5B,EAAAqC,OAAAvD,KAAAuD,OACArC,EAAAsC,OAAAxD,KAAAwD,OACAtC,EAAAuC,WAAAzD,KAAAyD,WACAvC,GAEAC,SAAA,SAAAD,GACAlB,KAAA8C,UAAA5B,EAAA4B,UACA9C,KAAAuD,OAAArC,EAAAqC,OACAvD,KAAAwD,OAAAtC,EAAAsC,OACAxD,KAAAyD,WAAAvC,EAAAuC,aAKAhH,EAAAkM,qBArFA,CAsFCpM,GACD,SAAAE,GACA,aACAA,EAAAkD,IAAA,IAIAoJ,EAAA,SAAAC,GACAhJ,KAAAiJ,OAAA,IAGAF,EAAA5M,UAAA,CAGA+M,WAAA,SAAAC,GAGAA,EAAA1L,OAAA,GAAyBqK,QAAA5K,IAAA,0DACzB,UAAAiM,EAAA,GAAAC,MAAoCtB,QAAA5K,IAAA,+CA0DpCiM,EAvDA,WAEA,IADA,IAAAE,EAAA,GACA9O,EAAA,EAAoBA,EAAA4O,EAAA1L,OAAclD,IAAA,CAClC,IAAA+O,EAAAH,EAAA5O,GAkCA,GAhCA,YAAA+O,EAAAF,MAAA,QAAAE,EAAAF,MAGAC,EAAA/J,KAAA,CAA2B8J,KAAA,KAAAjE,YAAAmE,EAAAC,cAG3B,eAAAD,EAAAF,MAGAC,EAAA/J,KAAA,CAA2B8J,KAAA,KAAAjE,YAAAmE,EAAAnE,cAG3B,OAAAmE,EAAAF,MAAA,SAAAE,EAAAF,WACA,IAAAE,EAAA,YACAA,EAAA3F,UAAA,OACA,IAAA2F,EAAAE,YAAA,SAAAF,EAAAE,aACAF,EAAA3F,UAAA,UAMA,IAAA2F,EAAAG,QAGAH,EAAAG,QACAJ,EAAA/J,KAAA,CAA6B8J,KAAA,kBAI7BC,EAAA/J,KAAAgK,QAEA,IAAAA,EAAAE,WACA,YAAAF,EAAAE,WAAyCH,EAAA/J,KAAA,CAAgB8J,KAAA,cACzD,eAAAE,EAAAE,WAAkDH,EAAA/J,KAAA,CAAgB8J,KAAA,iBAClE,YAAAE,EAAAE,WAA+CH,EAAA/J,KAAA,CAAgB8J,KAAA,cAC/D,cAAAE,EAAAE,WAAA,CAEA,IAAAE,EAAA,cAAAJ,EAAAnC,WAAAmC,EAAAnC,WAAA,EACAkC,EAAA/J,KAAA,CAA6B8J,KAAA,SAAAjC,WAAAuC,SAEX5B,QAAA5K,IAAA,gCAAAoM,EAAAE,iBAElB,IAAAF,EAAA7B,WAAA,YAAA6B,EAAAF,MACAC,EAAA/J,KAAA,CAA2B8J,KAAA,UAAA3B,UAAA6B,EAAA7B,YAI3B,OAAA4B,EAEAM,GAGA3J,KAAAiJ,OAAA,GACA,QAAA1O,EAAA,EAAkBA,EAAA4O,EAAA1L,OAAclD,IAAA,CAChC,IAAA+O,EAAAH,EAAA5O,GACA,GAAAA,EAAA,GACA,IAAAqP,EAAA5J,KAAAiJ,OAAA1O,EAAA,GACA+O,EAAArG,MAAA2G,EAAArG,OACA+F,EAAApG,MAAA0G,EAAApG,OACA8F,EAAAtG,SAAA4G,EAAA9G,UAGA,OAAAwG,EAAAF,MACA,SAAApJ,KAAAiJ,OAAA3J,KAAA,IAAA7C,EAAAyI,eAAAoE,IAAsE,MACtE,UAAAtJ,KAAAiJ,OAAA3J,KAAA,IAAA7C,EAAAkL,gCAAA2B,IAAwF,MACxF,cAAAtJ,KAAAiJ,OAAA3J,KAAA,IAAA7C,EAAA+K,aAAA8B,IAAyE,MACzE,YAAAtJ,KAAAiJ,OAAA3J,KAAA,IAAA7C,EAAAoJ,WAAAyD,IAAqE,MACrE,cAAAtJ,KAAAiJ,OAAA3J,KAAA,IAAA7C,EAAAqJ,aAAAwD,IAAyE,MACzE,iBAAAtJ,KAAAiJ,OAAA3J,KAAA,IAAA7C,EAAA4J,gBAAAiD,IAA+E,MAC/E,WAAAtJ,KAAAiJ,OAAA3J,KAAA,IAAA7C,EAAAoG,UAAAyG,IAAmE,MACnE,WAAAtJ,KAAAiJ,OAAA3J,KAAA,IAAA7C,EAAA+I,UAAA8D,IAAmE,MACnE,WAAAtJ,KAAAiJ,OAAA3J,KAAA,IAAA7C,EAAAmK,UAAA0C,IAAmE,MACnE,cAAAtJ,KAAAiJ,OAAA3J,KAAA,IAAA7C,EAAAuK,aAAAsC,IAAyE,MACzE,WAAAtJ,KAAAiJ,OAAA3J,KAAA,IAAA7C,EAAA8K,UAAA+B,IAAmE,MACnE,aAAAtJ,KAAAiJ,OAAA3J,KAAA,IAAA7C,EAAAyK,YAAAoC,IAAuE,MACvE,oBAAAtJ,KAAAiJ,OAAA3J,KAAA,IAAA7C,EAAAkM,mBAAAW,IAAqF,MACrF,UAAAtJ,KAAAiJ,OAAA3J,KAAA,IAAA7C,EAAAiK,SAAA4C,IAAiE,MACjE,QAAAxB,QAAA5K,IAAA,sCAMA2G,QAAA,SAAAhD,EAAAiD,QACA,QAAAA,GAAA,GAEA,IADA,IAAA+F,EAAA7J,KAAAiJ,OAAA,GAAApF,QAAAhD,EAAAiD,GACAvJ,EAAA,EAAkBA,EAAAyF,KAAAiJ,OAAAxL,OAAqBlD,IACvCsP,EAAA7J,KAAAiJ,OAAA1O,GAAAsJ,QAAAgG,EAAA/F,GAEA,OAAA+F,GAIAnF,SAAA,SAAAtE,GAGA,IAFA,IAAA0G,EAAA9G,KAAAiJ,OAAAxL,OACA6I,EAAAtG,KAAAiJ,OAAAnC,EAAA,GAAApC,SAAAtE,GACA7F,EAAAuM,EAAA,EAAoBvM,GAAA,EAAKA,IACzByF,KAAAiJ,OAAA1O,GAAAmK,WAEA,OAAA4B,GAEAxB,kBAAA,WAGA,IADA,IAAAC,EAAA,GACAxK,EAAA,EAAkBA,EAAAyF,KAAAiJ,OAAAxL,OAAqBlD,IAEvC,IADA,IAAAuP,EAAA9J,KAAAiJ,OAAA1O,GAAAuK,oBACAjG,EAAA,EAAoBA,EAAAiL,EAAArM,OAAuBoB,IAC3CkG,EAAAzF,KAAAwK,EAAAjL,IAGA,OAAAkG,GAEAgF,cAAA,WAKA,IAJA,IACA1N,EADA2D,KAAAiJ,OAAAjJ,KAAAiJ,OAAAxL,OAAA,GACAgH,QAAApG,EACAC,EAAAjC,EAAA,GACAmC,EAAA,EACAjE,EAAA,EAAkBA,EAAA8B,EAAAoB,OAAWlD,IAC7B8B,EAAA9B,GAAA+D,IAAyBA,EAAAjC,EAAA9B,GAAaiE,EAAAjE,GAEtC,OAAAiE,GAEAyC,OAAA,WAGA,IAFA,IAAAC,EAAA,CACA+H,OAAA,IACA1O,EAAA,EAAkBA,EAAAyF,KAAAiJ,OAAAxL,OAAqBlD,IACvC2G,EAAA+H,OAAA3J,KAAAU,KAAAiJ,OAAA1O,GAAA0G,UAEA,OAAAC,GAEAC,SAAA,SAAAD,GACAlB,KAAAiJ,OAAA,GACA,QAAA1O,EAAA,EAAkBA,EAAA2G,EAAA+H,OAAAxL,OAAqBlD,IAAA,CACvC,IAEAyP,EAFAC,EAAA/I,EAAA+H,OAAA1O,GACAkB,EAAAwO,EAAAxG,WAEA,UAAAhI,IAAyBuO,EAAA,IAAAvN,EAAAoJ,YACzB,SAAApK,IAAwBuO,EAAA,IAAAvN,EAAAmK,WACxB,YAAAnL,IAA2BuO,EAAA,IAAAvN,EAAAuK,cAC3B,SAAAvL,IAAwBuO,EAAA,IAAAvN,EAAA8K,WACxB,YAAA9L,IAA2BuO,EAAA,IAAAvN,EAAA+K,cAC3B,SAAA/L,IAAwBuO,EAAA,IAAAvN,EAAAoG,WACxB,SAAApH,IAAwBuO,EAAA,IAAAvN,EAAA+I,WACxB,QAAA/J,IAAuBuO,EAAA,IAAAvN,EAAAkL,iCACvB,YAAAlM,IAA2BuO,EAAA,IAAAvN,EAAAqJ,cAC3B,eAAArK,IAA8BuO,EAAA,IAAAvN,EAAA4J,iBAC9B,OAAA5K,IAAsBuO,EAAA,IAAAvN,EAAAyI,gBACtB,WAAAzJ,IAA0BuO,EAAA,IAAAvN,EAAAyK,aAC1B,kBAAAzL,IAAiCuO,EAAA,IAAAvN,EAAAkM,oBACjC,QAAAlN,IAAuBuO,EAAA,IAAAvN,EAAAiK,UACvBsD,EAAA7I,SAAA8I,GACAjK,KAAAiJ,OAAA3J,KAAA0K,MAMAvN,EAAAsM,MAvLA,CAwLCxM,GACD,SAAAE,GACA,aACAA,EAAAkD,IAAA,IAEAuK,EAAA,SAAAC,EAAAnB,GAEAhJ,KAAAmK,MAEAnB,KAAA,GACAhJ,KAAAoK,mBAAA,IAAApB,EAAAoB,cAAApB,EAAAoB,cAAA,IACApK,KAAAqK,cAAA,IAAArB,EAAAqB,SAAArB,EAAAqB,SAAA,EACArK,KAAAsK,cAAA,IAAAtB,EAAAsB,SAAAtB,EAAAsB,SAAA,EACAtK,KAAAuK,gBAAA,IAAAvB,EAAAuB,WAAAvB,EAAAuB,WAAA,EACAvK,KAAAwK,YAAA,IAAAxB,EAAAwB,OAAAxB,EAAAwB,OAAA,MAEAxK,KAAAyK,cAAA,IAAAzB,EAAAyB,SAAAzB,EAAAyB,SAAA,GACAzK,KAAA0K,QAAA,IAAA1B,EAAA0B,GAAA1B,EAAA0B,GAAA,IACA1K,KAAA2K,SAAA,IAAA3B,EAAA2B,IAAA3B,EAAA2B,IAAA,KAEA3K,KAAAZ,EAAA,EACAY,KAAA4K,KAAA,GACA5K,KAAA6K,KAAA,IAGAX,EAAA/N,UAAA,CACA2O,MAAA,SAAA3K,EAAAC,GAEA,IAAA2K,GAAA,IAAAC,MAAAC,UACAjL,KAAAmK,IAAAtG,QAAA1D,GAAA,GACA,IACA+K,GADA,IAAAF,MAAAC,UACAF,EAGAI,GADAJ,GAAA,IAAAC,MAAAC,UACAjL,KAAAmK,IAAAzF,SAAAtE,IACAgL,EAAA,EACAC,EAAA,EAEAC,GADA,IAAAN,MAAAC,UACAF,EAGA,GADA/K,KAAAZ,IACAY,KAAAZ,EAAAY,KAAAuK,YAAA,GAEA,IAAAgB,EAAAvL,KAAAmK,IAAArF,oBAGA,OAAA9E,KAAA4K,KAAAnN,SAAA,QAAAuC,KAAAwK,QAAAxK,KAAAyK,SAAA,GAKA,QAAAlQ,EAAA,EAAsBA,EAAAgR,EAAA9N,OAAgBlD,IACtCyF,KAAA4K,KAAAtL,KAAA7C,EAAAsB,MAAAwN,EAAAhR,GAAAyK,OAAAvH,SACA,aAAAuC,KAAAwK,OACAxK,KAAA6K,KAAAvL,KAAA7C,EAAAsB,MAAAwN,EAAAhR,GAAAyK,OAAAvH,SAEAuC,KAAA6K,KAAAvL,KAAA,IAMA,IAAA/E,EAAA,EAAoBA,EAAAgR,EAAA9N,OAAgBlD,IAYpC,IAXA,IAAAiR,EAAAD,EAAAhR,GACA8B,EAAAmP,EAAAxG,OACA0D,EAAA8C,EAAAvG,MAGA3B,OAAA,IAAAkI,EAAAlI,aAAAkI,EAAAlI,aAAA,EACAD,OAAA,IAAAmI,EAAAnI,aAAAmI,EAAAnI,aAAA,EACAiH,EAAAtK,KAAAsK,SAAAhH,EACA+G,EAAArK,KAAAqK,SAAAhH,EAEAoI,EAAApP,EAAAoB,OACAoB,EAAA,EAAsBA,EAAA4M,EAAO5M,IAAA,CAC7BuM,GAAAd,EAAAjO,EAAAwC,GAAAxC,EAAAwC,GAAA,EACAwM,GAAAhB,EAAAvN,KAAA4O,IAAArP,EAAAwC,IACA,IAAA8M,EAAAtB,GAAAhO,EAAAwC,GAAA,QAGA+M,GAFAtB,EAAAjO,EAAAwC,GAEA8M,EAAAjD,EAAA7J,IAAAmB,KAAAuK,WAEAsB,EAAA7L,KAAA4K,KAAArQ,GACAuR,EAAA9L,KAAA6K,KAAAtQ,GACA,eAAAyF,KAAAwK,OAAA,CAEAqB,EAAAhN,GAAAgN,EAAAhN,GAAA+M,IACA,IAAAtK,GAAAtB,KAAAoK,cAAAtN,KAAAG,KAAA4O,EAAAhN,GAAAmB,KAAA2K,KAAAiB,EACAvP,EAAAwC,IAAAyC,OACa,kBAAAtB,KAAAwK,OAAA,CAIbqB,EAAAhN,GAAAmB,KAAA0K,GAAAmB,EAAAhN,IAAA,EAAAmB,KAAA0K,IAAAkB,IACAtK,GAAAtB,KAAAoK,cAAAtN,KAAAG,KAAA4O,EAAAhN,GAAAmB,KAAA2K,KAAAiB,EACAvP,EAAAwC,IAAAyC,OACa,gBAAAtB,KAAAwK,OAAA,CAEbqB,EAAAhN,GAAAmB,KAAA0K,GAAAmB,EAAAhN,IAAA,EAAAmB,KAAA0K,IAAAkB,IACAtK,GAAAxE,KAAAG,MAAA6O,EAAAjN,GAAAmB,KAAA2K,MAAAkB,EAAAhN,GAAAmB,KAAA2K,MAAAiB,EACAE,EAAAjN,GAAAmB,KAAA0K,GAAAoB,EAAAjN,IAAA,EAAAmB,KAAA0K,IAAApJ,IACAjF,EAAAwC,IAAAyC,OAGA,GAAAtB,KAAAyK,SAAA,GAEAnJ,EAAAtB,KAAAyK,SAAAoB,EAAAhN,GAAAmB,KAAAoK,cAAAwB,EACAC,EAAAhN,GAAAyC,EACAjF,EAAAwC,IAAAyC,OAGAjF,EAAAwC,KAAAmB,KAAAoK,cAAAwB,EAGAlD,EAAA7J,GAAA,GASA,OAAcqM,WAAAI,WACdF,gBAAAC,gBACAF,YAAAY,aAAAZ,EACA7E,KAAA6E,EAAAE,EAAAD,KAIA3O,EAAAyN,UACAzN,EAAAuP,WAAA9B,EAlIA,CAmIC3N,GAED,SAAAE,GACA,aAGA,IAAAU,EAAAV,EAAAU,MACAO,EAAAjB,EAAAiB,MACAqL,EAAAtM,EAAAsM,IACAmB,EAAAzN,EAAAyN,QACA9L,EAAA3B,EAAA2B,OACAO,EAAAlC,EAAAkC,SACAK,EAAAvC,EAAAuC,eACAO,EAAA9C,EAAA8C,OACAF,EAAA5C,EAAA4C,UAUA4M,EAAA,SAAAxJ,EAAAyJ,EAAA1M,GACAA,KAAA,QACA,IAAAiD,IAAqCA,EAAA,SACrC,IAAAyJ,IAAuCA,EAAA,IAGvClM,KAAAyC,OACAzC,KAAAkM,SAGAlM,KAAAmM,YAAA5M,EAAAC,EAAA,kBACAQ,KAAAoM,UAAA7M,EAAAC,EAAA,gBACAQ,KAAAqM,eAAA9M,EAAAC,EAAA,qBAGAQ,KAAAsM,WAAA/M,EAAAC,EAAA,iBAEAQ,KAAAuM,cAAAhN,EAAAC,EAAA,oBAGAQ,KAAAwM,eAAAjN,EAAAC,EAAA,qBACAQ,KAAAyM,eAAAlN,EAAAC,EAAA,sBACAQ,KAAA0M,aAAAnN,EAAAC,EAAA,mBACAQ,KAAA2M,aAAApN,EAAAC,EAAA,kBACAQ,KAAA4M,kBAAArN,EAAAC,EAAA,wBACAQ,KAAA6M,kBAAAtN,EAAAC,EAAA,uBACAQ,KAAA8M,aAAAvN,EAAAC,EAAA,mBACAQ,KAAA+M,aAAAxN,EAAAC,EAAA,mBACAQ,KAAAgN,YAAAzN,EAAAC,EAAA,iBACAQ,KAAAiN,YAAA1N,EAAAC,EAAA,kBAGAQ,KAAAkN,MAAA,GACAlN,KAAAmN,WAAA,GACAnN,KAAAoN,qBAAA,GACApN,KAAAqN,cAAAhO,EAAA6M,GACAlM,KAAAsN,KAAA,EACAtN,KAAAuN,OAAA,EAGAvN,KAAAwN,qBAAA,KACAxN,KAAAyN,sBAAA,KAGAzN,KAAAyC,KAAAhF,OAAA,IACAuC,KAAA0N,cACA1N,KAAA2N,qBAIA1B,EAAA9P,UAAA,CAGAuR,YAAA,WACA,IAAA5G,EAAA9G,KAAAyC,KAAAhF,OACAmQ,EAAA9Q,KAAAa,MAAAqC,KAAAmM,YAAArF,GACA9G,KAAAkN,MAAA,GACA,QAAA3S,EAAA,EAAkBA,EAAAyF,KAAAoM,UAAiB7R,IAAA,CACnC,IAAA8B,EAAAsC,EAAAmI,GACA9G,KAAAkN,MAAA5N,KAAA,CAAyBuO,SAAAxR,EAAAyR,MAAA,EAAAF,GAAAG,QAAA1R,EAAAyR,MAAAF,EAAA9G,OAKzBkH,gBAAA,WACA,IAAAC,EAAAjO,KAAAyC,KAAA,GAAApE,EAAAZ,OACA8L,EAAAvJ,KAAAqN,cAAA5P,OAGAyQ,EAAA,GACAA,EAAA5O,KAAA,CAAuB8J,KAAA,QAAA7F,OAAA,EAAAC,OAAA,EAAAV,UAAAmL,IAEvB,IADA,IAAAE,EAAAnP,EAAA,yBACAD,EAAA,EAAkBA,EAAAoP,EAAKpP,IAAA,CACvB,IAAAqP,EAAA1Q,EAAAsC,KAAAgN,YAAAhN,KAAAiN,aACApD,EAAA,yBAAAnM,EAAA,MACA,GAAAP,EAAA,SACA,IAAAkR,EAAAvR,KAAAC,SACAmR,EAAA5O,KAAA,CAA2B8J,KAAA,KAAAjE,YAAAiJ,EAAA5E,WAAAK,EAAApC,UAAA4G,SAE3BH,EAAA5O,KAAA,CAA2B8J,KAAA,KAAAjE,YAAAiJ,EAAA5E,WAAAK,IAG3BqE,EAAA5O,KAAA,CAAuB8J,KAAA,UAAAG,gBACvB,IAAAY,EAAA,IAAApB,EACAoB,EAAAjB,WAAAgF,GAGA,IAKAI,EALAC,EAAA7Q,EAAAsC,KAAAwM,eAAAxM,KAAAyM,gBACA+B,EAAA1R,KAAAuL,IAAA,GAAAlL,EAAA6C,KAAA0M,aAAA1M,KAAA2M,eACA8B,EAAA3R,KAAAuL,IAAA,GAAAlL,EAAA6C,KAAA4M,kBAAA5M,KAAA6M,oBACA6B,EAAAvR,EAAA6C,KAAA8M,aAAA9M,KAAA+M,cACA4B,EAAAxR,EAAA,KAUAyR,EAAA,IAAA1E,EAAAC,EAPAmE,EADAK,EAAA,IACA,CAAuBnE,OAAA,WAAAD,WAAAgE,EAAAjE,SAAAkE,GAChBG,EAAA,IACP,CAAuBnE,OAAA,UAAAJ,cAAAqE,EAAAlE,WAAAgE,EAAAjE,SAAAkE,GAEvB,CAAuBhE,OAAA,MAAAJ,cAAAqE,EAAAhE,SAAAiE,EAAAnE,WAAAgE,EAAAjE,SAAAkE,IAKvBK,EAAA,CACAC,IAAA,GACAC,KAAA,GAKA,OAJAF,EAAAX,aACAW,EAAAP,cACAO,EAAA1E,MACA0E,EAAAD,UACAC,GAIAlB,iBAAA,WACA3N,KAAAmN,WAAA,GACA,QAAA5S,EAAA,EAAkBA,EAAAyF,KAAAqM,eAAsB9R,IAAA,CACxC,IAAAsU,EAAA7O,KAAAgO,kBACAhO,KAAAmN,WAAA7N,KAAAuP,KAIAG,KAAA,WAGAhP,KAAAsN,OAKA,IAFA,IAAA2B,EAAAjP,KAAAkN,MAAAlN,KAAAuN,QACA2B,EAAAD,EAAApB,SAAAnQ,EAAA,EAAAuR,EAAApB,SAAApQ,SACA2B,EAAA,EAAkBA,EAAAY,KAAAmN,WAAA1P,OAAyB2B,IAAA,CAC3C,IAAAe,EAAAH,KAAAyC,KAAAyM,GACA1U,EAAAwF,KAAAkM,OAAAgD,GACAlP,KAAAmN,WAAA/N,GAAAwP,QAAA9D,MAAA3K,EAAA3F,GAIA,IAAA2U,EAAAnP,KAAAsM,WAAA2C,EAAApB,SAAApQ,OACA,GAAAuC,KAAAsN,MAAA6B,EAAA,CAGA,IAAAC,EAAApP,KAAAqP,gBACA,IAAAjQ,EAAA,EAAoBA,EAAAY,KAAAmN,WAAA1P,OAAyB2B,IAAA,EAC7CxE,EAAAoF,KAAAmN,WAAA/N,IACA0P,IAAAxP,KAAA8P,EAAAhQ,IACAxE,EAAAmU,MAAAK,EAAAhQ,GASA,GAPAY,KAAAsN,KAAA,EACAtN,KAAAuN,SAEA,OAAAvN,KAAAwN,sBACAxN,KAAAwN,uBAGAxN,KAAAuN,QAAAvN,KAAAkN,MAAAzP,OAAA,CAGA,IAAA2B,EAAA,EAAsBA,EAAAY,KAAAmN,WAAA1P,OAAyB2B,IAC/CY,KAAAoN,qBAAA9N,KAAAU,KAAAmN,WAAA/N,IAGAY,KAAAoN,qBAAAkC,KAAA,SAAAlS,EAAAC,GACA,OAAAD,EAAA2R,KAAA3R,EAAA0R,IAAArR,OACAJ,EAAA0R,KAAA1R,EAAAyR,IAAArR,QACA,MAKAuC,KAAAoN,qBAAA3P,OAAA,EAAAuC,KAAAuM,gBACAvM,KAAAoN,qBAAApN,KAAAoN,qBAAAU,MAAA,IAAA9N,KAAAuM,gBAEA,OAAAvM,KAAAyN,uBACAzN,KAAAyN,wBAEAzN,KAAA2N,mBACA3N,KAAAuN,OAAA,OAGA,IAAAnO,EAAA,EAAsBA,EAAAY,KAAAmN,WAAA1P,OAAyB2B,IAAA,CAC/C,IAAAxE,EAAAoF,KAAAmN,WAAA/N,GACA+K,EAAA,IAAApB,EACAoB,EAAAjB,WAAAtO,EAAAsT,YACA,IAAAU,EAAA,IAAA1E,EAAAC,EAAAvP,EAAA0T,aACA1T,EAAAuP,MACAvP,EAAAgU,aAMAS,cAAA,WAKA,IAFA,IAAAE,EAAA,GACAN,EAAAjP,KAAAkN,MAAAlN,KAAAuN,QACAnO,EAAA,EAAkBA,EAAAY,KAAAmN,WAAA1P,OAAyB2B,IAAA,CAG3C,IAFA,IAAA+K,EAAAnK,KAAAmN,WAAA/N,GAAA+K,IACAnN,EAAA,EACA+B,EAAA,EAAoBA,EAAAkQ,EAAAlB,QAAAtQ,OAAsBsB,IAAA,CAC1C,IAAAoB,EAAAH,KAAAyC,KAAAwM,EAAAlB,QAAAhP,IACAvE,EAAAwF,KAAAkM,OAAA+C,EAAAlB,QAAAhP,IACAoL,EAAAtG,QAAA1D,GAEAnD,GADAmN,EAAAJ,kBACAvP,EAAA,IAEAwC,GAAAiS,EAAAlB,QAAAtQ,OACA8R,EAAAjQ,KAAAtC,GAEA,OAAAuS,GAMAC,aAAA,SAAA/M,GAGA,IAEAgN,EAAAzT,EAFA0T,EAAA5S,KAAAqL,IAAAnI,KAAAuM,cAAAvM,KAAAoN,qBAAA3P,QACA,OAAAiS,EAAoB,WAAAnT,EAAAoD,IAAA,OAEpB,QAAAd,EAAA,EAAkBA,EAAA6Q,EAAK7Q,IAAA,CACvB,IACAsB,EADAH,KAAAoN,qBAAAvO,GAAAsL,IACAtG,QAAApB,GACA,OAAA5D,EACA4Q,EAAAtP,EACAnE,EAAAmE,EAAA9B,EAAAZ,YAGA,QAAA5C,EAAA,EAAsBA,EAAAmB,EAAInB,IAC1B4U,EAAApR,EAAAxD,IAAAsF,EAAA9B,EAAAxD,GAKA,IAAAA,EAAA,EAAkBA,EAAAmB,EAAInB,IACtB4U,EAAApR,EAAAxD,IAAAmB,EAEA,OAAAyT,GAGAE,QAAA,SAAAlN,GACA,IAAAgN,EAAAzP,KAAAwP,aAAA/M,GACA,OAAAgN,EAAApR,EAAAZ,OACA,IACAmS,EADAxR,EAAAqR,EAAApR,GACAG,UAEAoR,GAAA,EAEA,OAAAA,GAIA3O,OAAA,WAKA,IAHA,IAAAyO,EAAA5S,KAAAqL,IAAAnI,KAAAuM,cAAAvM,KAAAoN,qBAAA3P,QACAyD,EAAA,CACA2O,KAAA,IACAtV,EAAA,EAAkBA,EAAAmV,EAAKnV,IACvB2G,EAAA2O,KAAAvQ,KAAAU,KAAAoN,qBAAA7S,GAAA4P,IAAAlJ,UAEA,OAAAC,GAGAC,SAAA,SAAAD,GACAlB,KAAAuM,cAAArL,EAAA2O,KAAApS,OACAuC,KAAAoN,qBAAA,GACA,QAAA7S,EAAA,EAAkBA,EAAAyF,KAAAuM,cAAqBhS,IAAA,CACvC,IAAA4P,EAAA,IAAApB,EACAoB,EAAAhJ,SAAAD,EAAA2O,KAAAtV,IACA,IAAAuV,EAAA,GACAA,EAAA3F,MACAnK,KAAAoN,qBAAA9N,KAAAwQ,KAMAC,aAAA,SAAA9L,GAA+BjE,KAAAwN,qBAAAvJ,GAE/B+L,cAAA,SAAA/L,GAAgCjE,KAAAyN,sBAAAxJ,IAIhCxH,EAAAwP,WAlTA,CAmTC1P,GACD,SAAA0T,GACA,kBACmC,IAAA3V,EAAAD,QACnC6V,OAAAC,OAAAF,EAEA3V,EAAAD,QAAA4V,EALA,CAOC1T,iCCzoEM,IAAK6T,EAMAC,kBANAD,gKAMAC,mDAIL,IAqPKC,EAzOCC,EAAe,EAAE,EAAG,aAyOrBD,gICtQZ,IAKIE,EAOArG,EACAyE,EAbErS,EAAYkU,EAAa,GACzBtO,EAAauO,KACbC,EAAmBD,KAAKE,YAE1BnO,EAAoB,GAGpBoO,GAAyB,EACzBC,EDyL0C,CAAC,EAAG,GCxL9CC,EAA6B,IAC7B3G,ED6CiC,ICxCrCjI,EAAI6O,iBAAiB,UAAW,SAACC,GAC/B,IAAMC,EAA4BC,KAAKC,MAAMH,EAAQxO,MAYrD,GAT0B,YAAtByO,EAAa9H,OACW,UAAtB8H,EAAa9H,OACfyH,EAAiBK,EAAaL,gBAAkBA,GAElDE,EAAsBG,EAAaH,qBAAuBA,EAC1D3G,EAAgB8G,EAAa9G,eAAiBA,GAItB,YAAtB8G,EAAa9H,MACQ,UAAtB8H,EAAa9H,MAAoB8H,EAAaG,aAAe,CAC9D5O,EAAOyO,EAAazO,MAAQA,EAC5BqO,EAAgBI,EAAaJ,eAAiBA,EAC9CN,EAAYU,EAAaV,WAAaA,EACtC,IAAMvH,EAAS,GACfA,EAAO3J,KAAK,CAAE8J,KAAM,QAAS7F,OAAQ,EAAGC,OAAQ,EAAGV,UAAW0N,IAC9DM,EAAcQ,QAAQ,SAACnM,GACrB8D,EAAO3J,KAAK,CAAE8J,KAAM,KAAMjE,cAAaqE,WAAY,WAErDP,EAAO3J,KAAK,CAAE8J,KAAM,UAAWG,YAAa9G,EAAKhF,UAGjD0M,EAAM,IAAI5N,EAAUwM,KAChBG,WAAWD,GAIjB,GAA0B,YAAtBiI,EAAa9H,MAAsB8H,EAAaG,aAAc,CAChE,IAAME,EAAmBV,EAMvB,CACErG,OAAQ,MACRJ,gBACAE,SAAUF,EAAgB,GAC1BG,WAAY,IATd,CACEC,OAAQ,UACRF,SAAU,KACVC,WAAY,IAQhBqE,EAAU,IAAIrS,EAAU2N,QAAQC,EAAKoH,GAoCvC,IAhCA,IA6BIjL,EAAO,EACPkL,EAAW,EACXC,EAAe,EACVC,EAAY,EAAGA,GAAmC,YAAtBR,EAAa9H,KAAqB,EAAI2H,GAAsBW,IAC/FjP,EAAK6O,QAAQ,SAACK,EAA2BC,GACvCD,EAAeL,QAAQ,SAACO,GACtB,IAAM1R,EAAI,IAAI5D,EAAUoD,IAAIkS,GACtBC,EAAMlD,EAAQ9D,MAAM3K,EAAGyR,GAC7BtL,GAAQwL,EAAIxL,KACZmL,QAKN,IAzCMM,EACAC,EAwCFC,EAAU3L,EAAOmL,EAErBQ,EAAUA,EAAU,EAAI,MAASA,EAEjC3L,EAAO4L,OAAQD,EAASE,QAAQ,IA7C1BJ,EAAc,EACdC,EAAW,EACfvP,EAAK6O,QAAQ,SAACc,EAAqBR,GACjCQ,EAAoBd,QAAQ,SAACO,GAC3BG,IACA,IAAM7R,EAAI,IAAI5D,EAAUoD,IAAIkS,GACtBQ,EAASlI,EAAItG,QAAQ1D,GAC3B4R,GAAeM,EAAOhU,EAAEuT,OAuC9BJ,EAtBSc,YAAYP,EAAcC,GAAUG,QAAQ,IA8BrD,IAJA,IAAMnD,GAAQuB,EAAa,GAAKA,EAAa,IDhFnB,GCiFpBgC,EAAyB,GAE3BC,EAAW,EACNrS,GAAK,EAAGA,EAAI,EAAGA,EAAI+R,QAAQ/R,EAAI6O,GAAMmD,QAAQ,IAAK,CACzDI,EAAUC,GAAY,GACtB,IAFyD,IAAAC,EAAA,SAEhDrS,GACP,IAAMsS,EAAY,IAAInW,EAAUoD,IAAI,CAACQ,EAAGC,IAClCiS,EAASlI,EAAItG,QAAQ6O,GACvBC,EAAoB,EAClBC,EAAY,EAAIP,EAAOhU,EAAEZ,OAC3BoV,EAAUD,EACdP,EAAOhU,EAAEiT,QAAQ,SAACwB,EAAelB,GAC3BkB,EAASF,IACXD,EAAaf,EACbiB,EAAUC,KAGdD,GAAWA,EAAUD,IAAc,EAAIA,GAEvCL,EAAUC,GAAUO,QAAQ,CAC1BnB,YAAae,EACbE,aAhBKzS,GAAK,EAAGA,EAAI,EAAGA,EAAI8R,QAAQ9R,EAAI4O,GAAMmD,QAAQ,IAAKM,EAAlDrS,GAmBToS,IAGF,IAAMQ,EAA8B,CAClC5J,KAA4B,YAAtB8H,EAAa9H,KAAqB,UAAY,SACpDe,IAAKgH,KAAK8B,UAAU9I,EAAIlJ,UACxBsR,YACAf,WACAlL,QAEFqK,EAAaqC","file":"36cf02fc423b8288e3b7.worker.js","sourcesContent":[" \t// The module cache\n \tvar installedModules = {};\n\n \t// The require function\n \tfunction __webpack_require__(moduleId) {\n\n \t\t// Check if module is in cache\n \t\tif(installedModules[moduleId]) {\n \t\t\treturn installedModules[moduleId].exports;\n \t\t}\n \t\t// Create a new module (and put it into the cache)\n \t\tvar module = installedModules[moduleId] = {\n \t\t\ti: moduleId,\n \t\t\tl: false,\n \t\t\texports: {}\n \t\t};\n\n \t\t// Execute the module function\n \t\tmodules[moduleId].call(module.exports, module, module.exports, __webpack_require__);\n\n \t\t// Flag the module as loaded\n \t\tmodule.l = true;\n\n \t\t// Return the exports of the module\n \t\treturn module.exports;\n \t}\n\n\n \t// expose the modules object (__webpack_modules__)\n \t__webpack_require__.m = modules;\n\n \t// expose the module cache\n \t__webpack_require__.c = installedModules;\n\n \t// define getter function for harmony exports\n \t__webpack_require__.d = function(exports, name, getter) {\n \t\tif(!__webpack_require__.o(exports, name)) {\n \t\t\tObject.defineProperty(exports, name, { enumerable: true, get: getter });\n \t\t}\n \t};\n\n \t// define __esModule on exports\n \t__webpack_require__.r = function(exports) {\n \t\tif(typeof Symbol !== 'undefined' && Symbol.toStringTag) {\n \t\t\tObject.defineProperty(exports, Symbol.toStringTag, { value: 'Module' });\n \t\t}\n \t\tObject.defineProperty(exports, '__esModule', { value: true });\n \t};\n\n \t// create a fake namespace object\n \t// mode & 1: value is a module id, require it\n \t// mode & 2: merge all properties of value into the ns\n \t// mode & 4: return value when already ns object\n \t// mode & 8|1: behave like require\n \t__webpack_require__.t = function(value, mode) {\n \t\tif(mode & 1) value = __webpack_require__(value);\n \t\tif(mode & 8) return value;\n \t\tif((mode & 4) && typeof value === 'object' && value && value.__esModule) return value;\n \t\tvar ns = Object.create(null);\n \t\t__webpack_require__.r(ns);\n \t\tObject.defineProperty(ns, 'default', { enumerable: true, value: value });\n \t\tif(mode & 2 && typeof value != 'string') for(var key in value) __webpack_require__.d(ns, key, function(key) { return value[key]; }.bind(null, key));\n \t\treturn ns;\n \t};\n\n \t// getDefaultExport function for compatibility with non-harmony modules\n \t__webpack_require__.n = function(module) {\n \t\tvar getter = module && module.__esModule ?\n \t\t\tfunction getDefault() { return module['default']; } :\n \t\t\tfunction getModuleExports() { return module; };\n \t\t__webpack_require__.d(getter, 'a', getter);\n \t\treturn getter;\n \t};\n\n \t// Object.prototype.hasOwnProperty.call\n \t__webpack_require__.o = function(object, property) { return Object.prototype.hasOwnProperty.call(object, property); };\n\n \t// __webpack_public_path__\n \t__webpack_require__.p = \"\";\n\n\n \t// Load entry module and return exports\n \treturn __webpack_require__(__webpack_require__.s = 1);\n","var convnetjs = convnetjs || { REVISION: 'ALPHA' };\n(function(global) {\n  \"use strict\";\n\n  // Random number utilities\n  var return_v = false;\n  var v_val = 0.0;\n  var gaussRandom = function() {\n    if(return_v) { \n      return_v = false;\n      return v_val; \n    }\n    var u = 2*Math.random()-1;\n    var v = 2*Math.random()-1;\n    var r = u*u + v*v;\n    if(r == 0 || r > 1) return gaussRandom();\n    var c = Math.sqrt(-2*Math.log(r)/r);\n    v_val = v*c; // cache this\n    return_v = true;\n    return u*c;\n  }\n  var randf = function(a, b) { return Math.random()*(b-a)+a; }\n  var randi = function(a, b) { return Math.floor(Math.random()*(b-a)+a); }\n  var randn = function(mu, std){ return mu+gaussRandom()*std; }\n\n  // Array utilities\n  var zeros = function(n) {\n    if(typeof(n)==='undefined' || isNaN(n)) { return []; }\n    if(typeof ArrayBuffer === 'undefined') {\n      // lacking browser support\n      var arr = new Array(n);\n      for(var i=0;i<n;i++) { arr[i]= 0; }\n      return arr;\n    } else {\n      return new Float64Array(n);\n    }\n  }\n\n  var arrContains = function(arr, elt) {\n    for(var i=0,n=arr.length;i<n;i++) {\n      if(arr[i]===elt) return true;\n    }\n    return false;\n  }\n\n  var arrUnique = function(arr) {\n    var b = [];\n    for(var i=0,n=arr.length;i<n;i++) {\n      if(!arrContains(b, arr[i])) {\n        b.push(arr[i]);\n      }\n    }\n    return b;\n  }\n\n  // return max and min of a given non-empty array.\n  var maxmin = function(w) {\n    if(w.length === 0) { return {}; } // ... ;s\n    var maxv = w[0];\n    var minv = w[0];\n    var maxi = 0;\n    var mini = 0;\n    var n = w.length;\n    for(var i=1;i<n;i++) {\n      if(w[i] > maxv) { maxv = w[i]; maxi = i; } \n      if(w[i] < minv) { minv = w[i]; mini = i; } \n    }\n    return {maxi: maxi, maxv: maxv, mini: mini, minv: minv, dv:maxv-minv};\n  }\n\n  // create random permutation of numbers, in range [0...n-1]\n  var randperm = function(n) {\n    var i = n,\n        j = 0,\n        temp;\n    var array = [];\n    for(var q=0;q<n;q++)array[q]=q;\n    while (i--) {\n        j = Math.floor(Math.random() * (i+1));\n        temp = array[i];\n        array[i] = array[j];\n        array[j] = temp;\n    }\n    return array;\n  }\n\n  // sample from list lst according to probabilities in list probs\n  // the two lists are of same size, and probs adds up to 1\n  var weightedSample = function(lst, probs) {\n    var p = randf(0, 1.0);\n    var cumprob = 0.0;\n    for(var k=0,n=lst.length;k<n;k++) {\n      cumprob += probs[k];\n      if(p < cumprob) { return lst[k]; }\n    }\n  }\n\n  // syntactic sugar function for getting default parameter values\n  var getopt = function(opt, field_name, default_value) {\n    return typeof opt[field_name] !== 'undefined' ? opt[field_name] : default_value;\n  }\n\n  global.randf = randf;\n  global.randi = randi;\n  global.randn = randn;\n  global.zeros = zeros;\n  global.maxmin = maxmin;\n  global.randperm = randperm;\n  global.weightedSample = weightedSample;\n  global.arrUnique = arrUnique;\n  global.arrContains = arrContains;\n  global.getopt = getopt;\n  \n})(convnetjs);\n(function(global) {\n  \"use strict\";\n\n  // Vol is the basic building block of all data in a net.\n  // it is essentially just a 3D volume of numbers, with a\n  // width (sx), height (sy), and depth (depth).\n  // it is used to hold data for all filters, all volumes,\n  // all weights, and also stores all gradients w.r.t. \n  // the data. c is optionally a value to initialize the volume\n  // with. If c is missing, fills the Vol with random numbers.\n  var Vol = function(sx, sy, depth, c) {\n    // this is how you check if a variable is an array. Oh, Javascript :)\n    if(Object.prototype.toString.call(sx) === '[object Array]') {\n      // we were given a list in sx, assume 1D volume and fill it up\n      this.sx = 1;\n      this.sy = 1;\n      this.depth = sx.length;\n      // we have to do the following copy because we want to use\n      // fast typed arrays, not an ordinary javascript array\n      this.w = global.zeros(this.depth);\n      this.dw = global.zeros(this.depth);\n      for(var i=0;i<this.depth;i++) {\n        this.w[i] = sx[i];\n      }\n    } else {\n      // we were given dimensions of the vol\n      this.sx = sx;\n      this.sy = sy;\n      this.depth = depth;\n      var n = sx*sy*depth;\n      this.w = global.zeros(n);\n      this.dw = global.zeros(n);\n      if(typeof c === 'undefined') {\n        // weight normalization is done to equalize the output\n        // variance of every neuron, otherwise neurons with a lot\n        // of incoming connections have outputs of larger variance\n        var scale = Math.sqrt(1.0/(sx*sy*depth));\n        for(var i=0;i<n;i++) { \n          this.w[i] = global.randn(0.0, scale);\n        }\n      } else {\n        for(var i=0;i<n;i++) { \n          this.w[i] = c;\n        }\n      }\n    }\n  }\n\n  Vol.prototype = {\n    get: function(x, y, d) { \n      var ix=((this.sx * y)+x)*this.depth+d;\n      return this.w[ix];\n    },\n    set: function(x, y, d, v) { \n      var ix=((this.sx * y)+x)*this.depth+d;\n      this.w[ix] = v; \n    },\n    add: function(x, y, d, v) { \n      var ix=((this.sx * y)+x)*this.depth+d;\n      this.w[ix] += v; \n    },\n    get_grad: function(x, y, d) { \n      var ix = ((this.sx * y)+x)*this.depth+d;\n      return this.dw[ix]; \n    },\n    set_grad: function(x, y, d, v) { \n      var ix = ((this.sx * y)+x)*this.depth+d;\n      this.dw[ix] = v; \n    },\n    add_grad: function(x, y, d, v) { \n      var ix = ((this.sx * y)+x)*this.depth+d;\n      this.dw[ix] += v; \n    },\n    cloneAndZero: function() { return new Vol(this.sx, this.sy, this.depth, 0.0)},\n    clone: function() {\n      var V = new Vol(this.sx, this.sy, this.depth, 0.0);\n      var n = this.w.length;\n      for(var i=0;i<n;i++) { V.w[i] = this.w[i]; }\n      return V;\n    },\n    addFrom: function(V) { for(var k=0;k<this.w.length;k++) { this.w[k] += V.w[k]; }},\n    addFromScaled: function(V, a) { for(var k=0;k<this.w.length;k++) { this.w[k] += a*V.w[k]; }},\n    setConst: function(a) { for(var k=0;k<this.w.length;k++) { this.w[k] = a; }},\n\n    toJSON: function() {\n      // todo: we may want to only save d most significant digits to save space\n      var json = {}\n      json.sx = this.sx; \n      json.sy = this.sy;\n      json.depth = this.depth;\n      json.w = this.w;\n      return json;\n      // we wont back up gradients to save space\n    },\n    fromJSON: function(json) {\n      this.sx = json.sx;\n      this.sy = json.sy;\n      this.depth = json.depth;\n\n      var n = this.sx*this.sy*this.depth;\n      this.w = global.zeros(n);\n      this.dw = global.zeros(n);\n      // copy over the elements.\n      for(var i=0;i<n;i++) {\n        this.w[i] = json.w[i];\n      }\n    }\n  }\n\n  global.Vol = Vol;\n})(convnetjs);\n(function(global) {\n  \"use strict\";\n  var Vol = global.Vol; // convenience\n\n  // Volume utilities\n  // intended for use with data augmentation\n  // crop is the size of output\n  // dx,dy are offset wrt incoming volume, of the shift\n  // fliplr is boolean on whether we also want to flip left<->right\n  var augment = function(V, crop, dx, dy, fliplr) {\n    // note assumes square outputs of size crop x crop\n    if(typeof(fliplr)==='undefined') var fliplr = false;\n    if(typeof(dx)==='undefined') var dx = global.randi(0, V.sx - crop);\n    if(typeof(dy)==='undefined') var dy = global.randi(0, V.sy - crop);\n    \n    // randomly sample a crop in the input volume\n    var W;\n    if(crop !== V.sx || dx!==0 || dy!==0) {\n      W = new Vol(crop, crop, V.depth, 0.0);\n      for(var x=0;x<crop;x++) {\n        for(var y=0;y<crop;y++) {\n          if(x+dx<0 || x+dx>=V.sx || y+dy<0 || y+dy>=V.sy) continue; // oob\n          for(var d=0;d<V.depth;d++) {\n           W.set(x,y,d,V.get(x+dx,y+dy,d)); // copy data over\n          }\n        }\n      }\n    } else {\n      W = V;\n    }\n\n    if(fliplr) {\n      // flip volume horziontally\n      var W2 = W.cloneAndZero();\n      for(var x=0;x<W.sx;x++) {\n        for(var y=0;y<W.sy;y++) {\n          for(var d=0;d<W.depth;d++) {\n           W2.set(x,y,d,W.get(W.sx - x - 1,y,d)); // copy data over\n          }\n        }\n      }\n      W = W2; //swap\n    }\n    return W;\n  }\n\n  // img is a DOM element that contains a loaded image\n  // returns a Vol of size (W, H, 4). 4 is for RGBA\n  var img_to_vol = function(img, convert_grayscale) {\n\n    if(typeof(convert_grayscale)==='undefined') var convert_grayscale = false;\n\n    var canvas = document.createElement('canvas');\n    canvas.width = img.width;\n    canvas.height = img.height;\n    var ctx = canvas.getContext(\"2d\");\n\n    // due to a Firefox bug\n    try {\n      ctx.drawImage(img, 0, 0);\n    } catch (e) {\n      if (e.name === \"NS_ERROR_NOT_AVAILABLE\") {\n        // sometimes happens, lets just abort\n        return false;\n      } else {\n        throw e;\n      }\n    }\n\n    try {\n      var img_data = ctx.getImageData(0, 0, canvas.width, canvas.height);\n    } catch (e) {\n      if(e.name === 'IndexSizeError') {\n        return false; // not sure what causes this sometimes but okay abort\n      } else {\n        throw e;\n      }\n    }\n\n    // prepare the input: get pixels and normalize them\n    var p = img_data.data;\n    var W = img.width;\n    var H = img.height;\n    var pv = []\n    for(var i=0;i<p.length;i++) {\n      pv.push(p[i]/255.0-0.5); // normalize image pixels to [-0.5, 0.5]\n    }\n    var x = new Vol(W, H, 4, 0.0); //input volume (image)\n    x.w = pv;\n\n    if(convert_grayscale) {\n      // flatten into depth=1 array\n      var x1 = new Vol(W, H, 1, 0.0);\n      for(var i=0;i<W;i++) {\n        for(var j=0;j<H;j++) {\n          x1.set(i,j,0,x.get(i,j,0));\n        }\n      }\n      x = x1;\n    }\n\n    return x;\n  }\n  \n  global.augment = augment;\n  global.img_to_vol = img_to_vol;\n\n})(convnetjs);\n(function(global) {\n  \"use strict\";\n  var Vol = global.Vol; // convenience\n\n  // This file contains all layers that do dot products with input,\n  // but usually in a different connectivity pattern and weight sharing\n  // schemes: \n  // - FullyConn is fully connected dot products \n  // - ConvLayer does convolutions (so weight sharing spatially)\n  // putting them together in one file because they are very similar\n  var ConvLayer = function(opt) {\n    var opt = opt || {};\n\n    // required\n    this.out_depth = opt.filters;\n    this.sx = opt.sx; // filter size. Should be odd if possible, it's cleaner.\n    this.in_depth = opt.in_depth;\n    this.in_sx = opt.in_sx;\n    this.in_sy = opt.in_sy;\n    \n    // optional\n    this.sy = typeof opt.sy !== 'undefined' ? opt.sy : this.sx;\n    this.stride = typeof opt.stride !== 'undefined' ? opt.stride : 1; // stride at which we apply filters to input volume\n    this.pad = typeof opt.pad !== 'undefined' ? opt.pad : 0; // amount of 0 padding to add around borders of input volume\n    this.l1_decay_mul = typeof opt.l1_decay_mul !== 'undefined' ? opt.l1_decay_mul : 0.0;\n    this.l2_decay_mul = typeof opt.l2_decay_mul !== 'undefined' ? opt.l2_decay_mul : 1.0;\n\n    // computed\n    // note we are doing floor, so if the strided convolution of the filter doesnt fit into the input\n    // volume exactly, the output volume will be trimmed and not contain the (incomplete) computed\n    // final application.\n    this.out_sx = Math.floor((this.in_sx + this.pad * 2 - this.sx) / this.stride + 1);\n    this.out_sy = Math.floor((this.in_sy + this.pad * 2 - this.sy) / this.stride + 1);\n    this.layer_type = 'conv';\n\n    // initializations\n    var bias = typeof opt.bias_pref !== 'undefined' ? opt.bias_pref : 0.0;\n    this.filters = [];\n    for(var i=0;i<this.out_depth;i++) { this.filters.push(new Vol(this.sx, this.sy, this.in_depth)); }\n    this.biases = new Vol(1, 1, this.out_depth, bias);\n  }\n  ConvLayer.prototype = {\n    forward: function(V, is_training) {\n      this.in_act = V;\n\n      var A = new Vol(this.out_sx, this.out_sy, this.out_depth, 0.0);\n      for(var d=0;d<this.out_depth;d++) {\n        var f = this.filters[d];\n        var x = -this.pad;\n        var y = -this.pad;\n        for(var ax=0; ax<this.out_sx; x+=this.stride,ax++) {\n          y = -this.pad;\n          for(var ay=0; ay<this.out_sy; y+=this.stride,ay++) {\n\n            // convolve centered at this particular location\n            // could be bit more efficient, going for correctness first\n            var a = 0.0;\n            for(var fx=0;fx<f.sx;fx++) {\n              for(var fy=0;fy<f.sy;fy++) {\n                for(var fd=0;fd<f.depth;fd++) {\n                  var oy = y+fy; // coordinates in the original input array coordinates\n                  var ox = x+fx;\n                  if(oy>=0 && oy<V.sy && ox>=0 && ox<V.sx) {\n                    //a += f.get(fx, fy, fd) * V.get(ox, oy, fd);\n                    // avoid function call overhead for efficiency, compromise modularity :(\n                    a += f.w[((f.sx * fy)+fx)*f.depth+fd] * V.w[((V.sx * oy)+ox)*V.depth+fd];\n                  }\n                }\n              }\n            }\n            a += this.biases.w[d];\n            A.set(ax, ay, d, a);\n          }\n        }\n      }\n      this.out_act = A;\n      return this.out_act;\n    },\n    backward: function() { \n\n      // compute gradient wrt weights, biases and input data\n      var V = this.in_act;\n      V.dw = global.zeros(V.w.length); // zero out gradient wrt bottom data, we're about to fill it\n      for(var d=0;d<this.out_depth;d++) {\n        var f = this.filters[d];\n        var x = -this.pad;\n        var y = -this.pad;\n        for(var ax=0; ax<this.out_sx; x+=this.stride,ax++) {\n          y = -this.pad;\n          for(var ay=0; ay<this.out_sy; y+=this.stride,ay++) {\n            // convolve and add up the gradients. \n            // could be more efficient, going for correctness first\n            var chain_grad = this.out_act.get_grad(ax,ay,d); // gradient from above, from chain rule\n            for(var fx=0;fx<f.sx;fx++) {\n              for(var fy=0;fy<f.sy;fy++) {\n                for(var fd=0;fd<f.depth;fd++) {\n                  var oy = y+fy;\n                  var ox = x+fx;\n                  if(oy>=0 && oy<V.sy && ox>=0 && ox<V.sx) {\n                    // forward prop calculated: a += f.get(fx, fy, fd) * V.get(ox, oy, fd);\n                    //f.add_grad(fx, fy, fd, V.get(ox, oy, fd) * chain_grad);\n                    //V.add_grad(ox, oy, fd, f.get(fx, fy, fd) * chain_grad);\n\n                    // avoid function call overhead and use Vols directly for efficiency\n                    var ix1 = ((V.sx * oy)+ox)*V.depth+fd;\n                    var ix2 = ((f.sx * fy)+fx)*f.depth+fd;\n                    f.dw[ix2] += V.w[ix1]*chain_grad;\n                    V.dw[ix1] += f.w[ix2]*chain_grad;\n                  }\n                }\n              }\n            }\n            this.biases.dw[d] += chain_grad;\n          }\n        }\n      }\n    },\n    getParamsAndGrads: function() {\n      var response = [];\n      for(var i=0;i<this.out_depth;i++) {\n        response.push({params: this.filters[i].w, grads: this.filters[i].dw, l2_decay_mul: this.l2_decay_mul, l1_decay_mul: this.l1_decay_mul});\n      }\n      response.push({params: this.biases.w, grads: this.biases.dw, l1_decay_mul: 0.0, l2_decay_mul: 0.0});\n      return response;\n    },\n    toJSON: function() {\n      var json = {};\n      json.sx = this.sx; // filter size in x, y dims\n      json.sy = this.sy;\n      json.stride = this.stride;\n      json.in_depth = this.in_depth;\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      json.l1_decay_mul = this.l1_decay_mul;\n      json.l2_decay_mul = this.l2_decay_mul;\n      json.pad = this.pad;\n      json.filters = [];\n      for(var i=0;i<this.filters.length;i++) {\n        json.filters.push(this.filters[i].toJSON());\n      }\n      json.biases = this.biases.toJSON();\n      return json;\n    },\n    fromJSON: function(json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type;\n      this.sx = json.sx; // filter size in x, y dims\n      this.sy = json.sy;\n      this.stride = json.stride;\n      this.in_depth = json.in_depth; // depth of input volume\n      this.filters = [];\n      this.l1_decay_mul = typeof json.l1_decay_mul !== 'undefined' ? json.l1_decay_mul : 1.0;\n      this.l2_decay_mul = typeof json.l2_decay_mul !== 'undefined' ? json.l2_decay_mul : 1.0;\n      this.pad = typeof json.pad !== 'undefined' ? json.pad : 0;\n      for(var i=0;i<json.filters.length;i++) {\n        var v = new Vol(0,0,0,0);\n        v.fromJSON(json.filters[i]);\n        this.filters.push(v);\n      }\n      this.biases = new Vol(0,0,0,0);\n      this.biases.fromJSON(json.biases);\n    }\n  }\n\n  var FullyConnLayer = function(opt) {\n    var opt = opt || {};\n\n    // required\n    // ok fine we will allow 'filters' as the word as well\n    this.out_depth = typeof opt.num_neurons !== 'undefined' ? opt.num_neurons : opt.filters;\n\n    // optional \n    this.l1_decay_mul = typeof opt.l1_decay_mul !== 'undefined' ? opt.l1_decay_mul : 0.0;\n    this.l2_decay_mul = typeof opt.l2_decay_mul !== 'undefined' ? opt.l2_decay_mul : 1.0;\n\n    // computed\n    this.num_inputs = opt.in_sx * opt.in_sy * opt.in_depth;\n    this.out_sx = 1;\n    this.out_sy = 1;\n    this.layer_type = 'fc';\n\n    // initializations\n    var bias = typeof opt.bias_pref !== 'undefined' ? opt.bias_pref : 0.0;\n    this.filters = [];\n    for(var i=0;i<this.out_depth ;i++) { this.filters.push(new Vol(1, 1, this.num_inputs)); }\n    this.biases = new Vol(1, 1, this.out_depth, bias);\n  }\n\n  FullyConnLayer.prototype = {\n    forward: function(V, is_training) {\n      this.in_act = V;\n      var A = new Vol(1, 1, this.out_depth, 0.0);\n      var Vw = V.w;\n      for(var i=0;i<this.out_depth;i++) {\n        var a = 0.0;\n        var wi = this.filters[i].w;\n        for(var d=0;d<this.num_inputs;d++) {\n          a += Vw[d] * wi[d]; // for efficiency use Vols directly for now\n        }\n        a += this.biases.w[i];\n        A.w[i] = a;\n      }\n      this.out_act = A;\n      return this.out_act;\n    },\n    backward: function() {\n      var V = this.in_act;\n      V.dw = global.zeros(V.w.length); // zero out the gradient in input Vol\n      \n      // compute gradient wrt weights and data\n      for(var i=0;i<this.out_depth;i++) {\n        var tfi = this.filters[i];\n        var chain_grad = this.out_act.dw[i];\n        for(var d=0;d<this.num_inputs;d++) {\n          V.dw[d] += tfi.w[d]*chain_grad; // grad wrt input data\n          tfi.dw[d] += V.w[d]*chain_grad; // grad wrt params\n        }\n        this.biases.dw[i] += chain_grad;\n      }\n    },\n    getParamsAndGrads: function() {\n      var response = [];\n      for(var i=0;i<this.out_depth;i++) {\n        response.push({params: this.filters[i].w, grads: this.filters[i].dw, l1_decay_mul: this.l1_decay_mul, l2_decay_mul: this.l2_decay_mul});\n      }\n      response.push({params: this.biases.w, grads: this.biases.dw, l1_decay_mul: 0.0, l2_decay_mul: 0.0});\n      return response;\n    },\n    toJSON: function() {\n      var json = {};\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      json.num_inputs = this.num_inputs;\n      json.l1_decay_mul = this.l1_decay_mul;\n      json.l2_decay_mul = this.l2_decay_mul;\n      json.filters = [];\n      for(var i=0;i<this.filters.length;i++) {\n        json.filters.push(this.filters[i].toJSON());\n      }\n      json.biases = this.biases.toJSON();\n      return json;\n    },\n    fromJSON: function(json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type;\n      this.num_inputs = json.num_inputs;\n      this.l1_decay_mul = typeof json.l1_decay_mul !== 'undefined' ? json.l1_decay_mul : 1.0;\n      this.l2_decay_mul = typeof json.l2_decay_mul !== 'undefined' ? json.l2_decay_mul : 1.0;\n      this.filters = [];\n      for(var i=0;i<json.filters.length;i++) {\n        var v = new Vol(0,0,0,0);\n        v.fromJSON(json.filters[i]);\n        this.filters.push(v);\n      }\n      this.biases = new Vol(0,0,0,0);\n      this.biases.fromJSON(json.biases);\n    }\n  }\n\n  global.ConvLayer = ConvLayer;\n  global.FullyConnLayer = FullyConnLayer;\n  \n})(convnetjs);\n(function(global) {\n  \"use strict\";\n  var Vol = global.Vol; // convenience\n  \n  var PoolLayer = function(opt) {\n\n    var opt = opt || {};\n\n    // required\n    this.sx = opt.sx; // filter size\n    this.in_depth = opt.in_depth;\n    this.in_sx = opt.in_sx;\n    this.in_sy = opt.in_sy;\n\n    // optional\n    this.sy = typeof opt.sy !== 'undefined' ? opt.sy : this.sx;\n    this.stride = typeof opt.stride !== 'undefined' ? opt.stride : 2;\n    this.pad = typeof opt.pad !== 'undefined' ? opt.pad : 0; // amount of 0 padding to add around borders of input volume\n\n    // computed\n    this.out_depth = this.in_depth;\n    this.out_sx = Math.floor((this.in_sx + this.pad * 2 - this.sx) / this.stride + 1);\n    this.out_sy = Math.floor((this.in_sy + this.pad * 2 - this.sy) / this.stride + 1);\n    this.layer_type = 'pool';\n    // store switches for x,y coordinates for where the max comes from, for each output neuron\n    this.switchx = global.zeros(this.out_sx*this.out_sy*this.out_depth);\n    this.switchy = global.zeros(this.out_sx*this.out_sy*this.out_depth);\n  }\n\n  PoolLayer.prototype = {\n    forward: function(V, is_training) {\n      this.in_act = V;\n\n      var A = new Vol(this.out_sx, this.out_sy, this.out_depth, 0.0);\n      \n      var n=0; // a counter for switches\n      for(var d=0;d<this.out_depth;d++) {\n        var x = -this.pad;\n        var y = -this.pad;\n        for(var ax=0; ax<this.out_sx; x+=this.stride,ax++) {\n          y = -this.pad;\n          for(var ay=0; ay<this.out_sy; y+=this.stride,ay++) {\n\n            // convolve centered at this particular location\n            var a = -99999; // hopefully small enough ;\\\n            var winx=-1,winy=-1;\n            for(var fx=0;fx<this.sx;fx++) {\n              for(var fy=0;fy<this.sy;fy++) {\n                var oy = y+fy;\n                var ox = x+fx;\n                if(oy>=0 && oy<V.sy && ox>=0 && ox<V.sx) {\n                  var v = V.get(ox, oy, d);\n                  // perform max pooling and store pointers to where\n                  // the max came from. This will speed up backprop \n                  // and can help make nice visualizations in future\n                  if(v > a) { a = v; winx=ox; winy=oy;}\n                }\n              }\n            }\n            this.switchx[n] = winx;\n            this.switchy[n] = winy;\n            n++;\n            A.set(ax, ay, d, a);\n          }\n        }\n      }\n      this.out_act = A;\n      return this.out_act;\n    },\n    backward: function() { \n      // pooling layers have no parameters, so simply compute \n      // gradient wrt data here\n      var V = this.in_act;\n      V.dw = global.zeros(V.w.length); // zero out gradient wrt data\n      var A = this.out_act; // computed in forward pass \n\n      var n = 0;\n      for(var d=0;d<this.out_depth;d++) {\n        var x = -this.pad;\n        var y = -this.pad;\n        for(var ax=0; ax<this.out_sx; x+=this.stride,ax++) {\n          y = -this.pad;\n          for(var ay=0; ay<this.out_sy; y+=this.stride,ay++) {\n\n            var chain_grad = this.out_act.get_grad(ax,ay,d);\n            V.add_grad(this.switchx[n], this.switchy[n], d, chain_grad);\n            n++;\n\n          }\n        }\n      }\n    },\n    getParamsAndGrads: function() {\n      return [];\n    },\n    toJSON: function() {\n      var json = {};\n      json.sx = this.sx;\n      json.sy = this.sy;\n      json.stride = this.stride;\n      json.in_depth = this.in_depth;\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      json.pad = this.pad;\n      return json;\n    },\n    fromJSON: function(json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type;\n      this.sx = json.sx;\n      this.sy = json.sy;\n      this.stride = json.stride;\n      this.in_depth = json.in_depth;\n      this.pad = typeof json.pad !== 'undefined' ? json.pad : 0; // backwards compatibility\n      this.switchx = global.zeros(this.out_sx*this.out_sy*this.out_depth); // need to re-init these appropriately\n      this.switchy = global.zeros(this.out_sx*this.out_sy*this.out_depth);\n    }\n  }\n\n  global.PoolLayer = PoolLayer;\n\n})(convnetjs);\n\n(function(global) {\n  \"use strict\";\n  var Vol = global.Vol; // convenience\n  \n  var InputLayer = function(opt) {\n    var opt = opt || {};\n\n    // this is a bit silly but lets allow people to specify either ins or outs\n    this.out_sx = typeof opt.out_sx !== 'undefined' ? opt.out_sx : opt.in_sx;\n    this.out_sy = typeof opt.out_sy !== 'undefined' ? opt.out_sy : opt.in_sy;\n    this.out_depth = typeof opt.out_depth !== 'undefined' ? opt.out_depth : opt.in_depth;\n    this.layer_type = 'input';\n  }\n  InputLayer.prototype = {\n    forward: function(V, is_training) {\n      this.in_act = V;\n      this.out_act = V;\n      return this.out_act; // dummy identity function for now\n    },\n    backward: function() { },\n    getParamsAndGrads: function() {\n      return [];\n    },\n    toJSON: function() {\n      var json = {};\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      return json;\n    },\n    fromJSON: function(json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type; \n    }\n  }\n\n  global.InputLayer = InputLayer;\n})(convnetjs);\n(function(global) {\n  \"use strict\";\n  var Vol = global.Vol; // convenience\n  \n  // Layers that implement a loss. Currently these are the layers that \n  // can initiate a backward() pass. In future we probably want a more \n  // flexible system that can accomodate multiple losses to do multi-task\n  // learning, and stuff like that. But for now, one of the layers in this\n  // file must be the final layer in a Net.\n\n  // This is a classifier, with N discrete classes from 0 to N-1\n  // it gets a stream of N incoming numbers and computes the softmax\n  // function (exponentiate and normalize to sum to 1 as probabilities should)\n  var SoftmaxLayer = function(opt) {\n    var opt = opt || {};\n\n    // computed\n    this.num_inputs = opt.in_sx * opt.in_sy * opt.in_depth;\n    this.out_depth = this.num_inputs;\n    this.out_sx = 1;\n    this.out_sy = 1;\n    this.layer_type = 'softmax';\n  }\n\n  SoftmaxLayer.prototype = {\n    forward: function(V, is_training) {\n      this.in_act = V;\n\n      var A = new Vol(1, 1, this.out_depth, 0.0);\n\n      // compute max activation\n      var as = V.w;\n      var amax = V.w[0];\n      for(var i=1;i<this.out_depth;i++) {\n        if(as[i] > amax) amax = as[i];\n      }\n\n      // compute exponentials (carefully to not blow up)\n      var es = global.zeros(this.out_depth);\n      var esum = 0.0;\n      for(var i=0;i<this.out_depth;i++) {\n        var e = Math.exp(as[i] - amax);\n        esum += e;\n        es[i] = e;\n      }\n\n      // normalize and output to sum to one\n      for(var i=0;i<this.out_depth;i++) {\n        es[i] /= esum;\n        A.w[i] = es[i];\n      }\n\n      this.es = es; // save these for backprop\n      this.out_act = A;\n      return this.out_act;\n    },\n    backward: function(y) {\n\n      // compute and accumulate gradient wrt weights and bias of this layer\n      var x = this.in_act;\n      x.dw = global.zeros(x.w.length); // zero out the gradient of input Vol\n\n      for(var i=0;i<this.out_depth;i++) {\n        var indicator = i === y ? 1.0 : 0.0;\n        var mul = -(indicator - this.es[i]);\n        x.dw[i] = mul;\n      }\n\n      // loss is the class negative log likelihood\n      return -Math.log(this.es[y]);\n    },\n    getParamsAndGrads: function() { \n      return [];\n    },\n    toJSON: function() {\n      var json = {};\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      json.num_inputs = this.num_inputs;\n      return json;\n    },\n    fromJSON: function(json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type;\n      this.num_inputs = json.num_inputs;\n    }\n  }\n\n  // implements an L2 regression cost layer,\n  // so penalizes \\sum_i(||x_i - y_i||^2), where x is its input\n  // and y is the user-provided array of \"correct\" values.\n  var RegressionLayer = function(opt) {\n    var opt = opt || {};\n\n    // computed\n    this.num_inputs = opt.in_sx * opt.in_sy * opt.in_depth;\n    this.out_depth = this.num_inputs;\n    this.out_sx = 1;\n    this.out_sy = 1;\n    this.layer_type = 'regression';\n  }\n\n  RegressionLayer.prototype = {\n    forward: function(V, is_training) {\n      this.in_act = V;\n      this.out_act = V;\n      return V; // identity function\n    },\n    // y is a list here of size num_inputs\n    backward: function(y) { \n\n      // compute and accumulate gradient wrt weights and bias of this layer\n      var x = this.in_act;\n      x.dw = global.zeros(x.w.length); // zero out the gradient of input Vol\n      var loss = 0.0;\n      if(y instanceof Array || y instanceof Float64Array) {\n        for(var i=0;i<this.out_depth;i++) {\n          var dy = x.w[i] - y[i];\n          x.dw[i] = dy;\n          loss += 2*dy*dy;\n        }\n      } else {\n        // assume it is a struct with entries .dim and .val\n        // and we pass gradient only along dimension dim to be equal to val\n        var i = y.dim;\n        var yi = y.val;\n        var dy = x.w[i] - yi;\n        x.dw[i] = dy;\n        loss += 2*dy*dy;\n      }\n      return loss;\n    },\n    getParamsAndGrads: function() { \n      return [];\n    },\n    toJSON: function() {\n      var json = {};\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      json.num_inputs = this.num_inputs;\n      return json;\n    },\n    fromJSON: function(json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type;\n      this.num_inputs = json.num_inputs;\n    }\n  }\n\n  var SVMLayer = function(opt) {\n    var opt = opt || {};\n\n    // computed\n    this.num_inputs = opt.in_sx * opt.in_sy * opt.in_depth;\n    this.out_depth = this.num_inputs;\n    this.out_sx = 1;\n    this.out_sy = 1;\n    this.layer_type = 'svm';\n  }\n\n  SVMLayer.prototype = {\n    forward: function(V, is_training) {\n      this.in_act = V;\n      this.out_act = V; // nothing to do, output raw scores\n      return V;\n    },\n    backward: function(y) {\n\n      // compute and accumulate gradient wrt weights and bias of this layer\n      var x = this.in_act;\n      x.dw = global.zeros(x.w.length); // zero out the gradient of input Vol\n\n      var yscore = x.w[y]; // score of ground truth\n      var margin = 1.0;\n      var loss = 0.0;\n      for(var i=0;i<this.out_depth;i++) {\n        if(-yscore + x.w[i] + margin > 0) {\n          // violating example, apply loss\n          // I love hinge loss, by the way. Truly.\n          // Seriously, compare this SVM code with Softmax forward AND backprop code above\n          // it's clear which one is superior, not only in code, simplicity\n          // and beauty, but also in practice.\n          x.dw[i] += 1;\n          x.dw[y] -= 1;\n          loss += -yscore + x.w[i] + margin;\n        }\n      }\n\n      return loss;\n    },\n    getParamsAndGrads: function() { \n      return [];\n    },\n    toJSON: function() {\n      var json = {};\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      json.num_inputs = this.num_inputs;\n      return json;\n    },\n    fromJSON: function(json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type;\n      this.num_inputs = json.num_inputs;\n    }\n  }\n  \n  global.RegressionLayer = RegressionLayer;\n  global.SoftmaxLayer = SoftmaxLayer;\n  global.SVMLayer = SVMLayer;\n\n})(convnetjs);\n\n(function(global) {\n  \"use strict\";\n  var Vol = global.Vol; // convenience\n  \n  // Implements ReLU nonlinearity elementwise\n  // x -> max(0, x)\n  // the output is in [0, inf)\n  var ReluLayer = function(opt) {\n    var opt = opt || {};\n\n    // computed\n    this.out_sx = opt.in_sx;\n    this.out_sy = opt.in_sy;\n    this.out_depth = opt.in_depth;\n    this.layer_type = 'relu';\n  }\n  ReluLayer.prototype = {\n    forward: function(V, is_training) {\n      this.in_act = V;\n      var V2 = V.clone();\n      var N = V.w.length;\n      var V2w = V2.w;\n      for(var i=0;i<N;i++) { \n        if(V2w[i] < 0) V2w[i] = 0; // threshold at 0\n      }\n      this.out_act = V2;\n      return this.out_act;\n    },\n    backward: function() {\n      var V = this.in_act; // we need to set dw of this\n      var V2 = this.out_act;\n      var N = V.w.length;\n      V.dw = global.zeros(N); // zero out gradient wrt data\n      for(var i=0;i<N;i++) {\n        if(V2.w[i] <= 0) V.dw[i] = 0; // threshold\n        else V.dw[i] = V2.dw[i];\n      }\n    },\n    getParamsAndGrads: function() {\n      return [];\n    },\n    toJSON: function() {\n      var json = {};\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      return json;\n    },\n    fromJSON: function(json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type; \n    }\n  }\n\n  // Implements Sigmoid nnonlinearity elementwise\n  // x -> 1/(1+e^(-x))\n  // so the output is between 0 and 1.\n  var SigmoidLayer = function(opt) {\n    var opt = opt || {};\n\n    // computed\n    this.out_sx = opt.in_sx;\n    this.out_sy = opt.in_sy;\n    this.out_depth = opt.in_depth;\n    this.layer_type = 'sigmoid';\n  }\n  SigmoidLayer.prototype = {\n    forward: function(V, is_training) {\n      this.in_act = V;\n      var V2 = V.cloneAndZero();\n      var N = V.w.length;\n      var V2w = V2.w;\n      var Vw = V.w;\n      for(var i=0;i<N;i++) { \n        V2w[i] = 1.0/(1.0+Math.exp(-Vw[i]));\n      }\n      this.out_act = V2;\n      return this.out_act;\n    },\n    backward: function() {\n      var V = this.in_act; // we need to set dw of this\n      var V2 = this.out_act;\n      var N = V.w.length;\n      V.dw = global.zeros(N); // zero out gradient wrt data\n      for(var i=0;i<N;i++) {\n        var v2wi = V2.w[i];\n        V.dw[i] =  v2wi * (1.0 - v2wi) * V2.dw[i];\n      }\n    },\n    getParamsAndGrads: function() {\n      return [];\n    },\n    toJSON: function() {\n      var json = {};\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      return json;\n    },\n    fromJSON: function(json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type; \n    }\n  }\n\n  // Implements Maxout nnonlinearity that computes\n  // x -> max(x)\n  // where x is a vector of size group_size. Ideally of course,\n  // the input size should be exactly divisible by group_size\n  var MaxoutLayer = function(opt) {\n    var opt = opt || {};\n\n    // required\n    this.group_size = typeof opt.group_size !== 'undefined' ? opt.group_size : 2;\n\n    // computed\n    this.out_sx = opt.in_sx;\n    this.out_sy = opt.in_sy;\n    this.out_depth = Math.floor(opt.in_depth / this.group_size);\n    this.layer_type = 'maxout';\n\n    this.switches = global.zeros(this.out_sx*this.out_sy*this.out_depth); // useful for backprop\n  }\n  MaxoutLayer.prototype = {\n    forward: function(V, is_training) {\n      this.in_act = V;\n      var N = this.out_depth; \n      var V2 = new Vol(this.out_sx, this.out_sy, this.out_depth, 0.0);\n\n      // optimization branch. If we're operating on 1D arrays we dont have\n      // to worry about keeping track of x,y,d coordinates inside\n      // input volumes. In convnets we do :(\n      if(this.out_sx === 1 && this.out_sy === 1) {\n        for(var i=0;i<N;i++) {\n          var ix = i * this.group_size; // base index offset\n          var a = V.w[ix];\n          var ai = 0;\n          for(var j=1;j<this.group_size;j++) {\n            var a2 = V.w[ix+j];\n            if(a2 > a) {\n              a = a2;\n              ai = j;\n            }\n          }\n          V2.w[i] = a;\n          this.switches[i] = ix + ai;\n        }\n      } else {\n        var n=0; // counter for switches\n        for(var x=0;x<V.sx;x++) {\n          for(var y=0;y<V.sy;y++) {\n            for(var i=0;i<N;i++) {\n              var ix = i * this.group_size;\n              var a = V.get(x, y, ix);\n              var ai = 0;\n              for(var j=1;j<this.group_size;j++) {\n                var a2 = V.get(x, y, ix+j);\n                if(a2 > a) {\n                  a = a2;\n                  ai = j;\n                }\n              }\n              V2.set(x,y,i,a);\n              this.switches[n] = ix + ai;\n              n++;\n            }\n          }\n        }\n\n      }\n      this.out_act = V2;\n      return this.out_act;\n    },\n    backward: function() {\n      var V = this.in_act; // we need to set dw of this\n      var V2 = this.out_act;\n      var N = this.out_depth;\n      V.dw = global.zeros(V.w.length); // zero out gradient wrt data\n\n      // pass the gradient through the appropriate switch\n      if(this.out_sx === 1 && this.out_sy === 1) {\n        for(var i=0;i<N;i++) {\n          var chain_grad = V2.dw[i];\n          V.dw[this.switches[i]] = chain_grad;\n        }\n      } else {\n        // bleh okay, lets do this the hard way\n        var n=0; // counter for switches\n        for(var x=0;x<V2.sx;x++) {\n          for(var y=0;y<V2.sy;y++) {\n            for(var i=0;i<N;i++) {\n              var chain_grad = V2.get_grad(x,y,i);\n              V.set_grad(x,y,this.switches[n],chain_grad);\n              n++;\n            }\n          }\n        }\n      }\n    },\n    getParamsAndGrads: function() {\n      return [];\n    },\n    toJSON: function() {\n      var json = {};\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      json.group_size = this.group_size;\n      return json;\n    },\n    fromJSON: function(json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type; \n      this.group_size = json.group_size;\n      this.switches = global.zeros(this.group_size);\n    }\n  }\n\n  // a helper function, since tanh is not yet part of ECMAScript. Will be in v6.\n  function tanh(x) {\n    var y = Math.exp(2 * x);\n    return (y - 1) / (y + 1);\n  }\n  // Implements Tanh nnonlinearity elementwise\n  // x -> tanh(x) \n  // so the output is between -1 and 1.\n  var TanhLayer = function(opt) {\n    var opt = opt || {};\n\n    // computed\n    this.out_sx = opt.in_sx;\n    this.out_sy = opt.in_sy;\n    this.out_depth = opt.in_depth;\n    this.layer_type = 'tanh';\n  }\n  TanhLayer.prototype = {\n    forward: function(V, is_training) {\n      this.in_act = V;\n      var V2 = V.cloneAndZero();\n      var N = V.w.length;\n      for(var i=0;i<N;i++) { \n        V2.w[i] = tanh(V.w[i]);\n      }\n      this.out_act = V2;\n      return this.out_act;\n    },\n    backward: function() {\n      var V = this.in_act; // we need to set dw of this\n      var V2 = this.out_act;\n      var N = V.w.length;\n      V.dw = global.zeros(N); // zero out gradient wrt data\n      for(var i=0;i<N;i++) {\n        var v2wi = V2.w[i];\n        V.dw[i] = (1.0 - v2wi * v2wi) * V2.dw[i];\n      }\n    },\n    getParamsAndGrads: function() {\n      return [];\n    },\n    toJSON: function() {\n      var json = {};\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      return json;\n    },\n    fromJSON: function(json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type; \n    }\n  }\n  \n  global.TanhLayer = TanhLayer;\n  global.MaxoutLayer = MaxoutLayer;\n  global.ReluLayer = ReluLayer;\n  global.SigmoidLayer = SigmoidLayer;\n\n})(convnetjs);\n\n(function(global) {\n  \"use strict\";\n  var Vol = global.Vol; // convenience\n\n  // An inefficient dropout layer\n  // Note this is not most efficient implementation since the layer before\n  // computed all these activations and now we're just going to drop them :(\n  // same goes for backward pass. Also, if we wanted to be efficient at test time\n  // we could equivalently be clever and upscale during train and copy pointers during test\n  // todo: make more efficient.\n  var DropoutLayer = function(opt) {\n    var opt = opt || {};\n\n    // computed\n    this.out_sx = opt.in_sx;\n    this.out_sy = opt.in_sy;\n    this.out_depth = opt.in_depth;\n    this.layer_type = 'dropout';\n    this.drop_prob = typeof opt.drop_prob !== 'undefined' ? opt.drop_prob : 0.5;\n    this.dropped = global.zeros(this.out_sx*this.out_sy*this.out_depth);\n  }\n  DropoutLayer.prototype = {\n    forward: function(V, is_training) {\n      this.in_act = V;\n      if(typeof(is_training)==='undefined') { is_training = false; } // default is prediction mode\n      var V2 = V.clone();\n      var N = V.w.length;\n      if(is_training) {\n        // do dropout\n        for(var i=0;i<N;i++) {\n          if(Math.random()<this.drop_prob) { V2.w[i]=0; this.dropped[i] = true; } // drop!\n          else {this.dropped[i] = false;}\n        }\n      } else {\n        // scale the activations during prediction\n        for(var i=0;i<N;i++) { V2.w[i]*=this.drop_prob; }\n      }\n      this.out_act = V2;\n      return this.out_act; // dummy identity function for now\n    },\n    backward: function() {\n      var V = this.in_act; // we need to set dw of this\n      var chain_grad = this.out_act;\n      var N = V.w.length;\n      V.dw = global.zeros(N); // zero out gradient wrt data\n      for(var i=0;i<N;i++) {\n        if(!(this.dropped[i])) { \n          V.dw[i] = chain_grad.dw[i]; // copy over the gradient\n        }\n      }\n    },\n    getParamsAndGrads: function() {\n      return [];\n    },\n    toJSON: function() {\n      var json = {};\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      json.drop_prob = this.drop_prob;\n      return json;\n    },\n    fromJSON: function(json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type; \n      this.drop_prob = json.drop_prob;\n    }\n  }\n  \n\n  global.DropoutLayer = DropoutLayer;\n})(convnetjs);\n(function(global) {\n  \"use strict\";\n  var Vol = global.Vol; // convenience\n  \n  // a bit experimental layer for now. I think it works but I'm not 100%\n  // the gradient check is a bit funky. I'll look into this a bit later.\n  // Local Response Normalization in window, along depths of volumes\n  var LocalResponseNormalizationLayer = function(opt) {\n    var opt = opt || {};\n\n    // required\n    this.k = opt.k;\n    this.n = opt.n;\n    this.alpha = opt.alpha;\n    this.beta = opt.beta;\n\n    // computed\n    this.out_sx = opt.in_sx;\n    this.out_sy = opt.in_sy;\n    this.out_depth = opt.in_depth;\n    this.layer_type = 'lrn';\n\n    // checks\n    if(this.n%2 === 0) { console.log('WARNING n should be odd for LRN layer'); }\n  }\n  LocalResponseNormalizationLayer.prototype = {\n    forward: function(V, is_training) {\n      this.in_act = V;\n\n      var A = V.cloneAndZero();\n      this.S_cache_ = V.cloneAndZero();\n      var n2 = Math.floor(this.n/2);\n      for(var x=0;x<V.sx;x++) {\n        for(var y=0;y<V.sy;y++) {\n          for(var i=0;i<V.depth;i++) {\n\n            var ai = V.get(x,y,i);\n\n            // normalize in a window of size n\n            var den = 0.0;\n            for(var j=Math.max(0,i-n2);j<=Math.min(i+n2,V.depth-1);j++) {\n              var aa = V.get(x,y,j);\n              den += aa*aa;\n            }\n            den *= this.alpha / this.n;\n            den += this.k;\n            this.S_cache_.set(x,y,i,den); // will be useful for backprop\n            den = Math.pow(den, this.beta);\n            A.set(x,y,i,ai/den);\n          }\n        }\n      }\n\n      this.out_act = A;\n      return this.out_act; // dummy identity function for now\n    },\n    backward: function() { \n      // evaluate gradient wrt data\n      var V = this.in_act; // we need to set dw of this\n      V.dw = global.zeros(V.w.length); // zero out gradient wrt data\n      var A = this.out_act; // computed in forward pass \n\n      var n2 = Math.floor(this.n/2);\n      for(var x=0;x<V.sx;x++) {\n        for(var y=0;y<V.sy;y++) {\n          for(var i=0;i<V.depth;i++) {\n\n            var chain_grad = this.out_act.get_grad(x,y,i);\n            var S = this.S_cache_.get(x,y,i);\n            var SB = Math.pow(S, this.beta);\n            var SB2 = SB*SB;\n\n            // normalize in a window of size n\n            for(var j=Math.max(0,i-n2);j<=Math.min(i+n2,V.depth-1);j++) {              \n              var aj = V.get(x,y,j); \n              var g = -aj*this.beta*Math.pow(S,this.beta-1)*this.alpha/this.n*2*aj;\n              if(j===i) g+= SB;\n              g /= SB2;\n              g *= chain_grad;\n              V.add_grad(x,y,j,g);\n            }\n\n          }\n        }\n      }\n    },\n    getParamsAndGrads: function() { return []; },\n    toJSON: function() {\n      var json = {};\n      json.k = this.k;\n      json.n = this.n;\n      json.alpha = this.alpha; // normalize by size\n      json.beta = this.beta;\n      json.out_sx = this.out_sx; \n      json.out_sy = this.out_sy;\n      json.out_depth = this.out_depth;\n      json.layer_type = this.layer_type;\n      return json;\n    },\n    fromJSON: function(json) {\n      this.k = json.k;\n      this.n = json.n;\n      this.alpha = json.alpha; // normalize by size\n      this.beta = json.beta;\n      this.out_sx = json.out_sx; \n      this.out_sy = json.out_sy;\n      this.out_depth = json.out_depth;\n      this.layer_type = json.layer_type;\n    }\n  }\n  \n\n  global.LocalResponseNormalizationLayer = LocalResponseNormalizationLayer;\n})(convnetjs);\n(function(global) {\n  \"use strict\";\n  var Vol = global.Vol; // convenience\n\n  // transforms x-> [x, x_i*x_j forall i,j]\n  // so the fully connected layer afters will essentially be doing tensor multiplies\n  var QuadTransformLayer = function(opt) {\n    var opt = opt || {};\n\n    // computed\n    this.out_sx = opt.in_sx;\n    this.out_sy = opt.in_sy;\n    // linear terms, and then quadratic terms, of which there are 1/2*n*(n+1),\n    // (offdiagonals and the diagonal total) and arithmetic series.\n    // Actually never mind, lets not be fancy here yet and just include\n    // terms x_ix_j and x_jx_i twice. Half as efficient but much less\n    // headache.\n    this.out_depth = opt.in_depth + opt.in_depth * opt.in_depth;\n    this.layer_type = 'quadtransform';\n\n  }\n  QuadTransformLayer.prototype = {\n    forward: function(V, is_training) {\n      this.in_act = V;\n      var N = this.out_depth;\n      var Ni = V.depth;\n      var V2 = new Vol(this.out_sx, this.out_sy, this.out_depth, 0.0);\n      for(var x=0;x<V.sx;x++) {\n        for(var y=0;y<V.sy;y++) {\n          for(var i=0;i<N;i++) {\n            if(i<Ni) {\n              V2.set(x,y,i,V.get(x,y,i)); // copy these over (linear terms)\n            } else {\n              var i0 = Math.floor((i-Ni)/Ni);\n              var i1 = (i-Ni) - i0*Ni;\n              V2.set(x,y,i,V.get(x,y,i0) * V.get(x,y,i1)); // quadratic\n            }\n          }\n        }\n      }\n      this.out_act = V2;\n      return this.out_act; // dummy identity function for now\n    },\n    backward: function() {\n      var V = this.in_act;\n      V.dw = global.zeros(V.w.length); // zero out gradient wrt data\n      var V2 = this.out_act;\n      var N = this.out_depth;\n      var Ni = V.depth;\n      for(var x=0;x<V.sx;x++) {\n        for(var y=0;y<V.sy;y++) {\n          for(var i=0;i<N;i++) {\n            var chain_grad = V2.get_grad(x,y,i);\n            if(i<Ni) {\n              V.add_grad(x,y,i,chain_grad);\n            } else {\n              var i0 = Math.floor((i-Ni)/Ni);\n              var i1 = (i-Ni) - i0*Ni;\n              V.add_grad(x,y,i0,V.get(x,y,i1)*chain_grad);\n              V.add_grad(x,y,i1,V.get(x,y,i0)*chain_grad);\n            }\n          }\n        }\n      }\n    },\n    getParamsAndGrads: function() {\n      return [];\n    },\n    toJSON: function() {\n      var json = {};\n      json.out_depth = this.out_depth;\n      json.out_sx = this.out_sx;\n      json.out_sy = this.out_sy;\n      json.layer_type = this.layer_type;\n      return json;\n    },\n    fromJSON: function(json) {\n      this.out_depth = json.out_depth;\n      this.out_sx = json.out_sx;\n      this.out_sy = json.out_sy;\n      this.layer_type = json.layer_type; \n    }\n  }\n  \n\n  global.QuadTransformLayer = QuadTransformLayer;\n})(convnetjs);\n(function(global) {\n  \"use strict\";\n  var Vol = global.Vol; // convenience\n  \n  // Net manages a set of layers\n  // For now constraints: Simple linear order of layers, first layer input last layer a cost layer\n  var Net = function(options) {\n    this.layers = [];\n  }\n\n  Net.prototype = {\n    \n    // takes a list of layer definitions and creates the network layer objects\n    makeLayers: function(defs) {\n\n      // few checks for now\n      if(defs.length<2) {console.log('ERROR! For now at least have input and softmax layers.');}\n      if(defs[0].type !== 'input') {console.log('ERROR! For now first layer should be input.');}\n\n      // desugar syntactic for adding activations and dropouts\n      var desugar = function() {\n        var new_defs = [];\n        for(var i=0;i<defs.length;i++) {\n          var def = defs[i];\n          \n          if(def.type==='softmax' || def.type==='svm') {\n            // add an fc layer here, there is no reason the user should\n            // have to worry about this and we almost always want to\n            new_defs.push({type:'fc', num_neurons: def.num_classes});\n          }\n\n          if(def.type==='regression') {\n            // add an fc layer here, there is no reason the user should\n            // have to worry about this and we almost always want to\n            new_defs.push({type:'fc', num_neurons: def.num_neurons});\n          }\n\n          if((def.type==='fc' || def.type==='conv') \n              && typeof(def.bias_pref) === 'undefined'){\n            def.bias_pref = 0.0;\n            if(typeof def.activation !== 'undefined' && def.activation === 'relu') {\n              def.bias_pref = 0.1; // relus like a bit of positive bias to get gradients early\n              // otherwise it's technically possible that a relu unit will never turn on (by chance)\n              // and will never get any gradient and never contribute any computation. Dead relu.\n            }\n          }\n          \n          if(typeof def.tensor !== 'undefined') {\n            // apply quadratic transform so that the upcoming multiply will include\n            // quadratic terms, equivalent to doing a tensor product\n            if(def.tensor) {\n              new_defs.push({type: 'quadtransform'});\n            }\n          }\n\n          new_defs.push(def);\n\n          if(typeof def.activation !== 'undefined') {\n            if(def.activation==='relu') { new_defs.push({type:'relu'}); }\n            else if (def.activation==='sigmoid') { new_defs.push({type:'sigmoid'}); }\n            else if (def.activation==='tanh') { new_defs.push({type:'tanh'}); }\n            else if (def.activation==='maxout') {\n              // create maxout activation, and pass along group size, if provided\n              var gs = def.group_size !== 'undefined' ? def.group_size : 2;\n              new_defs.push({type:'maxout', group_size:gs});\n            }\n            else { console.log('ERROR unsupported activation ' + def.activation); }\n          }\n          if(typeof def.drop_prob !== 'undefined' && def.type !== 'dropout') {\n            new_defs.push({type:'dropout', drop_prob: def.drop_prob});\n          }\n\n        }\n        return new_defs;\n      }\n      defs = desugar(defs);\n\n      // create the layers\n      this.layers = [];\n      for(var i=0;i<defs.length;i++) {\n        var def = defs[i];\n        if(i>0) {\n          var prev = this.layers[i-1];\n          def.in_sx = prev.out_sx;\n          def.in_sy = prev.out_sy;\n          def.in_depth = prev.out_depth;\n        }\n\n        switch(def.type) {\n          case 'fc': this.layers.push(new global.FullyConnLayer(def)); break;\n          case 'lrn': this.layers.push(new global.LocalResponseNormalizationLayer(def)); break;\n          case 'dropout': this.layers.push(new global.DropoutLayer(def)); break;\n          case 'input': this.layers.push(new global.InputLayer(def)); break;\n          case 'softmax': this.layers.push(new global.SoftmaxLayer(def)); break;\n          case 'regression': this.layers.push(new global.RegressionLayer(def)); break;\n          case 'conv': this.layers.push(new global.ConvLayer(def)); break;\n          case 'pool': this.layers.push(new global.PoolLayer(def)); break;\n          case 'relu': this.layers.push(new global.ReluLayer(def)); break;\n          case 'sigmoid': this.layers.push(new global.SigmoidLayer(def)); break;\n          case 'tanh': this.layers.push(new global.TanhLayer(def)); break;\n          case 'maxout': this.layers.push(new global.MaxoutLayer(def)); break;\n          case 'quadtransform': this.layers.push(new global.QuadTransformLayer(def)); break;\n          case 'svm': this.layers.push(new global.SVMLayer(def)); break;\n          default: console.log('ERROR: UNRECOGNIZED LAYER TYPE!');\n        }\n      }\n    },\n\n    // forward prop the network. A trainer will pass in is_training = true\n    forward: function(V, is_training) {\n      if(typeof(is_training)==='undefined') is_training = false;\n      var act = this.layers[0].forward(V, is_training);\n      for(var i=1;i<this.layers.length;i++) {\n        act = this.layers[i].forward(act, is_training);\n      }\n      return act;\n    },\n    \n    // backprop: compute gradients wrt all parameters\n    backward: function(y) {\n      var N = this.layers.length;\n      var loss = this.layers[N-1].backward(y); // last layer assumed softmax\n      for(var i=N-2;i>=0;i--) { // first layer assumed input\n        this.layers[i].backward();\n      }\n      return loss;\n    },\n    getParamsAndGrads: function() {\n      // accumulate parameters and gradients for the entire network\n      var response = [];\n      for(var i=0;i<this.layers.length;i++) {\n        var layer_reponse = this.layers[i].getParamsAndGrads();\n        for(var j=0;j<layer_reponse.length;j++) {\n          response.push(layer_reponse[j]);\n        }\n      }\n      return response;\n    },\n    getPrediction: function() {\n      var S = this.layers[this.layers.length-1]; // softmax layer\n      var p = S.out_act.w;\n      var maxv = p[0];\n      var maxi = 0;\n      for(var i=1;i<p.length;i++) {\n        if(p[i] > maxv) { maxv = p[i]; maxi = i;}\n      }\n      return maxi;\n    },\n    toJSON: function() {\n      var json = {};\n      json.layers = [];\n      for(var i=0;i<this.layers.length;i++) {\n        json.layers.push(this.layers[i].toJSON());\n      }\n      return json;\n    },\n    fromJSON: function(json) {\n      this.layers = [];\n      for(var i=0;i<json.layers.length;i++) {\n        var Lj = json.layers[i]\n        var t = Lj.layer_type;\n        var L;\n        if(t==='input') { L = new global.InputLayer(); }\n        if(t==='relu') { L = new global.ReluLayer(); }\n        if(t==='sigmoid') { L = new global.SigmoidLayer(); }\n        if(t==='tanh') { L = new global.TanhLayer(); }\n        if(t==='dropout') { L = new global.DropoutLayer(); }\n        if(t==='conv') { L = new global.ConvLayer(); }\n        if(t==='pool') { L = new global.PoolLayer(); }\n        if(t==='lrn') { L = new global.LocalResponseNormalizationLayer(); }\n        if(t==='softmax') { L = new global.SoftmaxLayer(); }\n        if(t==='regression') { L = new global.RegressionLayer(); }\n        if(t==='fc') { L = new global.FullyConnLayer(); }\n        if(t==='maxout') { L = new global.MaxoutLayer(); }\n        if(t==='quadtransform') { L = new global.QuadTransformLayer(); }\n        if(t==='svm') { L = new global.SVMLayer(); }\n        L.fromJSON(Lj);\n        this.layers.push(L);\n      }\n    }\n  }\n  \n\n  global.Net = Net;\n})(convnetjs);\n(function(global) {\n  \"use strict\";\n  var Vol = global.Vol; // convenience\n\n  var Trainer = function(net, options) {\n\n    this.net = net;\n\n    var options = options || {};\n    this.learning_rate = typeof options.learning_rate !== 'undefined' ? options.learning_rate : 0.01;\n    this.l1_decay = typeof options.l1_decay !== 'undefined' ? options.l1_decay : 0.0;\n    this.l2_decay = typeof options.l2_decay !== 'undefined' ? options.l2_decay : 0.0;\n    this.batch_size = typeof options.batch_size !== 'undefined' ? options.batch_size : 1;\n    this.method = typeof options.method !== 'undefined' ? options.method : 'sgd'; // sgd/adagrad/adadelta/windowgrad\n\n    this.momentum = typeof options.momentum !== 'undefined' ? options.momentum : 0.9;\n    this.ro = typeof options.ro !== 'undefined' ? options.ro : 0.95; // used in adadelta\n    this.eps = typeof options.eps !== 'undefined' ? options.eps : 1e-6; // used in adadelta\n\n    this.k = 0; // iteration counter\n    this.gsum = []; // last iteration gradients (used for momentum calculations)\n    this.xsum = []; // used in adadelta\n  }\n\n  Trainer.prototype = {\n    train: function(x, y) {\n\n      var start = new Date().getTime();\n      this.net.forward(x, true); // also set the flag that lets the net know we're just training\n      var end = new Date().getTime();\n      var fwd_time = end - start;\n\n      var start = new Date().getTime();\n      var cost_loss = this.net.backward(y);\n      var l2_decay_loss = 0.0;\n      var l1_decay_loss = 0.0;\n      var end = new Date().getTime();\n      var bwd_time = end - start;\n      \n      this.k++;\n      if(this.k % this.batch_size === 0) {\n\n        var pglist = this.net.getParamsAndGrads();\n\n        // initialize lists for accumulators. Will only be done once on first iteration\n        if(this.gsum.length === 0 && (this.method !== 'sgd' || this.momentum > 0.0)) {\n          // only vanilla sgd doesnt need either lists\n          // momentum needs gsum\n          // adagrad needs gsum\n          // adadelta needs gsum and xsum\n          for(var i=0;i<pglist.length;i++) {\n            this.gsum.push(global.zeros(pglist[i].params.length));\n            if(this.method === 'adadelta') {\n              this.xsum.push(global.zeros(pglist[i].params.length));\n            } else {\n              this.xsum.push([]); // conserve memory\n            }\n          }\n        }\n\n        // perform an update for all sets of weights\n        for(var i=0;i<pglist.length;i++) {\n          var pg = pglist[i]; // param, gradient, other options in future (custom learning rate etc)\n          var p = pg.params;\n          var g = pg.grads;\n\n          // learning rate for some parameters.\n          var l2_decay_mul = typeof pg.l2_decay_mul !== 'undefined' ? pg.l2_decay_mul : 1.0;\n          var l1_decay_mul = typeof pg.l1_decay_mul !== 'undefined' ? pg.l1_decay_mul : 1.0;\n          var l2_decay = this.l2_decay * l2_decay_mul;\n          var l1_decay = this.l1_decay * l1_decay_mul;\n\n          var plen = p.length;\n          for(var j=0;j<plen;j++) {\n            l2_decay_loss += l2_decay*p[j]*p[j]/2; // accumulate weight decay loss\n            l1_decay_loss += l1_decay*Math.abs(p[j]);\n            var l1grad = l1_decay * (p[j] > 0 ? 1 : -1);\n            var l2grad = l2_decay * (p[j]);\n\n            var gij = (l2grad + l1grad + g[j]) / this.batch_size; // raw batch gradient\n\n            var gsumi = this.gsum[i];\n            var xsumi = this.xsum[i];\n            if(this.method === 'adagrad') {\n              // adagrad update\n              gsumi[j] = gsumi[j] + gij * gij;\n              var dx = - this.learning_rate / Math.sqrt(gsumi[j] + this.eps) * gij;\n              p[j] += dx;\n            } else if(this.method === 'windowgrad') {\n              // this is adagrad but with a moving window weighted average\n              // so the gradient is not accumulated over the entire history of the run. \n              // it's also referred to as Idea #1 in Zeiler paper on Adadelta. Seems reasonable to me!\n              gsumi[j] = this.ro * gsumi[j] + (1-this.ro) * gij * gij;\n              var dx = - this.learning_rate / Math.sqrt(gsumi[j] + this.eps) * gij; // eps added for better conditioning\n              p[j] += dx;\n            } else if(this.method === 'adadelta') {\n              // assume adadelta if not sgd or adagrad\n              gsumi[j] = this.ro * gsumi[j] + (1-this.ro) * gij * gij;\n              var dx = - Math.sqrt((xsumi[j] + this.eps)/(gsumi[j] + this.eps)) * gij;\n              xsumi[j] = this.ro * xsumi[j] + (1-this.ro) * dx * dx; // yes, xsum lags behind gsum by 1.\n              p[j] += dx;\n            } else {\n              // assume SGD\n              if(this.momentum > 0.0) {\n                // momentum update\n                var dx = this.momentum * gsumi[j] - this.learning_rate * gij; // step\n                gsumi[j] = dx; // back this up for next iteration of momentum\n                p[j] += dx; // apply corrected gradient\n              } else {\n                // vanilla sgd\n                p[j] +=  - this.learning_rate * gij;\n              }\n            }\n            g[j] = 0.0; // zero out gradient so that we can begin accumulating anew\n          }\n        }\n      }\n\n      // appending softmax_loss for backwards compatibility, but from now on we will always use cost_loss\n      // in future, TODO: have to completely redo the way loss is done around the network as currently \n      // loss is a bit of a hack. Ideally, user should specify arbitrary number of loss functions on any layer\n      // and it should all be computed correctly and automatically. \n      return {fwd_time: fwd_time, bwd_time: bwd_time, \n              l2_decay_loss: l2_decay_loss, l1_decay_loss: l1_decay_loss,\n              cost_loss: cost_loss, softmax_loss: cost_loss, \n              loss: cost_loss + l1_decay_loss + l2_decay_loss}\n    }\n  }\n  \n  global.Trainer = Trainer;\n  global.SGDTrainer = Trainer; // backwards compatibility\n})(convnetjs);\n\n(function(global) {\n  \"use strict\";\n\n  // used utilities, make explicit local references\n  var randf = global.randf;\n  var randi = global.randi;\n  var Net = global.Net;\n  var Trainer = global.Trainer;\n  var maxmin = global.maxmin;\n  var randperm = global.randperm;\n  var weightedSample = global.weightedSample;\n  var getopt = global.getopt;\n  var arrUnique = global.arrUnique;\n\n  /*\n  A MagicNet takes data: a list of convnetjs.Vol(), and labels\n  which for now are assumed to be class indeces 0..K. MagicNet then:\n  - creates data folds for cross-validation\n  - samples candidate networks\n  - evaluates candidate networks on all data folds\n  - produces predictions by model-averaging the best networks\n  */\n  var MagicNet = function(data, labels, opt) {\n    var opt = opt || {};\n    if(typeof data === 'undefined') { data = []; }\n    if(typeof labels === 'undefined') { labels = []; }\n\n    // required inputs\n    this.data = data; // store these pointers to data\n    this.labels = labels;\n\n    // optional inputs\n    this.train_ratio = getopt(opt, 'train_ratio', 0.7);\n    this.num_folds = getopt(opt, 'num_folds', 10);\n    this.num_candidates = getopt(opt, 'num_candidates', 50); // we evaluate several in parallel\n    // how many epochs of data to train every network? for every fold?\n    // higher values mean higher accuracy in final results, but more expensive\n    this.num_epochs = getopt(opt, 'num_epochs', 50); \n    // number of best models to average during prediction. Usually higher = better\n    this.ensemble_size = getopt(opt, 'ensemble_size', 10);\n\n    // candidate parameters\n    this.batch_size_min = getopt(opt, 'batch_size_min', 10);\n    this.batch_size_max = getopt(opt, 'batch_size_max', 300);\n    this.l2_decay_min = getopt(opt, 'l2_decay_min', -4);\n    this.l2_decay_max = getopt(opt, 'l2_decay_max', 2);\n    this.learning_rate_min = getopt(opt, 'learning_rate_min', -4);\n    this.learning_rate_max = getopt(opt, 'learning_rate_max', 0);\n    this.momentum_min = getopt(opt, 'momentum_min', 0.9);\n    this.momentum_max = getopt(opt, 'momentum_max', 0.9);\n    this.neurons_min = getopt(opt, 'neurons_min', 5);\n    this.neurons_max = getopt(opt, 'neurons_max', 30);\n\n    // computed\n    this.folds = []; // data fold indices, gets filled by sampleFolds()\n    this.candidates = []; // candidate networks that are being currently evaluated\n    this.evaluated_candidates = []; // history of all candidates that were fully evaluated on all folds\n    this.unique_labels = arrUnique(labels);\n    this.iter = 0; // iteration counter, goes from 0 -> num_epochs * num_training_data\n    this.foldix = 0; // index of active fold\n\n    // callbacks\n    this.finish_fold_callback = null;\n    this.finish_batch_callback = null;\n\n    // initializations\n    if(this.data.length > 0) {\n      this.sampleFolds();\n      this.sampleCandidates();\n    }\n  };\n\n  MagicNet.prototype = {\n\n    // sets this.folds to a sampling of this.num_folds folds\n    sampleFolds: function() {\n      var N = this.data.length;\n      var num_train = Math.floor(this.train_ratio * N);\n      this.folds = []; // flush folds, if any\n      for(var i=0;i<this.num_folds;i++) {\n        var p = randperm(N);\n        this.folds.push({train_ix: p.slice(0, num_train), test_ix: p.slice(num_train, N)});\n      }\n    },\n\n    // returns a random candidate network\n    sampleCandidate: function() {\n      var input_depth = this.data[0].w.length;\n      var num_classes = this.unique_labels.length;\n\n      // sample network topology and hyperparameters\n      var layer_defs = [];\n      layer_defs.push({type:'input', out_sx:1, out_sy:1, out_depth: input_depth});\n      var nl = weightedSample([0,1,2,3], [0.2, 0.3, 0.3, 0.2]); // prefer nets with 1,2 hidden layers\n      for(var q=0;q<nl;q++) {\n        var ni = randi(this.neurons_min, this.neurons_max);\n        var act = ['tanh','maxout','relu'][randi(0,3)];\n        if(randf(0,1)<0.5) {\n          var dp = Math.random();\n          layer_defs.push({type:'fc', num_neurons: ni, activation: act, drop_prob: dp});\n        } else {\n          layer_defs.push({type:'fc', num_neurons: ni, activation: act});\n        }\n      }\n      layer_defs.push({type:'softmax', num_classes: num_classes});\n      var net = new Net();\n      net.makeLayers(layer_defs);\n\n      // sample training hyperparameters\n      var bs = randi(this.batch_size_min, this.batch_size_max); // batch size\n      var l2 = Math.pow(10, randf(this.l2_decay_min, this.l2_decay_max)); // l2 weight decay\n      var lr = Math.pow(10, randf(this.learning_rate_min, this.learning_rate_max)); // learning rate\n      var mom = randf(this.momentum_min, this.momentum_max); // momentum. Lets just use 0.9, works okay usually ;p\n      var tp = randf(0,1); // trainer type\n      var trainer_def;\n      if(tp<0.33) {\n        trainer_def = {method:'adadelta', batch_size:bs, l2_decay:l2};\n      } else if(tp<0.66) {\n        trainer_def = {method:'adagrad', learning_rate: lr, batch_size:bs, l2_decay:l2};\n      } else {\n        trainer_def = {method:'sgd', learning_rate: lr, momentum: mom, batch_size:bs, l2_decay:l2};\n      }\n      \n      var trainer = new Trainer(net, trainer_def);\n\n      var cand = {};\n      cand.acc = [];\n      cand.accv = 0; // this will maintained as sum(acc) for convenience\n      cand.layer_defs = layer_defs;\n      cand.trainer_def = trainer_def;\n      cand.net = net;\n      cand.trainer = trainer;\n      return cand;\n    },\n\n    // sets this.candidates with this.num_candidates candidate nets\n    sampleCandidates: function() {\n      this.candidates = []; // flush, if any\n      for(var i=0;i<this.num_candidates;i++) {\n        var cand = this.sampleCandidate();\n        this.candidates.push(cand);\n      }\n    },\n\n    step: function() {\n      \n      // run an example through current candidate\n      this.iter++;\n\n      // step all candidates on a random data point\n      var fold = this.folds[this.foldix]; // active fold\n      var dataix = fold.train_ix[randi(0, fold.train_ix.length)];\n      for(var k=0;k<this.candidates.length;k++) {\n        var x = this.data[dataix];\n        var l = this.labels[dataix];\n        this.candidates[k].trainer.train(x, l);\n      }\n\n      // process consequences: sample new folds, or candidates\n      var lastiter = this.num_epochs * fold.train_ix.length;\n      if(this.iter >= lastiter) {\n        // finished evaluation of this fold. Get final validation\n        // accuracies, record them, and go on to next fold.\n        var val_acc = this.evalValErrors();\n        for(var k=0;k<this.candidates.length;k++) {\n          var c = this.candidates[k];\n          c.acc.push(val_acc[k]);\n          c.accv += val_acc[k];\n        }\n        this.iter = 0; // reset step number\n        this.foldix++; // increment fold\n\n        if(this.finish_fold_callback !== null) {\n          this.finish_fold_callback();\n        }\n\n        if(this.foldix >= this.folds.length) {\n          // we finished all folds as well! Record these candidates\n          // and sample new ones to evaluate.\n          for(var k=0;k<this.candidates.length;k++) {\n            this.evaluated_candidates.push(this.candidates[k]);\n          }\n          // sort evaluated candidates according to accuracy achieved\n          this.evaluated_candidates.sort(function(a, b) { \n            return (a.accv / a.acc.length) \n                 > (b.accv / b.acc.length) \n                 ? -1 : 1;\n          });\n          // and clip only to the top few ones (lets place limit at 3*ensemble_size)\n          // otherwise there are concerns with keeping these all in memory \n          // if MagicNet is being evaluated for a very long time\n          if(this.evaluated_candidates.length > 3 * this.ensemble_size) {\n            this.evaluated_candidates = this.evaluated_candidates.slice(0, 3 * this.ensemble_size);\n          }\n          if(this.finish_batch_callback !== null) {\n            this.finish_batch_callback();\n          }\n          this.sampleCandidates(); // begin with new candidates\n          this.foldix = 0; // reset this\n        } else {\n          // we will go on to another fold. reset all candidates nets\n          for(var k=0;k<this.candidates.length;k++) {\n            var c = this.candidates[k];\n            var net = new Net();\n            net.makeLayers(c.layer_defs);\n            var trainer = new Trainer(net, c.trainer_def);\n            c.net = net;\n            c.trainer = trainer;\n          }\n        }\n      }\n    },\n\n    evalValErrors: function() {\n      // evaluate candidates on validation data and return performance of current networks\n      // as simple list\n      var vals = [];\n      var fold = this.folds[this.foldix]; // active fold\n      for(var k=0;k<this.candidates.length;k++) {\n        var net = this.candidates[k].net;\n        var v = 0.0;\n        for(var q=0;q<fold.test_ix.length;q++) {\n          var x = this.data[fold.test_ix[q]];\n          var l = this.labels[fold.test_ix[q]];\n          net.forward(x);\n          var yhat = net.getPrediction();\n          v += (yhat === l ? 1.0 : 0.0); // 0 1 loss\n        }\n        v /= fold.test_ix.length; // normalize\n        vals.push(v);\n      }\n      return vals;\n    },\n\n    // returns prediction scores for given test data point, as Vol\n    // uses an averaged prediction from the best ensemble_size models\n    // x is a Vol.\n    predict_soft: function(data) {\n      // forward prop the best networks\n      // and accumulate probabilities at last layer into a an output Vol\n      var nv = Math.min(this.ensemble_size, this.evaluated_candidates.length);\n      if(nv === 0) { return new convnetjs.Vol(0,0,0); } // not sure what to do here? we're not ready yet\n      var xout, n;\n      for(var j=0;j<nv;j++) {\n        var net = this.evaluated_candidates[j].net;\n        var x = net.forward(data);\n        if(j===0) { \n          xout = x; \n          n = x.w.length; \n        } else {\n          // add it on\n          for(var d=0;d<n;d++) {\n            xout.w[d] += x.w[d];\n          }\n        }\n      }\n      // produce average\n      for(var d=0;d<n;d++) {\n        xout.w[d] /= n;\n      }\n      return xout;\n    },\n\n    predict: function(data) {\n      var xout = this.predict_soft(data);\n      if(xout.w.length !== 0) {\n        var stats = maxmin(xout.w);\n        var predicted_label = stats.maxi; \n      } else {\n        var predicted_label = -1; // error out\n      }\n      return predicted_label;\n\n    },\n\n    toJSON: function() {\n      // dump the top ensemble_size networks as a list\n      var nv = Math.min(this.ensemble_size, this.evaluated_candidates.length);\n      var json = {};\n      json.nets = [];\n      for(var i=0;i<nv;i++) {\n        json.nets.push(this.evaluated_candidates[i].net.toJSON());\n      }\n      return json;\n    },\n\n    fromJSON: function(json) {\n      this.ensemble_size = json.nets.length;\n      this.evaluated_candidates = [];\n      for(var i=0;i<this.ensemble_size;i++) {\n        var net = new Net();\n        net.fromJSON(json.nets[i]);\n        var dummy_candidate = {};\n        dummy_candidate.net = net;\n        this.evaluated_candidates.push(dummy_candidate);\n      }\n    },\n\n    // callback functions\n    // called when a fold is finished, while evaluating a batch\n    onFinishFold: function(f) { this.finish_fold_callback = f; },\n    // called when a batch of candidates has finished evaluating\n    onFinishBatch: function(f) { this.finish_batch_callback = f; }\n    \n  };\n\n  global.MagicNet = MagicNet;\n})(convnetjs);\n(function(lib) {\n  \"use strict\";\n  if (typeof module === \"undefined\" || typeof module.exports === \"undefined\") {\n    window.jsfeat = lib; // in ordinary browser attach library to window\n  } else {\n    module.exports = lib; // in nodejs\n  }\n})(convnetjs);\n","import { LanguageDataType } from '@src/i18n';\nimport { ShowNoticeFunction, ShowConfirmDialogFunction, DataReportFunc } from '../deps';\n\n/**\n * Expressions in this project:\n *\n * iteration - call the trainer.train function once, i.e. train the net exactly once per iteration.\n * round - train the net several (ITER_PER_ROUND) times and draw the preview diagram once\n *         i.e. communicate between main thread (control layout) and worker (train & preview) once every round.\n *\n */\n\nexport enum TrainingStep {\n  EDITING_DATA,\n  CHOOSING_DATA_SET,\n  IMPORTING,\n  TRAINING,\n}\nexport enum ModelType {\n  IMG,\n  DATA,\n}\nexport const model_type_dict = ['img_recognition', 'data_recognition'];\n\n/**\n * Predict result and redraw the preview every round.\n * Train for 5 iterations per round.\n */\nexport const ITER_PER_ROUND = 5;\n\n/**\n * Target range of training.\n * Project all input data into this range before training to improve training perfermance.\n */\nexport const TARGET_RANGE = [-5, 5];\n\n/**\n * The edge of preview diagram.\n * Actual size of the diagram is 50x50\n */\nexport const SIZE_PREVIEW = 50;\n\n/**\n * Range of total training iterations in custom mode.\n */\nexport const RANGE_TRAINING_ITERATIONS = [50, 100, 200, 500, 1000, 2000];\n\n/**\n * Default training iterations for custom mode without user settings.\n */\nexport const DEFAULT_TRAINING_ITERATIONS = 100;\n\n/**\n * Selections for learning rate\n */\nexport const RANGE_LEARNING_RATE = [0.00001, 0.0001, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10];\n\n/**\n * Default learning rate without user settings.\n */\nexport const DEFAULT_LEARNING_RATE = 0.03;\n\n/**\n * Target accuracy (least accuracy) of training.\n * Training should be stoped when target accuracy and target loss are both reach.\n */\nexport const TARGET_ACCURACY = 0.95;\n\n/**\n * Target loss (max loss) of training.\n * Training should be stoped when target accuracy and target loss are both reach.\n */\nexport const TARGET_LOSS = 0.05;\n\n/**\n * Training should be stopped when it is stable, i.e. more iterations are meaningless for improving accuracy and reduce loss.\n * Only judge stable status when training rounds > 100 (training iterations: STABLE_BASE * ITER_PER_ROUND, in current case, 500)\n */\nexport const STABLE_BASE = 100;\n\n/**\n * Parameter to decide wheater a model is stable.\n */\nexport const STABLE_STANDARD = 0.0001;\n\n/**\n * The judgement of stability happens between current round and the round before STABLE_ROUND rounds.\n * When the differences of current accuracy and loss and that round is within STABLE_STANDARD, the model is stable.\n */\nexport const STABLE_ROUND = 500 / ITER_PER_ROUND; // 500 iterations, 100 rounds here\n\n/**\n * Length of empty data of every classification at the beginning of opening ai lab.\n */\nexport const DEFAULT_DATA_LENGTH = 5;\n\n/**\n * Least number of data rows per classification to start training.\n */\nexport const MIN_DATA_LENGTH = 5;\n\n/**\n * Range of number of features.\n */\nexport const RANGE_NUM_FEATURES = [1, 4];\n\n/**\n * Range of number of classifications.\n */\nexport const RANGE_NUM_CLASSES = [2, 4];\n\n/**\n * Colors for drawing preview diagram, indexed by classification.\n */\nexport const CLASS_COLORS = [\n  '#0057FF',\n  '#FF9948',\n  '#3DD1AE',\n  '#FF4B4B',\n];\n\nexport const ACC_LINE_COLOR = '#FF9948';\nexport const LOSS_LINE_COLOR = '#757575';\n\n/**\n * Data structure of the message data from main thread to worker.\n * Each message starts a round.\n */\nexport type TrainMessage = PreviewMessage | StartMessage | ContinueMessage;\n\ntype PreviewMessage = {\n  type:'preview',\n  data:number[][][], // data, indexed by classes -> row_index -> feature_index\n  dimension:number, // length of features\n  hidden_layers:number[], // structure of hidden layers\n};\n\ntype StartMessage = {\n  type:'start',\n  should_reset:boolean,\n\n  data?:number[][][], // data, indexed by classes -> row_index -> feature_index\n  dimension?:number, // length of features\n  hidden_layers?:number[], // structure of hidden layers\n\n  // Training parameters\n  is_custom_mode?:boolean,\n  // Can be modified any time\n  training_iterations?:number,\n  learning_rate?:number,\n};\n\ntype ContinueMessage = {\n  type:'continue',\n  should_reset:boolean,\n\n  // Can be modified any time\n  training_iterations?:number,\n  learning_rate?:number,\n};\n\n/**\n * Data structure of the message data from work to main thread at the end of every round.\n */\nexport interface TrainResult {\n  /**\n   * the value 'preview' is used when there is no model available\n   * i.e. before any train started, show the preview result of the random network\n   */\n  type:'preview' | 'update';\n  net:string;\n  draw_data:DrawData[][];\n  accuracy:number;\n  loss:number;\n}\n\n/**\n * 1. In status 'wait_start', the last train cannot be continued.\n *    i.e. although in custom mode, the user can only restart\n * 2. 'training' status indicates the train is processing.\n * 3. In status 'stopped', user can continue the train without resetting net in custom mode.\n */\nexport type TrainingStatus = 'wait_start' | 'training' | 'stopped';\n\n/**\n * Data structure of official data sets.\n */\nexport interface DataSet {\n  name:string;\n  classifications:string[];\n  preview:string;\n  source:(string | number)[][];\n  has_label:boolean;\n}\n\n/**\n * Default hidden layers without user settings.\n */\nexport const DEFAULT_HIDDEN_LAYERS:number[] = [4, 4];\n\n/**\n * Data structure for drawing preview diagram.\n */\nexport interface DrawData {\n  class_index:number;\n  density:number;\n}\n\nexport interface SavedTrainingState {\n  is_custom_mode:boolean;\n  training_status:TrainingStatus;\n  training_started_at:undefined | Date;\n  num_training_iterations:number;\n  learning_rate:number;\n\n  current_training_times:number;\n  current_acc:number;\n  current_loss:number;\n\n}\n\nexport interface TrainingState extends SavedTrainingState {\n  data_flag:string;\n}\n\n/**\n * Info about the model itself.\n */\nexport interface ModelState {\n  id?:string;\n  model_name:string;\n  model_type:undefined | ModelType;\n  features:string[]; // Used only in data recognition\n  classifications:string[];\n  hidden_layers:number[];\n  net:undefined | string;\n  max:number[];\n  min:number[];\n}\n\nexport interface DataState {\n  data:(number | string)[][][];\n  valid_rows:number[][];\n  should_hightlight:boolean;\n  // \n  symbol_index:number[][];\n}\n\nexport interface AIModelForSave {\n  /**\n * Functionality-related data is stored in model_state.\n */\n  model_state:ModelState;\n\n  /**\n   * Stores non-functionl data.\n   * Used only for revoking training parameters when editing a model\n   */\n  training_state:SavedTrainingState;\n\n  /**\n   * data-state\n   * Used for data edit\n  */\n  data_state:DataState;\n}\n\nexport enum LastStepBeforeSaving {\n  Close,\n  FinishTraining,\n  SaveInTheDataEditPage,\n}\n\nexport interface KittenAILabProps {\n  element:HTMLElement;\n  language_data:LanguageDataType;\n  cdn_path?:string;\n  /**\n   * callback to hide the rendered dom in kitten.\n   */\n  save_and_close:(model:AIModelForSave, lase_step?:LastStepBeforeSaving) => void;\n  show_notice?:ShowNoticeFunction;\n  show_confirm_dialog?:ShowConfirmDialogFunction;\n  report_event?:DataReportFunc;\n}\n","import {\n  TrainMessage, DEFAULT_HIDDEN_LAYERS, DEFAULT_LEARNING_RATE,\n  TrainResult, DrawData, SIZE_PREVIEW, TARGET_RANGE,\n} from '../../models/def';\nimport { Net, Trainer } from '../../types';\nconst convnetjs = require<any>('convnetjs');\nconst ctx:Worker = self as any;\nconst send_message:any = self.postMessage;\n\nlet data:number[][][] = []; // formatted data, in range [-5, 5]\nlet dimension:number; // number of features\n\nlet is_custom_mode:boolean = false;\nlet hidden_layers:number[] = DEFAULT_HIDDEN_LAYERS;\nlet training_iterations:number = 100;\nlet learning_rate:number = DEFAULT_LEARNING_RATE;\n\nlet net:Net;\nlet trainer:Trainer;\n\nctx.addEventListener('message', (message) => {\n  const receive_data:TrainMessage = JSON.parse(message.data);\n\n  // Training params can be modified while training\n  if (receive_data.type !== 'preview') {\n    if (receive_data.type === 'start') {\n      is_custom_mode = receive_data.is_custom_mode || is_custom_mode;\n    }\n    training_iterations = receive_data.training_iterations || training_iterations;\n    learning_rate = receive_data.learning_rate || learning_rate;\n  }\n\n  // Reset data and the net when data being changed\n  if (receive_data.type === 'preview' ||\n    (receive_data.type === 'start' && receive_data.should_reset)) {\n    data = receive_data.data || data;\n    hidden_layers = receive_data.hidden_layers || hidden_layers;\n    dimension = receive_data.dimension || dimension;\n    const layers = [];\n    layers.push({ type: 'input', out_sx: 1, out_sy: 1, out_depth: dimension });\n    hidden_layers.forEach((num_neurons) => {\n      layers.push({ type: 'fc', num_neurons, activation: 'tanh' }); // TODO: choose proper activation function\n    });\n    layers.push({ type: 'softmax', num_classes: data.length });\n\n    __DEV__ && console.info('Reset training data and net: ', data, hidden_layers);\n    net = new convnetjs.Net();\n    net.makeLayers(layers);\n  }\n\n  // Reset trainer when necessary\n  if (receive_data.type === 'preview' || receive_data.should_reset) {\n    const trainer_options = !is_custom_mode ?\n      {\n        method: 'adagrad',\n        l2_decay: 0.001, // FIXME\n        batch_size: 10,\n      } :\n      {\n        method: 'sgd',\n        learning_rate,\n        l2_decay: learning_rate / 10,\n        batch_size: 10,\n      };\n    trainer = new convnetjs.Trainer(net, trainer_options);\n  }\n\n  /** ***** Train ******/\n  const calculate_accuracy = () => {\n    // Decide accuracy by average accuracy of all input data\n    let num_correct = 0;\n    let num_data = 0;\n    data.forEach((classification_data, class_index) => {\n      classification_data.forEach((data_row) => {\n        num_data++;\n        const x = new convnetjs.Vol(data_row);\n        const scores = net.forward(x);\n        num_correct += scores.w[class_index];\n      });\n    });\n\n    // Alternatively: decide accuracy by ratio of correct results\n    // const num_correct =\n    //   data.map((classification_data, class_index) =>\n    //     classification_data.filter((data_row) => {\n    //       num_data++;\n    //       const x = new convnetjs.Vol(data_row);\n    //       const scores = net.forward(x);\n    //       return scores.w[class_index] > 0.5; // TODO define 'correct' -> possibility of right classification > 50%?\n    //     }).length)\n    //   .reduce((a, c) => a + c);\n\n    // TODO choose a proper algorithm for accuracy: correct ratio or average accuracy\n\n    return parseFloat((num_correct / num_data).toFixed(4));\n  };\n\n  let loss = 0;\n  let accuracy = 0;\n  let num_training = 0;\n  for (let loop_time = 0; loop_time < (receive_data.type === 'preview' ? 0 : training_iterations); loop_time++) {\n    data.forEach((classification:number[][], class_index:number) => {\n      classification.forEach((data_row) => {\n        const x = new convnetjs.Vol(data_row);\n        const res = trainer.train(x, class_index);\n        loss += res.loss;\n        num_training++;\n      });\n    });\n  }\n\n  let av_loss = loss / num_training;\n  //  1  0.9999  1  demo \n  av_loss = av_loss > 1 ? 0.9999 : av_loss;\n\n  loss = Number((av_loss).toFixed(4));\n  accuracy = calculate_accuracy();\n\n  /** ***** Draw ******/\n  // Forward all dots between -5 and 5\n  const step = (TARGET_RANGE[1] - TARGET_RANGE[0]) / SIZE_PREVIEW; // 10 / 50 = 0.2\n  const draw_data:DrawData[][] = [];\n\n  let num_line = 0;\n  for (let x = -5; x < 5; x = Number((x + step).toFixed(1))) { // Avoid data like 4.80000000000001\n    draw_data[num_line] = [];\n    for (let y = -5; y < 5; y = Number((y + step).toFixed(1))) {\n      const test_node = new convnetjs.Vol([x, y]);\n      const scores = net.forward(test_node);\n      let main_class:number = 0;\n      const benchmark = 1 / scores.w.length;\n      let density = benchmark;\n      scores.w.forEach((weight:number, class_index:number) => {\n        if (weight > benchmark) {\n          main_class = class_index;\n          density = weight;\n        }\n      });\n      density = (density - benchmark) / (1 - benchmark); // adjust density to range [0, 1], for better diagram quality\n\n      draw_data[num_line].unshift({\n        class_index: main_class,\n        density,\n      });\n    }\n    num_line++;\n  }\n\n  const training_result:TrainResult = {\n    type: receive_data.type === 'preview' ? 'preview' : 'update',\n    net: JSON.stringify(net.toJSON()),\n    draw_data,\n    accuracy,\n    loss,\n  };\n  send_message(training_result);\n});\n"],"sourceRoot":""}